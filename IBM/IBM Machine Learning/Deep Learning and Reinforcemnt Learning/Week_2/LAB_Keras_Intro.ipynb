{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part d: Keras Intro LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "from tensorflow.keras.models  import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set \n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv('diabetes.csv', names=names, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.258</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>9</td>\n",
       "      <td>164</td>\n",
       "      <td>84</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.831</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9</td>\n",
       "      <td>171</td>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "      <td>240</td>\n",
       "      <td>45.4</td>\n",
       "      <td>0.721</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>120</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.198</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "60                2                      84               0               0   \n",
       "463               5                      88              78              30   \n",
       "238               9                     164              84              21   \n",
       "43                9                     171             110              24   \n",
       "573               2                      98              60              17   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "60         0   0.0              0.304   21             0  \n",
       "463        0  27.6              0.258   37             0  \n",
       "238        0  30.8              0.831   32             1  \n",
       "43       240  45.4              0.721   54             1  \n",
       "573      120  34.7              0.198   22             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise 1: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.781\n",
      "roc-auc is 0.833\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (8,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8252 - accuracy: 0.3247 - val_loss: 0.8142 - val_accuracy: 0.3125\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.8108 - accuracy: 0.3229 - val_loss: 0.8014 - val_accuracy: 0.3125\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7979 - accuracy: 0.3264 - val_loss: 0.7898 - val_accuracy: 0.3021\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7861 - accuracy: 0.3142 - val_loss: 0.7793 - val_accuracy: 0.2969\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7754 - accuracy: 0.3090 - val_loss: 0.7697 - val_accuracy: 0.2917\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7657 - accuracy: 0.3108 - val_loss: 0.7611 - val_accuracy: 0.2812\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7568 - accuracy: 0.3038 - val_loss: 0.7532 - val_accuracy: 0.2865\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7487 - accuracy: 0.3056 - val_loss: 0.7461 - val_accuracy: 0.3073\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7414 - accuracy: 0.2934 - val_loss: 0.7396 - val_accuracy: 0.3021\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7347 - accuracy: 0.3038 - val_loss: 0.7337 - val_accuracy: 0.3229\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7286 - accuracy: 0.3142 - val_loss: 0.7283 - val_accuracy: 0.3229\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.7230 - accuracy: 0.3420 - val_loss: 0.7233 - val_accuracy: 0.3594\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7178 - accuracy: 0.3767 - val_loss: 0.7188 - val_accuracy: 0.4323\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7131 - accuracy: 0.4167 - val_loss: 0.7147 - val_accuracy: 0.4635\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7089 - accuracy: 0.4462 - val_loss: 0.7109 - val_accuracy: 0.5052\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7048 - accuracy: 0.5156 - val_loss: 0.7073 - val_accuracy: 0.5208\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.7011 - accuracy: 0.5625 - val_loss: 0.7041 - val_accuracy: 0.5885\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6977 - accuracy: 0.5972 - val_loss: 0.7011 - val_accuracy: 0.6198\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6945 - accuracy: 0.6233 - val_loss: 0.6983 - val_accuracy: 0.6354\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6916 - accuracy: 0.6476 - val_loss: 0.6957 - val_accuracy: 0.6354\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6888 - accuracy: 0.6528 - val_loss: 0.6932 - val_accuracy: 0.6406\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6862 - accuracy: 0.6545 - val_loss: 0.6909 - val_accuracy: 0.6406\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6838 - accuracy: 0.6545 - val_loss: 0.6888 - val_accuracy: 0.6406\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6815 - accuracy: 0.6545 - val_loss: 0.6868 - val_accuracy: 0.6406\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6793 - accuracy: 0.6545 - val_loss: 0.6848 - val_accuracy: 0.6406\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6773 - accuracy: 0.6545 - val_loss: 0.6830 - val_accuracy: 0.6406\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6754 - accuracy: 0.6545 - val_loss: 0.6813 - val_accuracy: 0.6406\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6735 - accuracy: 0.6545 - val_loss: 0.6796 - val_accuracy: 0.6406\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6717 - accuracy: 0.6545 - val_loss: 0.6781 - val_accuracy: 0.6406\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6701 - accuracy: 0.6545 - val_loss: 0.6766 - val_accuracy: 0.6406\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6685 - accuracy: 0.6545 - val_loss: 0.6751 - val_accuracy: 0.6406\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6669 - accuracy: 0.6545 - val_loss: 0.6737 - val_accuracy: 0.6406\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6655 - accuracy: 0.6545 - val_loss: 0.6724 - val_accuracy: 0.6406\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6640 - accuracy: 0.6545 - val_loss: 0.6711 - val_accuracy: 0.6406\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6626 - accuracy: 0.6545 - val_loss: 0.6698 - val_accuracy: 0.6406\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6613 - accuracy: 0.6545 - val_loss: 0.6686 - val_accuracy: 0.6406\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6600 - accuracy: 0.6545 - val_loss: 0.6674 - val_accuracy: 0.6406\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6587 - accuracy: 0.6545 - val_loss: 0.6662 - val_accuracy: 0.6406\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6575 - accuracy: 0.6545 - val_loss: 0.6650 - val_accuracy: 0.6406\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6563 - accuracy: 0.6545 - val_loss: 0.6639 - val_accuracy: 0.6406\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6551 - accuracy: 0.6545 - val_loss: 0.6628 - val_accuracy: 0.6406\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6540 - accuracy: 0.6545 - val_loss: 0.6618 - val_accuracy: 0.6406\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6529 - accuracy: 0.6545 - val_loss: 0.6607 - val_accuracy: 0.6406\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6518 - accuracy: 0.6545 - val_loss: 0.6597 - val_accuracy: 0.6406\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6507 - accuracy: 0.6545 - val_loss: 0.6586 - val_accuracy: 0.6406\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6496 - accuracy: 0.6545 - val_loss: 0.6576 - val_accuracy: 0.6406\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.6486 - accuracy: 0.6545 - val_loss: 0.6566 - val_accuracy: 0.6406\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6476 - accuracy: 0.6545 - val_loss: 0.6557 - val_accuracy: 0.6406\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6466 - accuracy: 0.6545 - val_loss: 0.6547 - val_accuracy: 0.6406\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6456 - accuracy: 0.6545 - val_loss: 0.6537 - val_accuracy: 0.6406\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6446 - accuracy: 0.6545 - val_loss: 0.6528 - val_accuracy: 0.6406\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6437 - accuracy: 0.6545 - val_loss: 0.6519 - val_accuracy: 0.6406\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6427 - accuracy: 0.6545 - val_loss: 0.6509 - val_accuracy: 0.6406\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6417 - accuracy: 0.6545 - val_loss: 0.6500 - val_accuracy: 0.6406\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6408 - accuracy: 0.6545 - val_loss: 0.6491 - val_accuracy: 0.6406\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6399 - accuracy: 0.6545 - val_loss: 0.6482 - val_accuracy: 0.6406\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6389 - accuracy: 0.6545 - val_loss: 0.6473 - val_accuracy: 0.6406\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6380 - accuracy: 0.6545 - val_loss: 0.6464 - val_accuracy: 0.6406\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6372 - accuracy: 0.6545 - val_loss: 0.6456 - val_accuracy: 0.6406\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6363 - accuracy: 0.6545 - val_loss: 0.6447 - val_accuracy: 0.6406\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6354 - accuracy: 0.6545 - val_loss: 0.6438 - val_accuracy: 0.6406\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6345 - accuracy: 0.6545 - val_loss: 0.6430 - val_accuracy: 0.6406\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6336 - accuracy: 0.6545 - val_loss: 0.6421 - val_accuracy: 0.6406\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6328 - accuracy: 0.6545 - val_loss: 0.6413 - val_accuracy: 0.6406\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6320 - accuracy: 0.6545 - val_loss: 0.6404 - val_accuracy: 0.6406\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6311 - accuracy: 0.6545 - val_loss: 0.6396 - val_accuracy: 0.6406\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6303 - accuracy: 0.6545 - val_loss: 0.6387 - val_accuracy: 0.6406\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6294 - accuracy: 0.6545 - val_loss: 0.6379 - val_accuracy: 0.6406\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6286 - accuracy: 0.6545 - val_loss: 0.6371 - val_accuracy: 0.6406\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6278 - accuracy: 0.6545 - val_loss: 0.6363 - val_accuracy: 0.6406\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6270 - accuracy: 0.6545 - val_loss: 0.6355 - val_accuracy: 0.6406\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6262 - accuracy: 0.6545 - val_loss: 0.6347 - val_accuracy: 0.6406\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.6254 - accuracy: 0.6545 - val_loss: 0.6339 - val_accuracy: 0.6406\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6246 - accuracy: 0.6545 - val_loss: 0.6331 - val_accuracy: 0.6406\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6238 - accuracy: 0.6545 - val_loss: 0.6323 - val_accuracy: 0.6406\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6230 - accuracy: 0.6545 - val_loss: 0.6315 - val_accuracy: 0.6406\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6222 - accuracy: 0.6545 - val_loss: 0.6307 - val_accuracy: 0.6406\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6214 - accuracy: 0.6545 - val_loss: 0.6299 - val_accuracy: 0.6406\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 780us/step - loss: 0.6206 - accuracy: 0.6545 - val_loss: 0.6291 - val_accuracy: 0.6406\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.6199 - accuracy: 0.6545 - val_loss: 0.6284 - val_accuracy: 0.6406\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6191 - accuracy: 0.6545 - val_loss: 0.6276 - val_accuracy: 0.6406\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6183 - accuracy: 0.6545 - val_loss: 0.6269 - val_accuracy: 0.6406\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.6176 - accuracy: 0.6545 - val_loss: 0.6261 - val_accuracy: 0.6406\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6169 - accuracy: 0.6545 - val_loss: 0.6253 - val_accuracy: 0.6406\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6161 - accuracy: 0.6545 - val_loss: 0.6246 - val_accuracy: 0.6406\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6154 - accuracy: 0.6545 - val_loss: 0.6238 - val_accuracy: 0.6406\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6146 - accuracy: 0.6545 - val_loss: 0.6231 - val_accuracy: 0.6406\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6139 - accuracy: 0.6545 - val_loss: 0.6224 - val_accuracy: 0.6406\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6132 - accuracy: 0.6545 - val_loss: 0.6216 - val_accuracy: 0.6406\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6124 - accuracy: 0.6545 - val_loss: 0.6209 - val_accuracy: 0.6406\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6117 - accuracy: 0.6545 - val_loss: 0.6202 - val_accuracy: 0.6406\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6110 - accuracy: 0.6545 - val_loss: 0.6195 - val_accuracy: 0.6406\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6103 - accuracy: 0.6545 - val_loss: 0.6187 - val_accuracy: 0.6406\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6096 - accuracy: 0.6545 - val_loss: 0.6180 - val_accuracy: 0.6406\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6088 - accuracy: 0.6545 - val_loss: 0.6173 - val_accuracy: 0.6406\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6082 - accuracy: 0.6545 - val_loss: 0.6166 - val_accuracy: 0.6406\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6075 - accuracy: 0.6545 - val_loss: 0.6159 - val_accuracy: 0.6406\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6068 - accuracy: 0.6545 - val_loss: 0.6152 - val_accuracy: 0.6406\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.6061 - accuracy: 0.6545 - val_loss: 0.6145 - val_accuracy: 0.6406\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6054 - accuracy: 0.6545 - val_loss: 0.6138 - val_accuracy: 0.6406\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6047 - accuracy: 0.6545 - val_loss: 0.6131 - val_accuracy: 0.6406\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6040 - accuracy: 0.6545 - val_loss: 0.6125 - val_accuracy: 0.6406\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6033 - accuracy: 0.6545 - val_loss: 0.6118 - val_accuracy: 0.6406\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6027 - accuracy: 0.6545 - val_loss: 0.6111 - val_accuracy: 0.6406\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6020 - accuracy: 0.6545 - val_loss: 0.6104 - val_accuracy: 0.6406\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6014 - accuracy: 0.6545 - val_loss: 0.6098 - val_accuracy: 0.6406\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6007 - accuracy: 0.6545 - val_loss: 0.6091 - val_accuracy: 0.6406\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6001 - accuracy: 0.6545 - val_loss: 0.6084 - val_accuracy: 0.6406\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5994 - accuracy: 0.6545 - val_loss: 0.6078 - val_accuracy: 0.6406\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5987 - accuracy: 0.6545 - val_loss: 0.6071 - val_accuracy: 0.6406\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5981 - accuracy: 0.6545 - val_loss: 0.6065 - val_accuracy: 0.6406\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5974 - accuracy: 0.6545 - val_loss: 0.6058 - val_accuracy: 0.6406\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5968 - accuracy: 0.6545 - val_loss: 0.6052 - val_accuracy: 0.6406\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5961 - accuracy: 0.6528 - val_loss: 0.6045 - val_accuracy: 0.6406\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5955 - accuracy: 0.6528 - val_loss: 0.6039 - val_accuracy: 0.6406\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5949 - accuracy: 0.6528 - val_loss: 0.6033 - val_accuracy: 0.6406\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5942 - accuracy: 0.6528 - val_loss: 0.6026 - val_accuracy: 0.6406\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5936 - accuracy: 0.6528 - val_loss: 0.6020 - val_accuracy: 0.6406\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5930 - accuracy: 0.6528 - val_loss: 0.6014 - val_accuracy: 0.6406\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5924 - accuracy: 0.6528 - val_loss: 0.6007 - val_accuracy: 0.6406\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5918 - accuracy: 0.6528 - val_loss: 0.6001 - val_accuracy: 0.6406\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5911 - accuracy: 0.6528 - val_loss: 0.5995 - val_accuracy: 0.6406\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5905 - accuracy: 0.6528 - val_loss: 0.5989 - val_accuracy: 0.6406\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5899 - accuracy: 0.6545 - val_loss: 0.5983 - val_accuracy: 0.6406\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5893 - accuracy: 0.6528 - val_loss: 0.5977 - val_accuracy: 0.6458\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5887 - accuracy: 0.6528 - val_loss: 0.5971 - val_accuracy: 0.6458\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5881 - accuracy: 0.6528 - val_loss: 0.5965 - val_accuracy: 0.6458\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5876 - accuracy: 0.6528 - val_loss: 0.5959 - val_accuracy: 0.6458\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5869 - accuracy: 0.6528 - val_loss: 0.5953 - val_accuracy: 0.6458\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5864 - accuracy: 0.6545 - val_loss: 0.5947 - val_accuracy: 0.6458\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5858 - accuracy: 0.6545 - val_loss: 0.5941 - val_accuracy: 0.6458\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5852 - accuracy: 0.6562 - val_loss: 0.5935 - val_accuracy: 0.6458\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5846 - accuracy: 0.6580 - val_loss: 0.5929 - val_accuracy: 0.6458\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5840 - accuracy: 0.6615 - val_loss: 0.5924 - val_accuracy: 0.6510\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5834 - accuracy: 0.6615 - val_loss: 0.5918 - val_accuracy: 0.6510\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5829 - accuracy: 0.6615 - val_loss: 0.5912 - val_accuracy: 0.6615\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5823 - accuracy: 0.6615 - val_loss: 0.5906 - val_accuracy: 0.6615\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5817 - accuracy: 0.6632 - val_loss: 0.5901 - val_accuracy: 0.6615\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5812 - accuracy: 0.6649 - val_loss: 0.5895 - val_accuracy: 0.6615\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5806 - accuracy: 0.6649 - val_loss: 0.5889 - val_accuracy: 0.6615\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5800 - accuracy: 0.6649 - val_loss: 0.5884 - val_accuracy: 0.6615\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5795 - accuracy: 0.6649 - val_loss: 0.5878 - val_accuracy: 0.6615\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5789 - accuracy: 0.6649 - val_loss: 0.5873 - val_accuracy: 0.6615\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5784 - accuracy: 0.6649 - val_loss: 0.5867 - val_accuracy: 0.6615\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5778 - accuracy: 0.6649 - val_loss: 0.5862 - val_accuracy: 0.6615\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5773 - accuracy: 0.6684 - val_loss: 0.5856 - val_accuracy: 0.6615\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5767 - accuracy: 0.6684 - val_loss: 0.5851 - val_accuracy: 0.6719\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5762 - accuracy: 0.6684 - val_loss: 0.5846 - val_accuracy: 0.6719\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5757 - accuracy: 0.6684 - val_loss: 0.5840 - val_accuracy: 0.6771\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5751 - accuracy: 0.6684 - val_loss: 0.5835 - val_accuracy: 0.6823\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5746 - accuracy: 0.6701 - val_loss: 0.5829 - val_accuracy: 0.6823\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5741 - accuracy: 0.6701 - val_loss: 0.5824 - val_accuracy: 0.6823\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5735 - accuracy: 0.6684 - val_loss: 0.5819 - val_accuracy: 0.6823\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5730 - accuracy: 0.6684 - val_loss: 0.5814 - val_accuracy: 0.6823\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5725 - accuracy: 0.6684 - val_loss: 0.5809 - val_accuracy: 0.6823\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5720 - accuracy: 0.6684 - val_loss: 0.5803 - val_accuracy: 0.6823\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5715 - accuracy: 0.6736 - val_loss: 0.5798 - val_accuracy: 0.6823\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5709 - accuracy: 0.6753 - val_loss: 0.5793 - val_accuracy: 0.6823\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5704 - accuracy: 0.6771 - val_loss: 0.5788 - val_accuracy: 0.6823\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5699 - accuracy: 0.6788 - val_loss: 0.5783 - val_accuracy: 0.6875\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5694 - accuracy: 0.6788 - val_loss: 0.5778 - val_accuracy: 0.6875\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5689 - accuracy: 0.6788 - val_loss: 0.5773 - val_accuracy: 0.6875\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5684 - accuracy: 0.6806 - val_loss: 0.5768 - val_accuracy: 0.6927\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5679 - accuracy: 0.6806 - val_loss: 0.5763 - val_accuracy: 0.6979\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5674 - accuracy: 0.6823 - val_loss: 0.5758 - val_accuracy: 0.6979\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5669 - accuracy: 0.6823 - val_loss: 0.5753 - val_accuracy: 0.6979\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5664 - accuracy: 0.6840 - val_loss: 0.5748 - val_accuracy: 0.6979\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5659 - accuracy: 0.6840 - val_loss: 0.5743 - val_accuracy: 0.6979\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5654 - accuracy: 0.6840 - val_loss: 0.5738 - val_accuracy: 0.6979\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5649 - accuracy: 0.6840 - val_loss: 0.5734 - val_accuracy: 0.6979\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5644 - accuracy: 0.6823 - val_loss: 0.5729 - val_accuracy: 0.7031\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 917us/step - loss: 0.5639 - accuracy: 0.6823 - val_loss: 0.5724 - val_accuracy: 0.7031\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5635 - accuracy: 0.6806 - val_loss: 0.5719 - val_accuracy: 0.7031\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5630 - accuracy: 0.6806 - val_loss: 0.5714 - val_accuracy: 0.7031\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5625 - accuracy: 0.6823 - val_loss: 0.5710 - val_accuracy: 0.7083\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5621 - accuracy: 0.6840 - val_loss: 0.5705 - val_accuracy: 0.7083\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5616 - accuracy: 0.6840 - val_loss: 0.5700 - val_accuracy: 0.7031\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5611 - accuracy: 0.6858 - val_loss: 0.5696 - val_accuracy: 0.7031\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5606 - accuracy: 0.6858 - val_loss: 0.5691 - val_accuracy: 0.7083\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5601 - accuracy: 0.6910 - val_loss: 0.5687 - val_accuracy: 0.7031\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5597 - accuracy: 0.6875 - val_loss: 0.5682 - val_accuracy: 0.7031\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5593 - accuracy: 0.6927 - val_loss: 0.5677 - val_accuracy: 0.7135\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5587 - accuracy: 0.6927 - val_loss: 0.5673 - val_accuracy: 0.7135\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5583 - accuracy: 0.6962 - val_loss: 0.5668 - val_accuracy: 0.7135\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5579 - accuracy: 0.6979 - val_loss: 0.5664 - val_accuracy: 0.7135\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5574 - accuracy: 0.7014 - val_loss: 0.5660 - val_accuracy: 0.7135\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5569 - accuracy: 0.7014 - val_loss: 0.5655 - val_accuracy: 0.7135\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5565 - accuracy: 0.7049 - val_loss: 0.5651 - val_accuracy: 0.7135\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5560 - accuracy: 0.7049 - val_loss: 0.5646 - val_accuracy: 0.7135\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5556 - accuracy: 0.7083 - val_loss: 0.5642 - val_accuracy: 0.7135\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5552 - accuracy: 0.7066 - val_loss: 0.5638 - val_accuracy: 0.7135\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5547 - accuracy: 0.7083 - val_loss: 0.5633 - val_accuracy: 0.7240\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5543 - accuracy: 0.7066 - val_loss: 0.5629 - val_accuracy: 0.7240\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5538 - accuracy: 0.7083 - val_loss: 0.5625 - val_accuracy: 0.7240\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5534 - accuracy: 0.7083 - val_loss: 0.5620 - val_accuracy: 0.7240\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5530 - accuracy: 0.7083 - val_loss: 0.5616 - val_accuracy: 0.7240\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5526 - accuracy: 0.7083 - val_loss: 0.5612 - val_accuracy: 0.7240\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5521 - accuracy: 0.7118 - val_loss: 0.5608 - val_accuracy: 0.7292\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5517 - accuracy: 0.7135 - val_loss: 0.5604 - val_accuracy: 0.7292\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5513 - accuracy: 0.7118 - val_loss: 0.5600 - val_accuracy: 0.7292\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-52856171b604>:4: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3920735 ],\n",
       "       [0.4610242 ],\n",
       "       [0.33009917],\n",
       "       [0.34910113],\n",
       "       [0.27623618],\n",
       "       [0.40524656],\n",
       "       [0.21943322],\n",
       "       [0.34542155],\n",
       "       [0.57708746],\n",
       "       [0.33400106]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.729\n",
      "roc-auc is 0.812\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25a2bb0cbb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 0.5446 - accuracy: 0.7812WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.7135 - val_loss: 0.5595 - val_accuracy: 0.7344\n",
      "Epoch 2/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5504 - accuracy: 0.7153 - val_loss: 0.5591 - val_accuracy: 0.7344\n",
      "Epoch 3/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5500 - accuracy: 0.7170 - val_loss: 0.5587 - val_accuracy: 0.7344\n",
      "Epoch 4/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5496 - accuracy: 0.7170 - val_loss: 0.5583 - val_accuracy: 0.7344\n",
      "Epoch 5/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5492 - accuracy: 0.7188 - val_loss: 0.5579 - val_accuracy: 0.7344\n",
      "Epoch 6/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5488 - accuracy: 0.7170 - val_loss: 0.5575 - val_accuracy: 0.7344\n",
      "Epoch 7/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5483 - accuracy: 0.7170 - val_loss: 0.5571 - val_accuracy: 0.7344\n",
      "Epoch 8/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5479 - accuracy: 0.7170 - val_loss: 0.5567 - val_accuracy: 0.7344\n",
      "Epoch 9/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5475 - accuracy: 0.7170 - val_loss: 0.5563 - val_accuracy: 0.7396\n",
      "Epoch 10/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5471 - accuracy: 0.7170 - val_loss: 0.5559 - val_accuracy: 0.7396\n",
      "Epoch 11/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5467 - accuracy: 0.7170 - val_loss: 0.5555 - val_accuracy: 0.7396\n",
      "Epoch 12/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5463 - accuracy: 0.7170 - val_loss: 0.5551 - val_accuracy: 0.7396\n",
      "Epoch 13/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5459 - accuracy: 0.7188 - val_loss: 0.5548 - val_accuracy: 0.7396\n",
      "Epoch 14/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5455 - accuracy: 0.7188 - val_loss: 0.5544 - val_accuracy: 0.7396\n",
      "Epoch 15/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5451 - accuracy: 0.7188 - val_loss: 0.5540 - val_accuracy: 0.7396\n",
      "Epoch 16/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5447 - accuracy: 0.7222 - val_loss: 0.5536 - val_accuracy: 0.7448\n",
      "Epoch 17/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5443 - accuracy: 0.7240 - val_loss: 0.5532 - val_accuracy: 0.7448\n",
      "Epoch 18/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5440 - accuracy: 0.7240 - val_loss: 0.5528 - val_accuracy: 0.7448\n",
      "Epoch 19/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5435 - accuracy: 0.7274 - val_loss: 0.5525 - val_accuracy: 0.7396\n",
      "Epoch 20/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5432 - accuracy: 0.7274 - val_loss: 0.5521 - val_accuracy: 0.7396\n",
      "Epoch 21/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5428 - accuracy: 0.7274 - val_loss: 0.5517 - val_accuracy: 0.7396\n",
      "Epoch 22/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5424 - accuracy: 0.7274 - val_loss: 0.5513 - val_accuracy: 0.7396\n",
      "Epoch 23/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5420 - accuracy: 0.7292 - val_loss: 0.5510 - val_accuracy: 0.7448\n",
      "Epoch 24/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5416 - accuracy: 0.7326 - val_loss: 0.5506 - val_accuracy: 0.7448\n",
      "Epoch 25/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5413 - accuracy: 0.7292 - val_loss: 0.5502 - val_accuracy: 0.7448\n",
      "Epoch 26/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5409 - accuracy: 0.7326 - val_loss: 0.5499 - val_accuracy: 0.7448\n",
      "Epoch 27/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5405 - accuracy: 0.7326 - val_loss: 0.5495 - val_accuracy: 0.7448\n",
      "Epoch 28/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5401 - accuracy: 0.7344 - val_loss: 0.5492 - val_accuracy: 0.7448\n",
      "Epoch 29/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5398 - accuracy: 0.7344 - val_loss: 0.5488 - val_accuracy: 0.7448\n",
      "Epoch 30/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5394 - accuracy: 0.7344 - val_loss: 0.5485 - val_accuracy: 0.7448\n",
      "Epoch 31/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5390 - accuracy: 0.7378 - val_loss: 0.5481 - val_accuracy: 0.7448\n",
      "Epoch 32/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5386 - accuracy: 0.7378 - val_loss: 0.5477 - val_accuracy: 0.7448\n",
      "Epoch 33/1000\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.5383 - accuracy: 0.7396 - val_loss: 0.5474 - val_accuracy: 0.7448\n",
      "Epoch 34/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.5379 - accuracy: 0.7326 - val_loss: 0.5471 - val_accuracy: 0.7448\n",
      "Epoch 35/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5375 - accuracy: 0.7361 - val_loss: 0.5467 - val_accuracy: 0.7500\n",
      "Epoch 36/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5372 - accuracy: 0.7361 - val_loss: 0.5464 - val_accuracy: 0.7500\n",
      "Epoch 37/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5368 - accuracy: 0.7361 - val_loss: 0.5460 - val_accuracy: 0.7500\n",
      "Epoch 38/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5365 - accuracy: 0.7361 - val_loss: 0.5457 - val_accuracy: 0.7500\n",
      "Epoch 39/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5361 - accuracy: 0.7361 - val_loss: 0.5453 - val_accuracy: 0.7500\n",
      "Epoch 40/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5358 - accuracy: 0.7361 - val_loss: 0.5450 - val_accuracy: 0.7500\n",
      "Epoch 41/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5354 - accuracy: 0.7361 - val_loss: 0.5447 - val_accuracy: 0.7500\n",
      "Epoch 42/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5351 - accuracy: 0.7361 - val_loss: 0.5443 - val_accuracy: 0.7500\n",
      "Epoch 43/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5347 - accuracy: 0.7361 - val_loss: 0.5440 - val_accuracy: 0.7552\n",
      "Epoch 44/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5344 - accuracy: 0.7378 - val_loss: 0.5437 - val_accuracy: 0.7552\n",
      "Epoch 45/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5340 - accuracy: 0.7378 - val_loss: 0.5433 - val_accuracy: 0.7604\n",
      "Epoch 46/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5337 - accuracy: 0.7378 - val_loss: 0.5430 - val_accuracy: 0.7604\n",
      "Epoch 47/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5333 - accuracy: 0.7396 - val_loss: 0.5427 - val_accuracy: 0.7604\n",
      "Epoch 48/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5330 - accuracy: 0.7396 - val_loss: 0.5424 - val_accuracy: 0.7656\n",
      "Epoch 49/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5326 - accuracy: 0.7413 - val_loss: 0.5420 - val_accuracy: 0.7656\n",
      "Epoch 50/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5323 - accuracy: 0.7413 - val_loss: 0.5417 - val_accuracy: 0.7656\n",
      "Epoch 51/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5320 - accuracy: 0.7431 - val_loss: 0.5414 - val_accuracy: 0.7656\n",
      "Epoch 52/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5316 - accuracy: 0.7413 - val_loss: 0.5411 - val_accuracy: 0.7604\n",
      "Epoch 53/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5313 - accuracy: 0.7431 - val_loss: 0.5408 - val_accuracy: 0.7604\n",
      "Epoch 54/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5310 - accuracy: 0.7448 - val_loss: 0.5405 - val_accuracy: 0.7604\n",
      "Epoch 55/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5307 - accuracy: 0.7431 - val_loss: 0.5402 - val_accuracy: 0.7604\n",
      "Epoch 56/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5303 - accuracy: 0.7396 - val_loss: 0.5398 - val_accuracy: 0.7604\n",
      "Epoch 57/1000\n",
      "18/18 [==============================] - 0s 835us/step - loss: 0.5300 - accuracy: 0.7413 - val_loss: 0.5395 - val_accuracy: 0.7604\n",
      "Epoch 58/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.5296 - accuracy: 0.7413 - val_loss: 0.5392 - val_accuracy: 0.7604\n",
      "Epoch 59/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5293 - accuracy: 0.7413 - val_loss: 0.5389 - val_accuracy: 0.7604\n",
      "Epoch 60/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5290 - accuracy: 0.7431 - val_loss: 0.5386 - val_accuracy: 0.7656\n",
      "Epoch 61/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5287 - accuracy: 0.7465 - val_loss: 0.5383 - val_accuracy: 0.7656\n",
      "Epoch 62/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5284 - accuracy: 0.7483 - val_loss: 0.5380 - val_accuracy: 0.7656\n",
      "Epoch 63/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5281 - accuracy: 0.7483 - val_loss: 0.5377 - val_accuracy: 0.7656\n",
      "Epoch 64/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5277 - accuracy: 0.7483 - val_loss: 0.5374 - val_accuracy: 0.7656\n",
      "Epoch 65/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5274 - accuracy: 0.7500 - val_loss: 0.5371 - val_accuracy: 0.7656\n",
      "Epoch 66/1000\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.5271 - accuracy: 0.7500 - val_loss: 0.5368 - val_accuracy: 0.7656\n",
      "Epoch 67/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.5268 - accuracy: 0.7483 - val_loss: 0.5365 - val_accuracy: 0.7656\n",
      "Epoch 68/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5265 - accuracy: 0.7483 - val_loss: 0.5362 - val_accuracy: 0.7656\n",
      "Epoch 69/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5262 - accuracy: 0.7483 - val_loss: 0.5360 - val_accuracy: 0.7708\n",
      "Epoch 70/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5259 - accuracy: 0.7483 - val_loss: 0.5357 - val_accuracy: 0.7708\n",
      "Epoch 71/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5256 - accuracy: 0.7483 - val_loss: 0.5354 - val_accuracy: 0.7708\n",
      "Epoch 72/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5253 - accuracy: 0.7465 - val_loss: 0.5351 - val_accuracy: 0.7708\n",
      "Epoch 73/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5249 - accuracy: 0.7465 - val_loss: 0.5348 - val_accuracy: 0.7708\n",
      "Epoch 74/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5246 - accuracy: 0.7500 - val_loss: 0.5345 - val_accuracy: 0.7708\n",
      "Epoch 75/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5244 - accuracy: 0.7500 - val_loss: 0.5342 - val_accuracy: 0.7708\n",
      "Epoch 76/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5241 - accuracy: 0.7500 - val_loss: 0.5340 - val_accuracy: 0.7708\n",
      "Epoch 77/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5237 - accuracy: 0.7517 - val_loss: 0.5337 - val_accuracy: 0.7708\n",
      "Epoch 78/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5235 - accuracy: 0.7535 - val_loss: 0.5334 - val_accuracy: 0.7656\n",
      "Epoch 79/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5231 - accuracy: 0.7552 - val_loss: 0.5331 - val_accuracy: 0.7656\n",
      "Epoch 80/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5229 - accuracy: 0.7552 - val_loss: 0.5329 - val_accuracy: 0.7656\n",
      "Epoch 81/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5226 - accuracy: 0.7552 - val_loss: 0.5326 - val_accuracy: 0.7604\n",
      "Epoch 82/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5223 - accuracy: 0.7552 - val_loss: 0.5323 - val_accuracy: 0.7604\n",
      "Epoch 83/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5220 - accuracy: 0.7552 - val_loss: 0.5321 - val_accuracy: 0.7604\n",
      "Epoch 84/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5217 - accuracy: 0.7552 - val_loss: 0.5318 - val_accuracy: 0.7604\n",
      "Epoch 85/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5214 - accuracy: 0.7569 - val_loss: 0.5315 - val_accuracy: 0.7656\n",
      "Epoch 86/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5211 - accuracy: 0.7569 - val_loss: 0.5313 - val_accuracy: 0.7656\n",
      "Epoch 87/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5208 - accuracy: 0.7535 - val_loss: 0.5310 - val_accuracy: 0.7656\n",
      "Epoch 88/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5206 - accuracy: 0.7535 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 89/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5203 - accuracy: 0.7569 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 90/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5200 - accuracy: 0.7569 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 91/1000\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.5197 - accuracy: 0.7569 - val_loss: 0.5300 - val_accuracy: 0.7656\n",
      "Epoch 92/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.5194 - accuracy: 0.7569 - val_loss: 0.5297 - val_accuracy: 0.7656\n",
      "Epoch 93/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5192 - accuracy: 0.7569 - val_loss: 0.5294 - val_accuracy: 0.7656\n",
      "Epoch 94/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5189 - accuracy: 0.7569 - val_loss: 0.5292 - val_accuracy: 0.7656\n",
      "Epoch 95/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5186 - accuracy: 0.7587 - val_loss: 0.5289 - val_accuracy: 0.7656\n",
      "Epoch 96/1000\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.5184 - accuracy: 0.7587 - val_loss: 0.5287 - val_accuracy: 0.7656\n",
      "Epoch 97/1000\n",
      "18/18 [==============================] - 0s 835us/step - loss: 0.5181 - accuracy: 0.7587 - val_loss: 0.5284 - val_accuracy: 0.7656\n",
      "Epoch 98/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.5178 - accuracy: 0.7587 - val_loss: 0.5282 - val_accuracy: 0.7656\n",
      "Epoch 99/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5175 - accuracy: 0.7552 - val_loss: 0.5279 - val_accuracy: 0.7656\n",
      "Epoch 100/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5173 - accuracy: 0.7569 - val_loss: 0.5277 - val_accuracy: 0.7656\n",
      "Epoch 101/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5170 - accuracy: 0.7569 - val_loss: 0.5275 - val_accuracy: 0.7656\n",
      "Epoch 102/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5167 - accuracy: 0.7552 - val_loss: 0.5272 - val_accuracy: 0.7708\n",
      "Epoch 103/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5165 - accuracy: 0.7552 - val_loss: 0.5270 - val_accuracy: 0.7708\n",
      "Epoch 104/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5162 - accuracy: 0.7552 - val_loss: 0.5267 - val_accuracy: 0.7708\n",
      "Epoch 105/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5159 - accuracy: 0.7552 - val_loss: 0.5265 - val_accuracy: 0.7708\n",
      "Epoch 106/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5157 - accuracy: 0.7552 - val_loss: 0.5263 - val_accuracy: 0.7708\n",
      "Epoch 107/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5154 - accuracy: 0.7535 - val_loss: 0.5260 - val_accuracy: 0.7708\n",
      "Epoch 108/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5152 - accuracy: 0.7535 - val_loss: 0.5258 - val_accuracy: 0.7708\n",
      "Epoch 109/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5149 - accuracy: 0.7535 - val_loss: 0.5256 - val_accuracy: 0.7708\n",
      "Epoch 110/1000\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.5147 - accuracy: 0.7535 - val_loss: 0.5253 - val_accuracy: 0.7708\n",
      "Epoch 111/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5144 - accuracy: 0.7535 - val_loss: 0.5251 - val_accuracy: 0.7708\n",
      "Epoch 112/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5141 - accuracy: 0.7552 - val_loss: 0.5249 - val_accuracy: 0.7708\n",
      "Epoch 113/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5139 - accuracy: 0.7552 - val_loss: 0.5246 - val_accuracy: 0.7708\n",
      "Epoch 114/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5136 - accuracy: 0.7552 - val_loss: 0.5244 - val_accuracy: 0.7708\n",
      "Epoch 115/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5134 - accuracy: 0.7552 - val_loss: 0.5242 - val_accuracy: 0.7708\n",
      "Epoch 116/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5132 - accuracy: 0.7569 - val_loss: 0.5240 - val_accuracy: 0.7708\n",
      "Epoch 117/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5129 - accuracy: 0.7569 - val_loss: 0.5237 - val_accuracy: 0.7708\n",
      "Epoch 118/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5126 - accuracy: 0.7569 - val_loss: 0.5235 - val_accuracy: 0.7708\n",
      "Epoch 119/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5124 - accuracy: 0.7569 - val_loss: 0.5233 - val_accuracy: 0.7708\n",
      "Epoch 120/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5121 - accuracy: 0.7569 - val_loss: 0.5231 - val_accuracy: 0.7708\n",
      "Epoch 121/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5119 - accuracy: 0.7569 - val_loss: 0.5229 - val_accuracy: 0.7708\n",
      "Epoch 122/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5117 - accuracy: 0.7569 - val_loss: 0.5226 - val_accuracy: 0.7760\n",
      "Epoch 123/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5114 - accuracy: 0.7569 - val_loss: 0.5224 - val_accuracy: 0.7708\n",
      "Epoch 124/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5112 - accuracy: 0.7587 - val_loss: 0.5222 - val_accuracy: 0.7708\n",
      "Epoch 125/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5109 - accuracy: 0.7604 - val_loss: 0.5220 - val_accuracy: 0.7708\n",
      "Epoch 126/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5107 - accuracy: 0.7604 - val_loss: 0.5218 - val_accuracy: 0.7708\n",
      "Epoch 127/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5105 - accuracy: 0.7622 - val_loss: 0.5216 - val_accuracy: 0.7708\n",
      "Epoch 128/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5102 - accuracy: 0.7622 - val_loss: 0.5214 - val_accuracy: 0.7656\n",
      "Epoch 129/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5100 - accuracy: 0.7622 - val_loss: 0.5211 - val_accuracy: 0.7656\n",
      "Epoch 130/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5098 - accuracy: 0.7622 - val_loss: 0.5209 - val_accuracy: 0.7656\n",
      "Epoch 131/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5096 - accuracy: 0.7622 - val_loss: 0.5207 - val_accuracy: 0.7656\n",
      "Epoch 132/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5093 - accuracy: 0.7622 - val_loss: 0.5205 - val_accuracy: 0.7656\n",
      "Epoch 133/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5091 - accuracy: 0.7622 - val_loss: 0.5203 - val_accuracy: 0.7708\n",
      "Epoch 134/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5089 - accuracy: 0.7604 - val_loss: 0.5201 - val_accuracy: 0.7708\n",
      "Epoch 135/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5086 - accuracy: 0.7587 - val_loss: 0.5199 - val_accuracy: 0.7708\n",
      "Epoch 136/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5084 - accuracy: 0.7587 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 137/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5082 - accuracy: 0.7604 - val_loss: 0.5195 - val_accuracy: 0.7760\n",
      "Epoch 138/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5079 - accuracy: 0.7622 - val_loss: 0.5193 - val_accuracy: 0.7760\n",
      "Epoch 139/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5077 - accuracy: 0.7604 - val_loss: 0.5191 - val_accuracy: 0.7760\n",
      "Epoch 140/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5075 - accuracy: 0.7604 - val_loss: 0.5189 - val_accuracy: 0.7760\n",
      "Epoch 141/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5073 - accuracy: 0.7604 - val_loss: 0.5187 - val_accuracy: 0.7760\n",
      "Epoch 142/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5071 - accuracy: 0.7604 - val_loss: 0.5185 - val_accuracy: 0.7760\n",
      "Epoch 143/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5068 - accuracy: 0.7604 - val_loss: 0.5183 - val_accuracy: 0.7760\n",
      "Epoch 144/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5066 - accuracy: 0.7604 - val_loss: 0.5181 - val_accuracy: 0.7760\n",
      "Epoch 145/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5064 - accuracy: 0.7604 - val_loss: 0.5179 - val_accuracy: 0.7760\n",
      "Epoch 146/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5062 - accuracy: 0.7604 - val_loss: 0.5178 - val_accuracy: 0.7760\n",
      "Epoch 147/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5060 - accuracy: 0.7604 - val_loss: 0.5176 - val_accuracy: 0.7760\n",
      "Epoch 148/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5058 - accuracy: 0.7604 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 149/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5055 - accuracy: 0.7604 - val_loss: 0.5172 - val_accuracy: 0.7760\n",
      "Epoch 150/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5053 - accuracy: 0.7604 - val_loss: 0.5170 - val_accuracy: 0.7760\n",
      "Epoch 151/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5051 - accuracy: 0.7622 - val_loss: 0.5168 - val_accuracy: 0.7760\n",
      "Epoch 152/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5049 - accuracy: 0.7622 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 153/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5047 - accuracy: 0.7622 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 154/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5045 - accuracy: 0.7622 - val_loss: 0.5163 - val_accuracy: 0.7812\n",
      "Epoch 155/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5043 - accuracy: 0.7622 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 156/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5041 - accuracy: 0.7622 - val_loss: 0.5159 - val_accuracy: 0.7812\n",
      "Epoch 157/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5039 - accuracy: 0.7622 - val_loss: 0.5157 - val_accuracy: 0.7812\n",
      "Epoch 158/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5037 - accuracy: 0.7622 - val_loss: 0.5155 - val_accuracy: 0.7760\n",
      "Epoch 159/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5035 - accuracy: 0.7622 - val_loss: 0.5154 - val_accuracy: 0.7760\n",
      "Epoch 160/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5033 - accuracy: 0.7622 - val_loss: 0.5152 - val_accuracy: 0.7760\n",
      "Epoch 161/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5031 - accuracy: 0.7639 - val_loss: 0.5150 - val_accuracy: 0.7760\n",
      "Epoch 162/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5028 - accuracy: 0.7639 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 163/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5027 - accuracy: 0.7639 - val_loss: 0.5147 - val_accuracy: 0.7760\n",
      "Epoch 164/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5024 - accuracy: 0.7639 - val_loss: 0.5145 - val_accuracy: 0.7708\n",
      "Epoch 165/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5023 - accuracy: 0.7639 - val_loss: 0.5143 - val_accuracy: 0.7708\n",
      "Epoch 166/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5021 - accuracy: 0.7656 - val_loss: 0.5142 - val_accuracy: 0.7708\n",
      "Epoch 167/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5019 - accuracy: 0.7656 - val_loss: 0.5140 - val_accuracy: 0.7708\n",
      "Epoch 168/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5017 - accuracy: 0.7639 - val_loss: 0.5138 - val_accuracy: 0.7708\n",
      "Epoch 169/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5015 - accuracy: 0.7639 - val_loss: 0.5136 - val_accuracy: 0.7708\n",
      "Epoch 170/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5013 - accuracy: 0.7656 - val_loss: 0.5135 - val_accuracy: 0.7708\n",
      "Epoch 171/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5011 - accuracy: 0.7639 - val_loss: 0.5133 - val_accuracy: 0.7708\n",
      "Epoch 172/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5009 - accuracy: 0.7639 - val_loss: 0.5131 - val_accuracy: 0.7708\n",
      "Epoch 173/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5007 - accuracy: 0.7656 - val_loss: 0.5130 - val_accuracy: 0.7708\n",
      "Epoch 174/1000\n",
      "18/18 [==============================] - 0s 1000us/step - loss: 0.5005 - accuracy: 0.7674 - val_loss: 0.5128 - val_accuracy: 0.7708\n",
      "Epoch 175/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5003 - accuracy: 0.7656 - val_loss: 0.5127 - val_accuracy: 0.7708\n",
      "Epoch 176/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5002 - accuracy: 0.7656 - val_loss: 0.5125 - val_accuracy: 0.7708\n",
      "Epoch 177/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5000 - accuracy: 0.7674 - val_loss: 0.5123 - val_accuracy: 0.7708\n",
      "Epoch 178/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4998 - accuracy: 0.7674 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 179/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4996 - accuracy: 0.7674 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 180/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4994 - accuracy: 0.7674 - val_loss: 0.5119 - val_accuracy: 0.7708\n",
      "Epoch 181/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4992 - accuracy: 0.7656 - val_loss: 0.5117 - val_accuracy: 0.7708\n",
      "Epoch 182/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.4990 - accuracy: 0.7674 - val_loss: 0.5115 - val_accuracy: 0.7708\n",
      "Epoch 183/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4988 - accuracy: 0.7674 - val_loss: 0.5114 - val_accuracy: 0.7708\n",
      "Epoch 184/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4987 - accuracy: 0.7674 - val_loss: 0.5112 - val_accuracy: 0.7708\n",
      "Epoch 185/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4985 - accuracy: 0.7674 - val_loss: 0.5111 - val_accuracy: 0.7708\n",
      "Epoch 186/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4983 - accuracy: 0.7674 - val_loss: 0.5109 - val_accuracy: 0.7708\n",
      "Epoch 187/1000\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4981 - accuracy: 0.7691 - val_loss: 0.5108 - val_accuracy: 0.7708\n",
      "Epoch 188/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4979 - accuracy: 0.7691 - val_loss: 0.5106 - val_accuracy: 0.7708\n",
      "Epoch 189/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4978 - accuracy: 0.7674 - val_loss: 0.5105 - val_accuracy: 0.7708\n",
      "Epoch 190/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4976 - accuracy: 0.7691 - val_loss: 0.5103 - val_accuracy: 0.7708\n",
      "Epoch 191/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4974 - accuracy: 0.7691 - val_loss: 0.5102 - val_accuracy: 0.7708\n",
      "Epoch 192/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4972 - accuracy: 0.7691 - val_loss: 0.5100 - val_accuracy: 0.7708\n",
      "Epoch 193/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4971 - accuracy: 0.7691 - val_loss: 0.5099 - val_accuracy: 0.7708\n",
      "Epoch 194/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4969 - accuracy: 0.7691 - val_loss: 0.5097 - val_accuracy: 0.7708\n",
      "Epoch 195/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4967 - accuracy: 0.7691 - val_loss: 0.5096 - val_accuracy: 0.7708\n",
      "Epoch 196/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4965 - accuracy: 0.7691 - val_loss: 0.5094 - val_accuracy: 0.7708\n",
      "Epoch 197/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4964 - accuracy: 0.7691 - val_loss: 0.5093 - val_accuracy: 0.7708\n",
      "Epoch 198/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4962 - accuracy: 0.7691 - val_loss: 0.5092 - val_accuracy: 0.7708\n",
      "Epoch 199/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4960 - accuracy: 0.7691 - val_loss: 0.5090 - val_accuracy: 0.7708\n",
      "Epoch 200/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4959 - accuracy: 0.7691 - val_loss: 0.5089 - val_accuracy: 0.7708\n",
      "Epoch 201/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4957 - accuracy: 0.7691 - val_loss: 0.5087 - val_accuracy: 0.7708\n",
      "Epoch 202/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4956 - accuracy: 0.7708 - val_loss: 0.5086 - val_accuracy: 0.7708\n",
      "Epoch 203/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4954 - accuracy: 0.7691 - val_loss: 0.5085 - val_accuracy: 0.7708\n",
      "Epoch 204/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4952 - accuracy: 0.7708 - val_loss: 0.5083 - val_accuracy: 0.7708\n",
      "Epoch 205/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4951 - accuracy: 0.7691 - val_loss: 0.5082 - val_accuracy: 0.7708\n",
      "Epoch 206/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4949 - accuracy: 0.7691 - val_loss: 0.5080 - val_accuracy: 0.7708\n",
      "Epoch 207/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4947 - accuracy: 0.7674 - val_loss: 0.5079 - val_accuracy: 0.7760\n",
      "Epoch 208/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4946 - accuracy: 0.7674 - val_loss: 0.5078 - val_accuracy: 0.7760\n",
      "Epoch 209/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4944 - accuracy: 0.7656 - val_loss: 0.5076 - val_accuracy: 0.7760\n",
      "Epoch 210/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4942 - accuracy: 0.7691 - val_loss: 0.5075 - val_accuracy: 0.7760\n",
      "Epoch 211/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4941 - accuracy: 0.7674 - val_loss: 0.5074 - val_accuracy: 0.7760\n",
      "Epoch 212/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4939 - accuracy: 0.7674 - val_loss: 0.5072 - val_accuracy: 0.7760\n",
      "Epoch 213/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4938 - accuracy: 0.7674 - val_loss: 0.5071 - val_accuracy: 0.7760\n",
      "Epoch 214/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4936 - accuracy: 0.7674 - val_loss: 0.5070 - val_accuracy: 0.7760\n",
      "Epoch 215/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4934 - accuracy: 0.7674 - val_loss: 0.5068 - val_accuracy: 0.7760\n",
      "Epoch 216/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4933 - accuracy: 0.7674 - val_loss: 0.5067 - val_accuracy: 0.7760\n",
      "Epoch 217/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4931 - accuracy: 0.7674 - val_loss: 0.5066 - val_accuracy: 0.7760\n",
      "Epoch 218/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4930 - accuracy: 0.7674 - val_loss: 0.5065 - val_accuracy: 0.7760\n",
      "Epoch 219/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4928 - accuracy: 0.7674 - val_loss: 0.5063 - val_accuracy: 0.7760\n",
      "Epoch 220/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4927 - accuracy: 0.7674 - val_loss: 0.5062 - val_accuracy: 0.7760\n",
      "Epoch 221/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4925 - accuracy: 0.7674 - val_loss: 0.5061 - val_accuracy: 0.7760\n",
      "Epoch 222/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4924 - accuracy: 0.7674 - val_loss: 0.5060 - val_accuracy: 0.7760\n",
      "Epoch 223/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4922 - accuracy: 0.7674 - val_loss: 0.5058 - val_accuracy: 0.7760\n",
      "Epoch 224/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4921 - accuracy: 0.7674 - val_loss: 0.5057 - val_accuracy: 0.7760\n",
      "Epoch 225/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4919 - accuracy: 0.7674 - val_loss: 0.5056 - val_accuracy: 0.7760\n",
      "Epoch 226/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4918 - accuracy: 0.7674 - val_loss: 0.5055 - val_accuracy: 0.7760\n",
      "Epoch 227/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4916 - accuracy: 0.7674 - val_loss: 0.5053 - val_accuracy: 0.7760\n",
      "Epoch 228/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4915 - accuracy: 0.7674 - val_loss: 0.5052 - val_accuracy: 0.7760\n",
      "Epoch 229/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4913 - accuracy: 0.7674 - val_loss: 0.5051 - val_accuracy: 0.7760\n",
      "Epoch 230/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4912 - accuracy: 0.7674 - val_loss: 0.5050 - val_accuracy: 0.7760\n",
      "Epoch 231/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4910 - accuracy: 0.7691 - val_loss: 0.5049 - val_accuracy: 0.7760\n",
      "Epoch 232/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4909 - accuracy: 0.7691 - val_loss: 0.5047 - val_accuracy: 0.7760\n",
      "Epoch 233/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4907 - accuracy: 0.7691 - val_loss: 0.5046 - val_accuracy: 0.7812\n",
      "Epoch 234/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4906 - accuracy: 0.7691 - val_loss: 0.5045 - val_accuracy: 0.7812\n",
      "Epoch 235/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4904 - accuracy: 0.7691 - val_loss: 0.5044 - val_accuracy: 0.7812\n",
      "Epoch 236/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4903 - accuracy: 0.7691 - val_loss: 0.5043 - val_accuracy: 0.7812\n",
      "Epoch 237/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4902 - accuracy: 0.7691 - val_loss: 0.5042 - val_accuracy: 0.7812\n",
      "Epoch 238/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4900 - accuracy: 0.7691 - val_loss: 0.5040 - val_accuracy: 0.7812\n",
      "Epoch 239/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4899 - accuracy: 0.7691 - val_loss: 0.5039 - val_accuracy: 0.7812\n",
      "Epoch 240/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4897 - accuracy: 0.7691 - val_loss: 0.5038 - val_accuracy: 0.7812\n",
      "Epoch 241/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4896 - accuracy: 0.7691 - val_loss: 0.5037 - val_accuracy: 0.7812\n",
      "Epoch 242/1000\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4895 - accuracy: 0.7691 - val_loss: 0.5036 - val_accuracy: 0.7812\n",
      "Epoch 243/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4893 - accuracy: 0.7691 - val_loss: 0.5035 - val_accuracy: 0.7812\n",
      "Epoch 244/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4892 - accuracy: 0.7691 - val_loss: 0.5034 - val_accuracy: 0.7812\n",
      "Epoch 245/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4890 - accuracy: 0.7691 - val_loss: 0.5033 - val_accuracy: 0.7812\n",
      "Epoch 246/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4889 - accuracy: 0.7691 - val_loss: 0.5032 - val_accuracy: 0.7812\n",
      "Epoch 247/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4888 - accuracy: 0.7691 - val_loss: 0.5030 - val_accuracy: 0.7812\n",
      "Epoch 248/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4887 - accuracy: 0.7691 - val_loss: 0.5029 - val_accuracy: 0.7812\n",
      "Epoch 249/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4885 - accuracy: 0.7691 - val_loss: 0.5028 - val_accuracy: 0.7812\n",
      "Epoch 250/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4884 - accuracy: 0.7691 - val_loss: 0.5027 - val_accuracy: 0.7812\n",
      "Epoch 251/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4882 - accuracy: 0.7708 - val_loss: 0.5026 - val_accuracy: 0.7812\n",
      "Epoch 252/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4881 - accuracy: 0.7708 - val_loss: 0.5025 - val_accuracy: 0.7812\n",
      "Epoch 253/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4880 - accuracy: 0.7708 - val_loss: 0.5024 - val_accuracy: 0.7812\n",
      "Epoch 254/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4878 - accuracy: 0.7708 - val_loss: 0.5023 - val_accuracy: 0.7812\n",
      "Epoch 255/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4877 - accuracy: 0.7708 - val_loss: 0.5022 - val_accuracy: 0.7812\n",
      "Epoch 256/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4876 - accuracy: 0.7708 - val_loss: 0.5021 - val_accuracy: 0.7812\n",
      "Epoch 257/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4874 - accuracy: 0.7708 - val_loss: 0.5020 - val_accuracy: 0.7812\n",
      "Epoch 258/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4873 - accuracy: 0.7708 - val_loss: 0.5019 - val_accuracy: 0.7812\n",
      "Epoch 259/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4872 - accuracy: 0.7708 - val_loss: 0.5018 - val_accuracy: 0.7812\n",
      "Epoch 260/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4871 - accuracy: 0.7708 - val_loss: 0.5017 - val_accuracy: 0.7812\n",
      "Epoch 261/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4869 - accuracy: 0.7708 - val_loss: 0.5016 - val_accuracy: 0.7760\n",
      "Epoch 262/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4868 - accuracy: 0.7708 - val_loss: 0.5015 - val_accuracy: 0.7760\n",
      "Epoch 263/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4867 - accuracy: 0.7708 - val_loss: 0.5014 - val_accuracy: 0.7760\n",
      "Epoch 264/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4866 - accuracy: 0.7726 - val_loss: 0.5013 - val_accuracy: 0.7760\n",
      "Epoch 265/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4864 - accuracy: 0.7708 - val_loss: 0.5012 - val_accuracy: 0.7708\n",
      "Epoch 266/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4863 - accuracy: 0.7726 - val_loss: 0.5011 - val_accuracy: 0.7708\n",
      "Epoch 267/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4862 - accuracy: 0.7708 - val_loss: 0.5010 - val_accuracy: 0.7708\n",
      "Epoch 268/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4861 - accuracy: 0.7726 - val_loss: 0.5009 - val_accuracy: 0.7708\n",
      "Epoch 269/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4860 - accuracy: 0.7708 - val_loss: 0.5008 - val_accuracy: 0.7708\n",
      "Epoch 270/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4858 - accuracy: 0.7726 - val_loss: 0.5007 - val_accuracy: 0.7708\n",
      "Epoch 271/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4857 - accuracy: 0.7726 - val_loss: 0.5006 - val_accuracy: 0.7708\n",
      "Epoch 272/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4856 - accuracy: 0.7726 - val_loss: 0.5005 - val_accuracy: 0.7708\n",
      "Epoch 273/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4855 - accuracy: 0.7726 - val_loss: 0.5004 - val_accuracy: 0.7708\n",
      "Epoch 274/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4853 - accuracy: 0.7726 - val_loss: 0.5003 - val_accuracy: 0.7708\n",
      "Epoch 275/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4852 - accuracy: 0.7726 - val_loss: 0.5002 - val_accuracy: 0.7708\n",
      "Epoch 276/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4851 - accuracy: 0.7708 - val_loss: 0.5001 - val_accuracy: 0.7708\n",
      "Epoch 277/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4850 - accuracy: 0.7708 - val_loss: 0.5001 - val_accuracy: 0.7708\n",
      "Epoch 278/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4849 - accuracy: 0.7726 - val_loss: 0.5000 - val_accuracy: 0.7708\n",
      "Epoch 279/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4847 - accuracy: 0.7708 - val_loss: 0.4999 - val_accuracy: 0.7708\n",
      "Epoch 280/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4847 - accuracy: 0.7726 - val_loss: 0.4998 - val_accuracy: 0.7708\n",
      "Epoch 281/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4845 - accuracy: 0.7708 - val_loss: 0.4997 - val_accuracy: 0.7708\n",
      "Epoch 282/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4844 - accuracy: 0.7708 - val_loss: 0.4996 - val_accuracy: 0.7656\n",
      "Epoch 283/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4843 - accuracy: 0.7708 - val_loss: 0.4995 - val_accuracy: 0.7656\n",
      "Epoch 284/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4842 - accuracy: 0.7708 - val_loss: 0.4994 - val_accuracy: 0.7656\n",
      "Epoch 285/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4841 - accuracy: 0.7708 - val_loss: 0.4993 - val_accuracy: 0.7656\n",
      "Epoch 286/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4839 - accuracy: 0.7708 - val_loss: 0.4992 - val_accuracy: 0.7656\n",
      "Epoch 287/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4838 - accuracy: 0.7708 - val_loss: 0.4992 - val_accuracy: 0.7656\n",
      "Epoch 288/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4837 - accuracy: 0.7708 - val_loss: 0.4991 - val_accuracy: 0.7656\n",
      "Epoch 289/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4836 - accuracy: 0.7708 - val_loss: 0.4990 - val_accuracy: 0.7656\n",
      "Epoch 290/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4835 - accuracy: 0.7708 - val_loss: 0.4989 - val_accuracy: 0.7656\n",
      "Epoch 291/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4834 - accuracy: 0.7708 - val_loss: 0.4988 - val_accuracy: 0.7656\n",
      "Epoch 292/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4833 - accuracy: 0.7708 - val_loss: 0.4987 - val_accuracy: 0.7656\n",
      "Epoch 293/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4832 - accuracy: 0.7708 - val_loss: 0.4986 - val_accuracy: 0.7656\n",
      "Epoch 294/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4831 - accuracy: 0.7708 - val_loss: 0.4986 - val_accuracy: 0.7656\n",
      "Epoch 295/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4830 - accuracy: 0.7708 - val_loss: 0.4985 - val_accuracy: 0.7656\n",
      "Epoch 296/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4829 - accuracy: 0.7708 - val_loss: 0.4984 - val_accuracy: 0.7656\n",
      "Epoch 297/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4827 - accuracy: 0.7708 - val_loss: 0.4983 - val_accuracy: 0.7656\n",
      "Epoch 298/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4826 - accuracy: 0.7708 - val_loss: 0.4982 - val_accuracy: 0.7656\n",
      "Epoch 299/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4825 - accuracy: 0.7708 - val_loss: 0.4982 - val_accuracy: 0.7656\n",
      "Epoch 300/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4824 - accuracy: 0.7708 - val_loss: 0.4981 - val_accuracy: 0.7656\n",
      "Epoch 301/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4823 - accuracy: 0.7708 - val_loss: 0.4980 - val_accuracy: 0.7656\n",
      "Epoch 302/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4822 - accuracy: 0.7708 - val_loss: 0.4979 - val_accuracy: 0.7656\n",
      "Epoch 303/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4821 - accuracy: 0.7708 - val_loss: 0.4978 - val_accuracy: 0.7656\n",
      "Epoch 304/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4820 - accuracy: 0.7708 - val_loss: 0.4978 - val_accuracy: 0.7656\n",
      "Epoch 305/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4819 - accuracy: 0.7708 - val_loss: 0.4977 - val_accuracy: 0.7656\n",
      "Epoch 306/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4818 - accuracy: 0.7708 - val_loss: 0.4976 - val_accuracy: 0.7656\n",
      "Epoch 307/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4817 - accuracy: 0.7708 - val_loss: 0.4975 - val_accuracy: 0.7656\n",
      "Epoch 308/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4816 - accuracy: 0.7708 - val_loss: 0.4974 - val_accuracy: 0.7656\n",
      "Epoch 309/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4815 - accuracy: 0.7708 - val_loss: 0.4974 - val_accuracy: 0.7656\n",
      "Epoch 310/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4814 - accuracy: 0.7708 - val_loss: 0.4973 - val_accuracy: 0.7656\n",
      "Epoch 311/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4813 - accuracy: 0.7708 - val_loss: 0.4972 - val_accuracy: 0.7656\n",
      "Epoch 312/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4812 - accuracy: 0.7708 - val_loss: 0.4971 - val_accuracy: 0.7656\n",
      "Epoch 313/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4811 - accuracy: 0.7708 - val_loss: 0.4971 - val_accuracy: 0.7656\n",
      "Epoch 314/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4810 - accuracy: 0.7708 - val_loss: 0.4970 - val_accuracy: 0.7656\n",
      "Epoch 315/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4809 - accuracy: 0.7708 - val_loss: 0.4969 - val_accuracy: 0.7656\n",
      "Epoch 316/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.4808 - accuracy: 0.7708 - val_loss: 0.4968 - val_accuracy: 0.7656\n",
      "Epoch 317/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4807 - accuracy: 0.7708 - val_loss: 0.4968 - val_accuracy: 0.7656\n",
      "Epoch 318/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4806 - accuracy: 0.7708 - val_loss: 0.4967 - val_accuracy: 0.7656\n",
      "Epoch 319/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4805 - accuracy: 0.7708 - val_loss: 0.4966 - val_accuracy: 0.7656\n",
      "Epoch 320/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4804 - accuracy: 0.7708 - val_loss: 0.4965 - val_accuracy: 0.7656\n",
      "Epoch 321/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4803 - accuracy: 0.7708 - val_loss: 0.4965 - val_accuracy: 0.7656\n",
      "Epoch 322/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4802 - accuracy: 0.7708 - val_loss: 0.4964 - val_accuracy: 0.7656\n",
      "Epoch 323/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4801 - accuracy: 0.7708 - val_loss: 0.4963 - val_accuracy: 0.7656\n",
      "Epoch 324/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4800 - accuracy: 0.7708 - val_loss: 0.4963 - val_accuracy: 0.7656\n",
      "Epoch 325/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4799 - accuracy: 0.7708 - val_loss: 0.4962 - val_accuracy: 0.7656\n",
      "Epoch 326/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4798 - accuracy: 0.7708 - val_loss: 0.4961 - val_accuracy: 0.7656\n",
      "Epoch 327/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4797 - accuracy: 0.7708 - val_loss: 0.4961 - val_accuracy: 0.7656\n",
      "Epoch 328/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4796 - accuracy: 0.7708 - val_loss: 0.4960 - val_accuracy: 0.7656\n",
      "Epoch 329/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4795 - accuracy: 0.7708 - val_loss: 0.4959 - val_accuracy: 0.7708\n",
      "Epoch 330/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4794 - accuracy: 0.7708 - val_loss: 0.4958 - val_accuracy: 0.7708\n",
      "Epoch 331/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4793 - accuracy: 0.7708 - val_loss: 0.4958 - val_accuracy: 0.7708\n",
      "Epoch 332/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4793 - accuracy: 0.7708 - val_loss: 0.4957 - val_accuracy: 0.7708\n",
      "Epoch 333/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4791 - accuracy: 0.7708 - val_loss: 0.4956 - val_accuracy: 0.7708\n",
      "Epoch 334/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4790 - accuracy: 0.7708 - val_loss: 0.4956 - val_accuracy: 0.7708\n",
      "Epoch 335/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4790 - accuracy: 0.7708 - val_loss: 0.4955 - val_accuracy: 0.7708\n",
      "Epoch 336/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4789 - accuracy: 0.7708 - val_loss: 0.4954 - val_accuracy: 0.7708\n",
      "Epoch 337/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4788 - accuracy: 0.7708 - val_loss: 0.4954 - val_accuracy: 0.7708\n",
      "Epoch 338/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4787 - accuracy: 0.7726 - val_loss: 0.4953 - val_accuracy: 0.7708\n",
      "Epoch 339/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4786 - accuracy: 0.7726 - val_loss: 0.4952 - val_accuracy: 0.7708\n",
      "Epoch 340/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4785 - accuracy: 0.7708 - val_loss: 0.4952 - val_accuracy: 0.7708\n",
      "Epoch 341/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4784 - accuracy: 0.7726 - val_loss: 0.4951 - val_accuracy: 0.7708\n",
      "Epoch 342/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4783 - accuracy: 0.7726 - val_loss: 0.4951 - val_accuracy: 0.7708\n",
      "Epoch 343/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4782 - accuracy: 0.7726 - val_loss: 0.4950 - val_accuracy: 0.7708\n",
      "Epoch 344/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4782 - accuracy: 0.7726 - val_loss: 0.4949 - val_accuracy: 0.7708\n",
      "Epoch 345/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4781 - accuracy: 0.7726 - val_loss: 0.4949 - val_accuracy: 0.7708\n",
      "Epoch 346/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4780 - accuracy: 0.7726 - val_loss: 0.4948 - val_accuracy: 0.7708\n",
      "Epoch 347/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4779 - accuracy: 0.7726 - val_loss: 0.4947 - val_accuracy: 0.7708\n",
      "Epoch 348/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4778 - accuracy: 0.7726 - val_loss: 0.4947 - val_accuracy: 0.7708\n",
      "Epoch 349/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4777 - accuracy: 0.7726 - val_loss: 0.4946 - val_accuracy: 0.7708\n",
      "Epoch 350/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4776 - accuracy: 0.7726 - val_loss: 0.4946 - val_accuracy: 0.7708\n",
      "Epoch 351/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4776 - accuracy: 0.7726 - val_loss: 0.4945 - val_accuracy: 0.7708\n",
      "Epoch 352/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4775 - accuracy: 0.7726 - val_loss: 0.4944 - val_accuracy: 0.7708\n",
      "Epoch 353/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4774 - accuracy: 0.7726 - val_loss: 0.4944 - val_accuracy: 0.7708\n",
      "Epoch 354/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4773 - accuracy: 0.7726 - val_loss: 0.4943 - val_accuracy: 0.7708\n",
      "Epoch 355/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4772 - accuracy: 0.7726 - val_loss: 0.4943 - val_accuracy: 0.7760\n",
      "Epoch 356/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4772 - accuracy: 0.7726 - val_loss: 0.4942 - val_accuracy: 0.7760\n",
      "Epoch 357/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4771 - accuracy: 0.7726 - val_loss: 0.4941 - val_accuracy: 0.7760\n",
      "Epoch 358/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4770 - accuracy: 0.7726 - val_loss: 0.4941 - val_accuracy: 0.7760\n",
      "Epoch 359/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4769 - accuracy: 0.7726 - val_loss: 0.4940 - val_accuracy: 0.7760\n",
      "Epoch 360/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4768 - accuracy: 0.7726 - val_loss: 0.4940 - val_accuracy: 0.7760\n",
      "Epoch 361/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4767 - accuracy: 0.7726 - val_loss: 0.4939 - val_accuracy: 0.7760\n",
      "Epoch 362/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4767 - accuracy: 0.7726 - val_loss: 0.4939 - val_accuracy: 0.7760\n",
      "Epoch 363/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4766 - accuracy: 0.7726 - val_loss: 0.4938 - val_accuracy: 0.7760\n",
      "Epoch 364/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4765 - accuracy: 0.7726 - val_loss: 0.4937 - val_accuracy: 0.7760\n",
      "Epoch 365/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4764 - accuracy: 0.7743 - val_loss: 0.4937 - val_accuracy: 0.7760\n",
      "Epoch 366/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4763 - accuracy: 0.7726 - val_loss: 0.4936 - val_accuracy: 0.7760\n",
      "Epoch 367/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4762 - accuracy: 0.7726 - val_loss: 0.4936 - val_accuracy: 0.7760\n",
      "Epoch 368/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4762 - accuracy: 0.7743 - val_loss: 0.4935 - val_accuracy: 0.7760\n",
      "Epoch 369/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4761 - accuracy: 0.7743 - val_loss: 0.4935 - val_accuracy: 0.7760\n",
      "Epoch 370/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.4760 - accuracy: 0.7743 - val_loss: 0.4934 - val_accuracy: 0.7760\n",
      "Epoch 371/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4759 - accuracy: 0.7743 - val_loss: 0.4934 - val_accuracy: 0.7760\n",
      "Epoch 372/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4759 - accuracy: 0.7743 - val_loss: 0.4933 - val_accuracy: 0.7760\n",
      "Epoch 373/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4758 - accuracy: 0.7743 - val_loss: 0.4932 - val_accuracy: 0.7708\n",
      "Epoch 374/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4757 - accuracy: 0.7743 - val_loss: 0.4932 - val_accuracy: 0.7708\n",
      "Epoch 375/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4756 - accuracy: 0.7743 - val_loss: 0.4931 - val_accuracy: 0.7708\n",
      "Epoch 376/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4755 - accuracy: 0.7743 - val_loss: 0.4931 - val_accuracy: 0.7708\n",
      "Epoch 377/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4755 - accuracy: 0.7743 - val_loss: 0.4930 - val_accuracy: 0.7708\n",
      "Epoch 378/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4754 - accuracy: 0.7743 - val_loss: 0.4930 - val_accuracy: 0.7708\n",
      "Epoch 379/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4753 - accuracy: 0.7743 - val_loss: 0.4929 - val_accuracy: 0.7708\n",
      "Epoch 380/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4753 - accuracy: 0.7743 - val_loss: 0.4929 - val_accuracy: 0.7708\n",
      "Epoch 381/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4752 - accuracy: 0.7743 - val_loss: 0.4928 - val_accuracy: 0.7708\n",
      "Epoch 382/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4751 - accuracy: 0.7743 - val_loss: 0.4928 - val_accuracy: 0.7708\n",
      "Epoch 383/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.4750 - accuracy: 0.7743 - val_loss: 0.4927 - val_accuracy: 0.7708\n",
      "Epoch 384/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4750 - accuracy: 0.7743 - val_loss: 0.4927 - val_accuracy: 0.7708\n",
      "Epoch 385/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4749 - accuracy: 0.7743 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
      "Epoch 386/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4748 - accuracy: 0.7760 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
      "Epoch 387/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4747 - accuracy: 0.7743 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
      "Epoch 388/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4747 - accuracy: 0.7760 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
      "Epoch 389/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4746 - accuracy: 0.7778 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
      "Epoch 390/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4745 - accuracy: 0.7778 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
      "Epoch 391/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4745 - accuracy: 0.7778 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
      "Epoch 392/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4744 - accuracy: 0.7778 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
      "Epoch 393/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4743 - accuracy: 0.7778 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
      "Epoch 394/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4743 - accuracy: 0.7778 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
      "Epoch 395/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4742 - accuracy: 0.7778 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
      "Epoch 396/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4741 - accuracy: 0.7778 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
      "Epoch 397/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4740 - accuracy: 0.7778 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
      "Epoch 398/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4740 - accuracy: 0.7778 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
      "Epoch 399/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4739 - accuracy: 0.7778 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
      "Epoch 400/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4739 - accuracy: 0.7760 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
      "Epoch 401/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4738 - accuracy: 0.7778 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
      "Epoch 402/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4737 - accuracy: 0.7778 - val_loss: 0.4918 - val_accuracy: 0.7708\n",
      "Epoch 403/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4736 - accuracy: 0.7760 - val_loss: 0.4918 - val_accuracy: 0.7708\n",
      "Epoch 404/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4736 - accuracy: 0.7778 - val_loss: 0.4917 - val_accuracy: 0.7708\n",
      "Epoch 405/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4735 - accuracy: 0.7760 - val_loss: 0.4917 - val_accuracy: 0.7708\n",
      "Epoch 406/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4734 - accuracy: 0.7760 - val_loss: 0.4916 - val_accuracy: 0.7708\n",
      "Epoch 407/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4733 - accuracy: 0.7760 - val_loss: 0.4916 - val_accuracy: 0.7708\n",
      "Epoch 408/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4733 - accuracy: 0.7760 - val_loss: 0.4915 - val_accuracy: 0.7708\n",
      "Epoch 409/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4732 - accuracy: 0.7760 - val_loss: 0.4915 - val_accuracy: 0.7708\n",
      "Epoch 410/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4732 - accuracy: 0.7778 - val_loss: 0.4915 - val_accuracy: 0.7708\n",
      "Epoch 411/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4731 - accuracy: 0.7778 - val_loss: 0.4914 - val_accuracy: 0.7708\n",
      "Epoch 412/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4730 - accuracy: 0.7778 - val_loss: 0.4914 - val_accuracy: 0.7708\n",
      "Epoch 413/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4730 - accuracy: 0.7778 - val_loss: 0.4913 - val_accuracy: 0.7708\n",
      "Epoch 414/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4729 - accuracy: 0.7778 - val_loss: 0.4913 - val_accuracy: 0.7708\n",
      "Epoch 415/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4728 - accuracy: 0.7778 - val_loss: 0.4912 - val_accuracy: 0.7708\n",
      "Epoch 416/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4728 - accuracy: 0.7778 - val_loss: 0.4912 - val_accuracy: 0.7708\n",
      "Epoch 417/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4727 - accuracy: 0.7778 - val_loss: 0.4912 - val_accuracy: 0.7708\n",
      "Epoch 418/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4727 - accuracy: 0.7778 - val_loss: 0.4911 - val_accuracy: 0.7708\n",
      "Epoch 419/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4726 - accuracy: 0.7778 - val_loss: 0.4911 - val_accuracy: 0.7708\n",
      "Epoch 420/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4725 - accuracy: 0.7778 - val_loss: 0.4910 - val_accuracy: 0.7708\n",
      "Epoch 421/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4725 - accuracy: 0.7778 - val_loss: 0.4910 - val_accuracy: 0.7708\n",
      "Epoch 422/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4724 - accuracy: 0.7778 - val_loss: 0.4910 - val_accuracy: 0.7708\n",
      "Epoch 423/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4723 - accuracy: 0.7778 - val_loss: 0.4909 - val_accuracy: 0.7708\n",
      "Epoch 424/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4723 - accuracy: 0.7760 - val_loss: 0.4909 - val_accuracy: 0.7708\n",
      "Epoch 425/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4722 - accuracy: 0.7760 - val_loss: 0.4908 - val_accuracy: 0.7708\n",
      "Epoch 426/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4722 - accuracy: 0.7760 - val_loss: 0.4908 - val_accuracy: 0.7708\n",
      "Epoch 427/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4721 - accuracy: 0.7760 - val_loss: 0.4908 - val_accuracy: 0.7708\n",
      "Epoch 428/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4720 - accuracy: 0.7760 - val_loss: 0.4907 - val_accuracy: 0.7708\n",
      "Epoch 429/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4720 - accuracy: 0.7760 - val_loss: 0.4907 - val_accuracy: 0.7708\n",
      "Epoch 430/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4719 - accuracy: 0.7760 - val_loss: 0.4906 - val_accuracy: 0.7708\n",
      "Epoch 431/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4719 - accuracy: 0.7778 - val_loss: 0.4906 - val_accuracy: 0.7656\n",
      "Epoch 432/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4718 - accuracy: 0.7778 - val_loss: 0.4906 - val_accuracy: 0.7656\n",
      "Epoch 433/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4717 - accuracy: 0.7778 - val_loss: 0.4905 - val_accuracy: 0.7656\n",
      "Epoch 434/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4717 - accuracy: 0.7778 - val_loss: 0.4905 - val_accuracy: 0.7656\n",
      "Epoch 435/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4716 - accuracy: 0.7778 - val_loss: 0.4904 - val_accuracy: 0.7656\n",
      "Epoch 436/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4716 - accuracy: 0.7778 - val_loss: 0.4904 - val_accuracy: 0.7656\n",
      "Epoch 437/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4715 - accuracy: 0.7778 - val_loss: 0.4904 - val_accuracy: 0.7656\n",
      "Epoch 438/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4714 - accuracy: 0.7778 - val_loss: 0.4903 - val_accuracy: 0.7656\n",
      "Epoch 439/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4714 - accuracy: 0.7778 - val_loss: 0.4903 - val_accuracy: 0.7656\n",
      "Epoch 440/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4713 - accuracy: 0.7778 - val_loss: 0.4903 - val_accuracy: 0.7656\n",
      "Epoch 441/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4713 - accuracy: 0.7778 - val_loss: 0.4902 - val_accuracy: 0.7656\n",
      "Epoch 442/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4712 - accuracy: 0.7778 - val_loss: 0.4902 - val_accuracy: 0.7656\n",
      "Epoch 443/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4711 - accuracy: 0.7778 - val_loss: 0.4902 - val_accuracy: 0.7656\n",
      "Epoch 444/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4711 - accuracy: 0.7778 - val_loss: 0.4901 - val_accuracy: 0.7656\n",
      "Epoch 445/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4710 - accuracy: 0.7778 - val_loss: 0.4901 - val_accuracy: 0.7656\n",
      "Epoch 446/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4710 - accuracy: 0.7778 - val_loss: 0.4900 - val_accuracy: 0.7656\n",
      "Epoch 447/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4709 - accuracy: 0.7778 - val_loss: 0.4900 - val_accuracy: 0.7656\n",
      "Epoch 448/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4709 - accuracy: 0.7778 - val_loss: 0.4900 - val_accuracy: 0.7656\n",
      "Epoch 449/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4708 - accuracy: 0.7778 - val_loss: 0.4899 - val_accuracy: 0.7656\n",
      "Epoch 450/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4708 - accuracy: 0.7778 - val_loss: 0.4899 - val_accuracy: 0.7656\n",
      "Epoch 451/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4707 - accuracy: 0.7778 - val_loss: 0.4899 - val_accuracy: 0.7656\n",
      "Epoch 452/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4707 - accuracy: 0.7778 - val_loss: 0.4898 - val_accuracy: 0.7656\n",
      "Epoch 453/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4706 - accuracy: 0.7812 - val_loss: 0.4898 - val_accuracy: 0.7656\n",
      "Epoch 454/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4705 - accuracy: 0.7778 - val_loss: 0.4898 - val_accuracy: 0.7656\n",
      "Epoch 455/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4705 - accuracy: 0.7778 - val_loss: 0.4897 - val_accuracy: 0.7656\n",
      "Epoch 456/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4705 - accuracy: 0.7778 - val_loss: 0.4897 - val_accuracy: 0.7656\n",
      "Epoch 457/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4704 - accuracy: 0.7778 - val_loss: 0.4897 - val_accuracy: 0.7656\n",
      "Epoch 458/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4703 - accuracy: 0.7778 - val_loss: 0.4896 - val_accuracy: 0.7656\n",
      "Epoch 459/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4703 - accuracy: 0.7812 - val_loss: 0.4896 - val_accuracy: 0.7656\n",
      "Epoch 460/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4702 - accuracy: 0.7778 - val_loss: 0.4896 - val_accuracy: 0.7656\n",
      "Epoch 461/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4702 - accuracy: 0.7812 - val_loss: 0.4895 - val_accuracy: 0.7656\n",
      "Epoch 462/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4701 - accuracy: 0.7812 - val_loss: 0.4895 - val_accuracy: 0.7656\n",
      "Epoch 463/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4701 - accuracy: 0.7795 - val_loss: 0.4895 - val_accuracy: 0.7656\n",
      "Epoch 464/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4700 - accuracy: 0.7812 - val_loss: 0.4894 - val_accuracy: 0.7656\n",
      "Epoch 465/1000\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4700 - accuracy: 0.7812 - val_loss: 0.4894 - val_accuracy: 0.7656\n",
      "Epoch 466/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4699 - accuracy: 0.7830 - val_loss: 0.4894 - val_accuracy: 0.7656\n",
      "Epoch 467/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4699 - accuracy: 0.7830 - val_loss: 0.4893 - val_accuracy: 0.7656\n",
      "Epoch 468/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4698 - accuracy: 0.7812 - val_loss: 0.4893 - val_accuracy: 0.7656\n",
      "Epoch 469/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4698 - accuracy: 0.7812 - val_loss: 0.4893 - val_accuracy: 0.7656\n",
      "Epoch 470/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4697 - accuracy: 0.7830 - val_loss: 0.4893 - val_accuracy: 0.7656\n",
      "Epoch 471/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4697 - accuracy: 0.7830 - val_loss: 0.4892 - val_accuracy: 0.7656\n",
      "Epoch 472/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4696 - accuracy: 0.7830 - val_loss: 0.4892 - val_accuracy: 0.7656\n",
      "Epoch 473/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4696 - accuracy: 0.7830 - val_loss: 0.4892 - val_accuracy: 0.7656\n",
      "Epoch 474/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4695 - accuracy: 0.7830 - val_loss: 0.4891 - val_accuracy: 0.7656\n",
      "Epoch 475/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4695 - accuracy: 0.7830 - val_loss: 0.4891 - val_accuracy: 0.7656\n",
      "Epoch 476/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4694 - accuracy: 0.7830 - val_loss: 0.4891 - val_accuracy: 0.7656\n",
      "Epoch 477/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4694 - accuracy: 0.7830 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 478/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4693 - accuracy: 0.7830 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 479/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4693 - accuracy: 0.7830 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 480/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4692 - accuracy: 0.7830 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 481/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4692 - accuracy: 0.7830 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 482/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4691 - accuracy: 0.7830 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 483/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4691 - accuracy: 0.7830 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 484/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4690 - accuracy: 0.7812 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 485/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4690 - accuracy: 0.7830 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 486/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4690 - accuracy: 0.7830 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 487/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4689 - accuracy: 0.7830 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 488/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4688 - accuracy: 0.7830 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 489/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4688 - accuracy: 0.7830 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 490/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4687 - accuracy: 0.7830 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 491/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4687 - accuracy: 0.7830 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 492/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4687 - accuracy: 0.7830 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 493/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4686 - accuracy: 0.7830 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 494/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4686 - accuracy: 0.7830 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 495/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4685 - accuracy: 0.7830 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 496/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4685 - accuracy: 0.7812 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 497/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4684 - accuracy: 0.7812 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 498/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4684 - accuracy: 0.7830 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 499/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4683 - accuracy: 0.7812 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 500/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4683 - accuracy: 0.7830 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 501/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4682 - accuracy: 0.7830 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 502/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4682 - accuracy: 0.7830 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 503/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4681 - accuracy: 0.7830 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 504/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4681 - accuracy: 0.7830 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 505/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4681 - accuracy: 0.7830 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 506/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4680 - accuracy: 0.7847 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 507/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4680 - accuracy: 0.7830 - val_loss: 0.4882 - val_accuracy: 0.7656\n",
      "Epoch 508/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4679 - accuracy: 0.7830 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 509/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4679 - accuracy: 0.7847 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 510/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4679 - accuracy: 0.7847 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 511/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4678 - accuracy: 0.7847 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 512/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4678 - accuracy: 0.7847 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 513/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4677 - accuracy: 0.7847 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 514/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4677 - accuracy: 0.7865 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 515/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4677 - accuracy: 0.7865 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 516/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4676 - accuracy: 0.7865 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 517/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4675 - accuracy: 0.7865 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 518/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4675 - accuracy: 0.7865 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 519/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4675 - accuracy: 0.7865 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 520/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4674 - accuracy: 0.7847 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 521/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4674 - accuracy: 0.7865 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 522/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4674 - accuracy: 0.7865 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 523/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4673 - accuracy: 0.7865 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 524/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4673 - accuracy: 0.7865 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 525/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4672 - accuracy: 0.7865 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 526/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4672 - accuracy: 0.7865 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 527/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4672 - accuracy: 0.7865 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 528/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4671 - accuracy: 0.7865 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 529/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4671 - accuracy: 0.7865 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 530/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4670 - accuracy: 0.7865 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 531/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4670 - accuracy: 0.7865 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 532/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4670 - accuracy: 0.7865 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 533/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4669 - accuracy: 0.7865 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 534/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4669 - accuracy: 0.7865 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 535/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4668 - accuracy: 0.7865 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 536/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4668 - accuracy: 0.7865 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 537/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4668 - accuracy: 0.7865 - val_loss: 0.4875 - val_accuracy: 0.7604\n",
      "Epoch 538/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4667 - accuracy: 0.7865 - val_loss: 0.4875 - val_accuracy: 0.7604\n",
      "Epoch 539/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4667 - accuracy: 0.7847 - val_loss: 0.4875 - val_accuracy: 0.7604\n",
      "Epoch 540/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4667 - accuracy: 0.7847 - val_loss: 0.4875 - val_accuracy: 0.7604\n",
      "Epoch 541/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4666 - accuracy: 0.7865 - val_loss: 0.4875 - val_accuracy: 0.7604\n",
      "Epoch 542/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4666 - accuracy: 0.7865 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 543/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4665 - accuracy: 0.7865 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 544/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4665 - accuracy: 0.7865 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 545/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4665 - accuracy: 0.7865 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 546/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4664 - accuracy: 0.7865 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 547/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4664 - accuracy: 0.7865 - val_loss: 0.4873 - val_accuracy: 0.7604\n",
      "Epoch 548/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4663 - accuracy: 0.7865 - val_loss: 0.4873 - val_accuracy: 0.7604\n",
      "Epoch 549/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4663 - accuracy: 0.7865 - val_loss: 0.4873 - val_accuracy: 0.7604\n",
      "Epoch 550/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4663 - accuracy: 0.7865 - val_loss: 0.4873 - val_accuracy: 0.7604\n",
      "Epoch 551/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4662 - accuracy: 0.7865 - val_loss: 0.4873 - val_accuracy: 0.7604\n",
      "Epoch 552/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4662 - accuracy: 0.7865 - val_loss: 0.4873 - val_accuracy: 0.7604\n",
      "Epoch 553/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4662 - accuracy: 0.7865 - val_loss: 0.4872 - val_accuracy: 0.7604\n",
      "Epoch 554/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4661 - accuracy: 0.7865 - val_loss: 0.4872 - val_accuracy: 0.7604\n",
      "Epoch 555/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4661 - accuracy: 0.7865 - val_loss: 0.4872 - val_accuracy: 0.7604\n",
      "Epoch 556/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4661 - accuracy: 0.7865 - val_loss: 0.4872 - val_accuracy: 0.7604\n",
      "Epoch 557/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4660 - accuracy: 0.7865 - val_loss: 0.4872 - val_accuracy: 0.7604\n",
      "Epoch 558/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4660 - accuracy: 0.7865 - val_loss: 0.4871 - val_accuracy: 0.7604\n",
      "Epoch 559/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4660 - accuracy: 0.7865 - val_loss: 0.4871 - val_accuracy: 0.7604\n",
      "Epoch 560/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4659 - accuracy: 0.7865 - val_loss: 0.4871 - val_accuracy: 0.7604\n",
      "Epoch 561/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4659 - accuracy: 0.7865 - val_loss: 0.4871 - val_accuracy: 0.7604\n",
      "Epoch 562/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4659 - accuracy: 0.7865 - val_loss: 0.4871 - val_accuracy: 0.7604\n",
      "Epoch 563/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4658 - accuracy: 0.7865 - val_loss: 0.4870 - val_accuracy: 0.7604\n",
      "Epoch 564/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4658 - accuracy: 0.7865 - val_loss: 0.4870 - val_accuracy: 0.7604\n",
      "Epoch 565/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4657 - accuracy: 0.7865 - val_loss: 0.4870 - val_accuracy: 0.7604\n",
      "Epoch 566/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4657 - accuracy: 0.7865 - val_loss: 0.4870 - val_accuracy: 0.7604\n",
      "Epoch 567/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4657 - accuracy: 0.7865 - val_loss: 0.4870 - val_accuracy: 0.7604\n",
      "Epoch 568/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4656 - accuracy: 0.7865 - val_loss: 0.4870 - val_accuracy: 0.7604\n",
      "Epoch 569/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4656 - accuracy: 0.7865 - val_loss: 0.4869 - val_accuracy: 0.7604\n",
      "Epoch 570/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4656 - accuracy: 0.7865 - val_loss: 0.4869 - val_accuracy: 0.7604\n",
      "Epoch 571/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4655 - accuracy: 0.7865 - val_loss: 0.4869 - val_accuracy: 0.7604\n",
      "Epoch 572/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4655 - accuracy: 0.7865 - val_loss: 0.4869 - val_accuracy: 0.7604\n",
      "Epoch 573/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4655 - accuracy: 0.7865 - val_loss: 0.4869 - val_accuracy: 0.7604\n",
      "Epoch 574/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4654 - accuracy: 0.7882 - val_loss: 0.4869 - val_accuracy: 0.7604\n",
      "Epoch 575/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4654 - accuracy: 0.7882 - val_loss: 0.4868 - val_accuracy: 0.7604\n",
      "Epoch 576/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4654 - accuracy: 0.7882 - val_loss: 0.4868 - val_accuracy: 0.7604\n",
      "Epoch 577/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4653 - accuracy: 0.7882 - val_loss: 0.4868 - val_accuracy: 0.7604\n",
      "Epoch 578/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4653 - accuracy: 0.7882 - val_loss: 0.4868 - val_accuracy: 0.7604\n",
      "Epoch 579/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4653 - accuracy: 0.7882 - val_loss: 0.4868 - val_accuracy: 0.7604\n",
      "Epoch 580/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4653 - accuracy: 0.7882 - val_loss: 0.4868 - val_accuracy: 0.7604\n",
      "Epoch 581/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4652 - accuracy: 0.7882 - val_loss: 0.4867 - val_accuracy: 0.7604\n",
      "Epoch 582/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4652 - accuracy: 0.7882 - val_loss: 0.4867 - val_accuracy: 0.7604\n",
      "Epoch 583/1000\n",
      "18/18 [==============================] - 0s 944us/step - loss: 0.4651 - accuracy: 0.7882 - val_loss: 0.4867 - val_accuracy: 0.7604\n",
      "Epoch 584/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4651 - accuracy: 0.7882 - val_loss: 0.4867 - val_accuracy: 0.7604\n",
      "Epoch 585/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4651 - accuracy: 0.7882 - val_loss: 0.4867 - val_accuracy: 0.7604\n",
      "Epoch 586/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4651 - accuracy: 0.7882 - val_loss: 0.4867 - val_accuracy: 0.7604\n",
      "Epoch 587/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4650 - accuracy: 0.7882 - val_loss: 0.4866 - val_accuracy: 0.7604\n",
      "Epoch 588/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4650 - accuracy: 0.7882 - val_loss: 0.4866 - val_accuracy: 0.7604\n",
      "Epoch 589/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4650 - accuracy: 0.7882 - val_loss: 0.4866 - val_accuracy: 0.7604\n",
      "Epoch 590/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4649 - accuracy: 0.7882 - val_loss: 0.4866 - val_accuracy: 0.7604\n",
      "Epoch 591/1000\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4649 - accuracy: 0.7882 - val_loss: 0.4866 - val_accuracy: 0.7604\n",
      "Epoch 592/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4649 - accuracy: 0.7882 - val_loss: 0.4866 - val_accuracy: 0.7604\n",
      "Epoch 593/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4648 - accuracy: 0.7882 - val_loss: 0.4866 - val_accuracy: 0.7604\n",
      "Epoch 594/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4648 - accuracy: 0.7882 - val_loss: 0.4865 - val_accuracy: 0.7604\n",
      "Epoch 595/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4648 - accuracy: 0.7882 - val_loss: 0.4865 - val_accuracy: 0.7604\n",
      "Epoch 596/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4647 - accuracy: 0.7882 - val_loss: 0.4865 - val_accuracy: 0.7604\n",
      "Epoch 597/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4647 - accuracy: 0.7882 - val_loss: 0.4865 - val_accuracy: 0.7604\n",
      "Epoch 598/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4647 - accuracy: 0.7882 - val_loss: 0.4865 - val_accuracy: 0.7604\n",
      "Epoch 599/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4647 - accuracy: 0.7882 - val_loss: 0.4865 - val_accuracy: 0.7604\n",
      "Epoch 600/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4646 - accuracy: 0.7882 - val_loss: 0.4865 - val_accuracy: 0.7604\n",
      "Epoch 601/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4646 - accuracy: 0.7882 - val_loss: 0.4864 - val_accuracy: 0.7604\n",
      "Epoch 602/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4646 - accuracy: 0.7882 - val_loss: 0.4864 - val_accuracy: 0.7604\n",
      "Epoch 603/1000\n",
      "18/18 [==============================] - 0s 748us/step - loss: 0.4645 - accuracy: 0.7882 - val_loss: 0.4864 - val_accuracy: 0.7604\n",
      "Epoch 604/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4645 - accuracy: 0.7882 - val_loss: 0.4864 - val_accuracy: 0.7604\n",
      "Epoch 605/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4645 - accuracy: 0.7882 - val_loss: 0.4864 - val_accuracy: 0.7604\n",
      "Epoch 606/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4645 - accuracy: 0.7882 - val_loss: 0.4864 - val_accuracy: 0.7604\n",
      "Epoch 607/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4644 - accuracy: 0.7882 - val_loss: 0.4864 - val_accuracy: 0.7604\n",
      "Epoch 608/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4644 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 609/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4644 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 610/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4643 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 611/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4643 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 612/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4643 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 613/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4643 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 614/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4642 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 615/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4642 - accuracy: 0.7882 - val_loss: 0.4863 - val_accuracy: 0.7604\n",
      "Epoch 616/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4642 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 617/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4641 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 618/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4641 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 619/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4641 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 620/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4641 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 621/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4640 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 622/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4640 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 623/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4640 - accuracy: 0.7882 - val_loss: 0.4862 - val_accuracy: 0.7604\n",
      "Epoch 624/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4639 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 625/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4639 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 626/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4639 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 627/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4639 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 628/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4638 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 629/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4638 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 630/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4638 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 631/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4638 - accuracy: 0.7882 - val_loss: 0.4861 - val_accuracy: 0.7604\n",
      "Epoch 632/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4637 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 633/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4637 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 634/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4637 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 635/1000\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4637 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 636/1000\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.4636 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 637/1000\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4636 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 638/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4636 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 639/1000\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4636 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 640/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4635 - accuracy: 0.7882 - val_loss: 0.4860 - val_accuracy: 0.7604\n",
      "Epoch 641/1000\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4635 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 642/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4635 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 643/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4635 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 644/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4634 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 645/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4634 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 646/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4634 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 647/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4634 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 648/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4634 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 649/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4633 - accuracy: 0.7882 - val_loss: 0.4859 - val_accuracy: 0.7604\n",
      "Epoch 650/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4633 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 651/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4633 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 652/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4632 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 653/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4632 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 654/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4632 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 655/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4632 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 656/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4631 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 657/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4631 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 658/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4631 - accuracy: 0.7865 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 659/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4631 - accuracy: 0.7882 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "Epoch 660/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4630 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 661/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4630 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 662/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4630 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 663/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4630 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 664/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4629 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 665/1000\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4629 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 666/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4629 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 667/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4629 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 668/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4628 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 669/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4628 - accuracy: 0.7882 - val_loss: 0.4857 - val_accuracy: 0.7604\n",
      "Epoch 670/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4628 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 671/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4628 - accuracy: 0.7865 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 672/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4628 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 673/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4627 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 674/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4627 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 675/1000\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4627 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 676/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4627 - accuracy: 0.7865 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 677/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4626 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 678/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4626 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 679/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4626 - accuracy: 0.7882 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 680/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4626 - accuracy: 0.7865 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 681/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4626 - accuracy: 0.7865 - val_loss: 0.4856 - val_accuracy: 0.7604\n",
      "Epoch 682/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4626 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 683/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4626 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 684/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4625 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 685/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4625 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 686/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4625 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 687/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4624 - accuracy: 0.7882 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 688/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4624 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 689/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4624 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 690/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4624 - accuracy: 0.7882 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 691/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4623 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 692/1000\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4623 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 693/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4623 - accuracy: 0.7865 - val_loss: 0.4855 - val_accuracy: 0.7604\n",
      "Epoch 694/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4623 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 695/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4623 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 696/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4623 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 697/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4622 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 698/1000\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4622 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 699/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4622 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 700/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4622 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 701/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4622 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 702/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4621 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 703/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4621 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 704/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4621 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 705/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4621 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 706/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4621 - accuracy: 0.7865 - val_loss: 0.4854 - val_accuracy: 0.7656\n",
      "Epoch 707/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4620 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 708/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4620 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 709/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4620 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 710/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4620 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 711/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4620 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 712/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4619 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 713/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4619 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 714/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4619 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 715/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4619 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 716/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4619 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 717/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4618 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7656\n",
      "Epoch 718/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4618 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7604\n",
      "Epoch 719/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4618 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7604\n",
      "Epoch 720/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4618 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7604\n",
      "Epoch 721/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4617 - accuracy: 0.7865 - val_loss: 0.4853 - val_accuracy: 0.7604\n",
      "Epoch 722/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4617 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 723/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4617 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 724/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4617 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 725/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4617 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 726/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4617 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 727/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4616 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 728/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4616 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 729/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4616 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 730/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4616 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 731/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4616 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 732/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4615 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 733/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4616 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 734/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4615 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 735/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4615 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 736/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4615 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 737/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4615 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 738/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4614 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.7604\n",
      "Epoch 739/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4614 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 740/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4614 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 741/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4614 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 742/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4614 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 743/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4613 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 744/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4613 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 745/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4613 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 746/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4613 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 747/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4613 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 748/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4613 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 749/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4612 - accuracy: 0.7865 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 750/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4613 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 751/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4612 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 752/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4612 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 753/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4612 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 754/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4612 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 755/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4612 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 756/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4611 - accuracy: 0.7847 - val_loss: 0.4851 - val_accuracy: 0.7604\n",
      "Epoch 757/1000\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4611 - accuracy: 0.7830 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 758/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4611 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 759/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4611 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 760/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4611 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 761/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4610 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 762/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4610 - accuracy: 0.7830 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 763/1000\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4610 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 764/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4610 - accuracy: 0.7830 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 765/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4610 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 766/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4610 - accuracy: 0.7830 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 767/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4609 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 768/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4609 - accuracy: 0.7830 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 769/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4609 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 770/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4609 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 771/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4609 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 772/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4609 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 773/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4609 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 774/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4608 - accuracy: 0.7830 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 775/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4608 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 776/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4608 - accuracy: 0.7847 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 777/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4608 - accuracy: 0.7830 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 778/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4608 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 779/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4608 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 780/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4607 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 781/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4607 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 782/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4607 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 783/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4607 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 784/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4607 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 785/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4607 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 786/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4607 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 787/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4607 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 788/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4606 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 789/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4606 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 790/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4606 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 791/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4606 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 792/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4606 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 793/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4606 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 794/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4605 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 795/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4605 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 796/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4605 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 797/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4605 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 798/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4605 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 799/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4605 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 800/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4605 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 801/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4605 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 802/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4604 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 803/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4604 - accuracy: 0.7830 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 804/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4604 - accuracy: 0.7847 - val_loss: 0.4849 - val_accuracy: 0.7552\n",
      "Epoch 805/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4604 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 806/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4604 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 807/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4604 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 808/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4603 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 809/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4603 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 810/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4603 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 811/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4603 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 812/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4603 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 813/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4603 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 814/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4603 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 815/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4602 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 816/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4602 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 817/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4602 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 818/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4602 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 819/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4602 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 820/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4602 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 821/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4602 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 822/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4602 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 823/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4601 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 824/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4601 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 825/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4601 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 826/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4601 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 827/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4601 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 828/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4601 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 829/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4600 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 830/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4601 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 831/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4600 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 832/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4600 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 833/1000\n",
      "18/18 [==============================] - 0s 772us/step - loss: 0.4600 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 834/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4600 - accuracy: 0.7847 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 835/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4600 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 836/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4600 - accuracy: 0.7830 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 837/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 838/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 839/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 840/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 841/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 842/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 843/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4598 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 844/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 845/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4599 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 846/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4598 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 847/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4598 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 848/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4598 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 849/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4598 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 850/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4598 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 851/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4598 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 852/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 853/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 854/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 855/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 856/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 857/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 858/1000\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 859/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 860/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 861/1000\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 862/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 863/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 864/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 865/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 866/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 867/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 868/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 869/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4596 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 870/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 871/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 872/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 873/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 874/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 875/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 876/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4595 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 877/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 878/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 879/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 880/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 881/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 882/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 883/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 884/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 885/1000\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 886/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 887/1000\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4594 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 888/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4593 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 889/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4593 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 890/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4593 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 891/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4593 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 892/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4593 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 893/1000\n",
      "18/18 [==============================] - 0s 780us/step - loss: 0.4593 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 894/1000\n",
      "18/18 [==============================] - 0s 749us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 895/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 896/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 897/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 898/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 899/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 900/1000\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 901/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 902/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 903/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 904/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4592 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 905/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 906/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 907/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 908/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 909/1000\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 910/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 911/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 912/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4591 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 913/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 914/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 915/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 916/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 917/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 918/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 919/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 920/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 921/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 922/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4590 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 923/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 924/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 925/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 926/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 927/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 928/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 929/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 930/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4589 - accuracy: 0.7847 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 931/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 932/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 933/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 934/1000\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 935/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4589 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7552\n",
      "Epoch 936/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 937/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 938/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 939/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 940/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 941/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 942/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 943/1000\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 944/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 945/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 946/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 947/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 948/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 949/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 950/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 951/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 952/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 953/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4587 - accuracy: 0.7812 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 954/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4587 - accuracy: 0.7812 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 955/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 956/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 957/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 958/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 959/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 960/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 961/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 962/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 963/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 964/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 965/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4585 - accuracy: 0.7812 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 966/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4586 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 967/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 968/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 969/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 970/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4585 - accuracy: 0.7812 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 971/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 972/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 973/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4585 - accuracy: 0.7812 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 974/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 975/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 976/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 977/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 978/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 979/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 980/1000\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 981/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 982/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 983/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4584 - accuracy: 0.7830 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 984/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 985/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 986/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4584 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 987/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 988/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 989/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 990/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 991/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 992/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 993/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 994/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 995/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 996/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4583 - accuracy: 0.7830 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 997/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4583 - accuracy: 0.7830 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 998/1000\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4583 - accuracy: 0.7830 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 999/1000\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4582 - accuracy: 0.7830 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 1000/1000\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4582 - accuracy: 0.7812 - val_loss: 0.4845 - val_accuracy: 0.7604\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25a2ba45b20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "For this exercise, do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8242 - accuracy: 0.4983 - val_loss: 0.8919 - val_accuracy: 0.4167\n",
      "Epoch 2/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.8120 - accuracy: 0.5000 - val_loss: 0.8776 - val_accuracy: 0.4219\n",
      "Epoch 3/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.8008 - accuracy: 0.5243 - val_loss: 0.8646 - val_accuracy: 0.4115\n",
      "Epoch 4/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7907 - accuracy: 0.5295 - val_loss: 0.8527 - val_accuracy: 0.4219\n",
      "Epoch 5/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7814 - accuracy: 0.5417 - val_loss: 0.8417 - val_accuracy: 0.4271\n",
      "Epoch 6/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7731 - accuracy: 0.5503 - val_loss: 0.8315 - val_accuracy: 0.4271\n",
      "Epoch 7/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7651 - accuracy: 0.5573 - val_loss: 0.8220 - val_accuracy: 0.4427\n",
      "Epoch 8/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7580 - accuracy: 0.5642 - val_loss: 0.8132 - val_accuracy: 0.4375\n",
      "Epoch 9/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7512 - accuracy: 0.5764 - val_loss: 0.8049 - val_accuracy: 0.4635\n",
      "Epoch 10/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7449 - accuracy: 0.5833 - val_loss: 0.7972 - val_accuracy: 0.4792\n",
      "Epoch 11/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7389 - accuracy: 0.5868 - val_loss: 0.7899 - val_accuracy: 0.4896\n",
      "Epoch 12/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7333 - accuracy: 0.5972 - val_loss: 0.7830 - val_accuracy: 0.5052\n",
      "Epoch 13/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7281 - accuracy: 0.6076 - val_loss: 0.7765 - val_accuracy: 0.4948\n",
      "Epoch 14/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7231 - accuracy: 0.6285 - val_loss: 0.7704 - val_accuracy: 0.5260\n",
      "Epoch 15/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7183 - accuracy: 0.6406 - val_loss: 0.7646 - val_accuracy: 0.5469\n",
      "Epoch 16/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7139 - accuracy: 0.6476 - val_loss: 0.7591 - val_accuracy: 0.5625\n",
      "Epoch 17/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7098 - accuracy: 0.6493 - val_loss: 0.7537 - val_accuracy: 0.5729\n",
      "Epoch 18/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.7058 - accuracy: 0.6510 - val_loss: 0.7487 - val_accuracy: 0.5729\n",
      "Epoch 19/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.7021 - accuracy: 0.6528 - val_loss: 0.7439 - val_accuracy: 0.5833\n",
      "Epoch 20/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6985 - accuracy: 0.6580 - val_loss: 0.7393 - val_accuracy: 0.5938\n",
      "Epoch 21/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6951 - accuracy: 0.6562 - val_loss: 0.7348 - val_accuracy: 0.6042\n",
      "Epoch 22/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6917 - accuracy: 0.6615 - val_loss: 0.7304 - val_accuracy: 0.6094\n",
      "Epoch 23/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6885 - accuracy: 0.6597 - val_loss: 0.7263 - val_accuracy: 0.6250\n",
      "Epoch 24/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6853 - accuracy: 0.6632 - val_loss: 0.7223 - val_accuracy: 0.6302\n",
      "Epoch 25/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6822 - accuracy: 0.6632 - val_loss: 0.7185 - val_accuracy: 0.6250\n",
      "Epoch 26/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6793 - accuracy: 0.6684 - val_loss: 0.7148 - val_accuracy: 0.6302\n",
      "Epoch 27/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6765 - accuracy: 0.6684 - val_loss: 0.7112 - val_accuracy: 0.6302\n",
      "Epoch 28/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6738 - accuracy: 0.6684 - val_loss: 0.7078 - val_accuracy: 0.6354\n",
      "Epoch 29/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.6712 - accuracy: 0.6649 - val_loss: 0.7046 - val_accuracy: 0.6406\n",
      "Epoch 30/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6685 - accuracy: 0.6597 - val_loss: 0.7014 - val_accuracy: 0.6458\n",
      "Epoch 31/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6662 - accuracy: 0.6632 - val_loss: 0.6984 - val_accuracy: 0.6510\n",
      "Epoch 32/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6638 - accuracy: 0.6632 - val_loss: 0.6955 - val_accuracy: 0.6458\n",
      "Epoch 33/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6616 - accuracy: 0.6667 - val_loss: 0.6927 - val_accuracy: 0.6458\n",
      "Epoch 34/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6593 - accuracy: 0.6684 - val_loss: 0.6899 - val_accuracy: 0.6458\n",
      "Epoch 35/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6572 - accuracy: 0.6701 - val_loss: 0.6872 - val_accuracy: 0.6458\n",
      "Epoch 36/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6552 - accuracy: 0.6701 - val_loss: 0.6845 - val_accuracy: 0.6458\n",
      "Epoch 37/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6532 - accuracy: 0.6701 - val_loss: 0.6819 - val_accuracy: 0.6458\n",
      "Epoch 38/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6513 - accuracy: 0.6753 - val_loss: 0.6793 - val_accuracy: 0.6458\n",
      "Epoch 39/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6494 - accuracy: 0.6736 - val_loss: 0.6768 - val_accuracy: 0.6510\n",
      "Epoch 40/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6475 - accuracy: 0.6736 - val_loss: 0.6744 - val_accuracy: 0.6510\n",
      "Epoch 41/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.6456 - accuracy: 0.6736 - val_loss: 0.6720 - val_accuracy: 0.6510\n",
      "Epoch 42/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6439 - accuracy: 0.6736 - val_loss: 0.6696 - val_accuracy: 0.6562\n",
      "Epoch 43/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6421 - accuracy: 0.6753 - val_loss: 0.6673 - val_accuracy: 0.6562\n",
      "Epoch 44/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6404 - accuracy: 0.6753 - val_loss: 0.6651 - val_accuracy: 0.6562\n",
      "Epoch 45/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6387 - accuracy: 0.6753 - val_loss: 0.6628 - val_accuracy: 0.6562\n",
      "Epoch 46/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6371 - accuracy: 0.6753 - val_loss: 0.6607 - val_accuracy: 0.6562\n",
      "Epoch 47/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6353 - accuracy: 0.6771 - val_loss: 0.6585 - val_accuracy: 0.6615\n",
      "Epoch 48/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6337 - accuracy: 0.6823 - val_loss: 0.6564 - val_accuracy: 0.6615\n",
      "Epoch 49/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6320 - accuracy: 0.6823 - val_loss: 0.6544 - val_accuracy: 0.6615\n",
      "Epoch 50/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6303 - accuracy: 0.6806 - val_loss: 0.6523 - val_accuracy: 0.6667\n",
      "Epoch 51/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6287 - accuracy: 0.6806 - val_loss: 0.6503 - val_accuracy: 0.6771\n",
      "Epoch 52/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6271 - accuracy: 0.6806 - val_loss: 0.6483 - val_accuracy: 0.6771\n",
      "Epoch 53/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6254 - accuracy: 0.6806 - val_loss: 0.6464 - val_accuracy: 0.6771\n",
      "Epoch 54/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6239 - accuracy: 0.6823 - val_loss: 0.6445 - val_accuracy: 0.6771\n",
      "Epoch 55/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6223 - accuracy: 0.6840 - val_loss: 0.6426 - val_accuracy: 0.6771\n",
      "Epoch 56/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6207 - accuracy: 0.6875 - val_loss: 0.6407 - val_accuracy: 0.6771\n",
      "Epoch 57/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6192 - accuracy: 0.6892 - val_loss: 0.6388 - val_accuracy: 0.6823\n",
      "Epoch 58/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6176 - accuracy: 0.6875 - val_loss: 0.6370 - val_accuracy: 0.6823\n",
      "Epoch 59/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6161 - accuracy: 0.6875 - val_loss: 0.6351 - val_accuracy: 0.6823\n",
      "Epoch 60/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6146 - accuracy: 0.6910 - val_loss: 0.6333 - val_accuracy: 0.6823\n",
      "Epoch 61/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6131 - accuracy: 0.6892 - val_loss: 0.6316 - val_accuracy: 0.6875\n",
      "Epoch 62/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.6117 - accuracy: 0.6892 - val_loss: 0.6298 - val_accuracy: 0.6875\n",
      "Epoch 63/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6102 - accuracy: 0.6892 - val_loss: 0.6280 - val_accuracy: 0.6875\n",
      "Epoch 64/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.6088 - accuracy: 0.6875 - val_loss: 0.6262 - val_accuracy: 0.6875\n",
      "Epoch 65/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6072 - accuracy: 0.6875 - val_loss: 0.6244 - val_accuracy: 0.6875\n",
      "Epoch 66/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6058 - accuracy: 0.6875 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 67/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.6043 - accuracy: 0.6910 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 68/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6030 - accuracy: 0.6892 - val_loss: 0.6193 - val_accuracy: 0.6927\n",
      "Epoch 69/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6016 - accuracy: 0.6910 - val_loss: 0.6177 - val_accuracy: 0.6927\n",
      "Epoch 70/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.6002 - accuracy: 0.6927 - val_loss: 0.6161 - val_accuracy: 0.6927\n",
      "Epoch 71/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5989 - accuracy: 0.6944 - val_loss: 0.6145 - val_accuracy: 0.6927\n",
      "Epoch 72/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5976 - accuracy: 0.6944 - val_loss: 0.6129 - val_accuracy: 0.6927\n",
      "Epoch 73/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5964 - accuracy: 0.6962 - val_loss: 0.6113 - val_accuracy: 0.7031\n",
      "Epoch 74/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5951 - accuracy: 0.6962 - val_loss: 0.6098 - val_accuracy: 0.7031\n",
      "Epoch 75/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5938 - accuracy: 0.6979 - val_loss: 0.6082 - val_accuracy: 0.7031\n",
      "Epoch 76/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5925 - accuracy: 0.6979 - val_loss: 0.6067 - val_accuracy: 0.6979\n",
      "Epoch 77/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5913 - accuracy: 0.6979 - val_loss: 0.6051 - val_accuracy: 0.6979\n",
      "Epoch 78/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5901 - accuracy: 0.6997 - val_loss: 0.6035 - val_accuracy: 0.6979\n",
      "Epoch 79/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5888 - accuracy: 0.6997 - val_loss: 0.6020 - val_accuracy: 0.6979\n",
      "Epoch 80/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5875 - accuracy: 0.7014 - val_loss: 0.6005 - val_accuracy: 0.7031\n",
      "Epoch 81/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5862 - accuracy: 0.7031 - val_loss: 0.5990 - val_accuracy: 0.7031\n",
      "Epoch 82/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5850 - accuracy: 0.7031 - val_loss: 0.5975 - val_accuracy: 0.7083\n",
      "Epoch 83/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5838 - accuracy: 0.7049 - val_loss: 0.5960 - val_accuracy: 0.7083\n",
      "Epoch 84/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5826 - accuracy: 0.7083 - val_loss: 0.5945 - val_accuracy: 0.7083\n",
      "Epoch 85/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5814 - accuracy: 0.7066 - val_loss: 0.5930 - val_accuracy: 0.7083\n",
      "Epoch 86/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5801 - accuracy: 0.7083 - val_loss: 0.5915 - val_accuracy: 0.7083\n",
      "Epoch 87/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5789 - accuracy: 0.7083 - val_loss: 0.5901 - val_accuracy: 0.7083\n",
      "Epoch 88/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5778 - accuracy: 0.7118 - val_loss: 0.5887 - val_accuracy: 0.7083\n",
      "Epoch 89/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5766 - accuracy: 0.7118 - val_loss: 0.5872 - val_accuracy: 0.7083\n",
      "Epoch 90/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5755 - accuracy: 0.7135 - val_loss: 0.5859 - val_accuracy: 0.7083\n",
      "Epoch 91/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5743 - accuracy: 0.7135 - val_loss: 0.5845 - val_accuracy: 0.7083\n",
      "Epoch 92/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5732 - accuracy: 0.7135 - val_loss: 0.5832 - val_accuracy: 0.7083\n",
      "Epoch 93/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5721 - accuracy: 0.7135 - val_loss: 0.5819 - val_accuracy: 0.7135\n",
      "Epoch 94/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5710 - accuracy: 0.7135 - val_loss: 0.5806 - val_accuracy: 0.7135\n",
      "Epoch 95/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5700 - accuracy: 0.7135 - val_loss: 0.5794 - val_accuracy: 0.7135\n",
      "Epoch 96/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5689 - accuracy: 0.7135 - val_loss: 0.5782 - val_accuracy: 0.7135\n",
      "Epoch 97/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5678 - accuracy: 0.7135 - val_loss: 0.5770 - val_accuracy: 0.7188\n",
      "Epoch 98/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5667 - accuracy: 0.7153 - val_loss: 0.5758 - val_accuracy: 0.7188\n",
      "Epoch 99/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5657 - accuracy: 0.7135 - val_loss: 0.5746 - val_accuracy: 0.7188\n",
      "Epoch 100/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5647 - accuracy: 0.7135 - val_loss: 0.5735 - val_accuracy: 0.7188\n",
      "Epoch 101/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5637 - accuracy: 0.7101 - val_loss: 0.5724 - val_accuracy: 0.7240\n",
      "Epoch 102/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5627 - accuracy: 0.7083 - val_loss: 0.5713 - val_accuracy: 0.7240\n",
      "Epoch 103/1500\n",
      "18/18 [==============================] - 0s 944us/step - loss: 0.5618 - accuracy: 0.7101 - val_loss: 0.5702 - val_accuracy: 0.7240\n",
      "Epoch 104/1500\n",
      "18/18 [==============================] - 0s 917us/step - loss: 0.5608 - accuracy: 0.7083 - val_loss: 0.5691 - val_accuracy: 0.7188\n",
      "Epoch 105/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5598 - accuracy: 0.7083 - val_loss: 0.5680 - val_accuracy: 0.7188\n",
      "Epoch 106/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5588 - accuracy: 0.7083 - val_loss: 0.5670 - val_accuracy: 0.7240\n",
      "Epoch 107/1500\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.5580 - accuracy: 0.7066 - val_loss: 0.5659 - val_accuracy: 0.7240\n",
      "Epoch 108/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5570 - accuracy: 0.7066 - val_loss: 0.5649 - val_accuracy: 0.7240\n",
      "Epoch 109/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5561 - accuracy: 0.7066 - val_loss: 0.5640 - val_accuracy: 0.7240\n",
      "Epoch 110/1500\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.5552 - accuracy: 0.7101 - val_loss: 0.5630 - val_accuracy: 0.7344\n",
      "Epoch 111/1500\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.5543 - accuracy: 0.7101 - val_loss: 0.5621 - val_accuracy: 0.7344\n",
      "Epoch 112/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5534 - accuracy: 0.7135 - val_loss: 0.5611 - val_accuracy: 0.7396\n",
      "Epoch 113/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5525 - accuracy: 0.7135 - val_loss: 0.5602 - val_accuracy: 0.7396\n",
      "Epoch 114/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5517 - accuracy: 0.7153 - val_loss: 0.5593 - val_accuracy: 0.7396\n",
      "Epoch 115/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5508 - accuracy: 0.7153 - val_loss: 0.5585 - val_accuracy: 0.7448\n",
      "Epoch 116/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5500 - accuracy: 0.7153 - val_loss: 0.5577 - val_accuracy: 0.7500\n",
      "Epoch 117/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5492 - accuracy: 0.7170 - val_loss: 0.5568 - val_accuracy: 0.7500\n",
      "Epoch 118/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5484 - accuracy: 0.7153 - val_loss: 0.5560 - val_accuracy: 0.7500\n",
      "Epoch 119/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5477 - accuracy: 0.7153 - val_loss: 0.5552 - val_accuracy: 0.7500\n",
      "Epoch 120/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.5470 - accuracy: 0.7153 - val_loss: 0.5544 - val_accuracy: 0.7500\n",
      "Epoch 121/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5463 - accuracy: 0.7135 - val_loss: 0.5537 - val_accuracy: 0.7500\n",
      "Epoch 122/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5455 - accuracy: 0.7135 - val_loss: 0.5529 - val_accuracy: 0.7500\n",
      "Epoch 123/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5447 - accuracy: 0.7170 - val_loss: 0.5522 - val_accuracy: 0.7500\n",
      "Epoch 124/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5440 - accuracy: 0.7153 - val_loss: 0.5515 - val_accuracy: 0.7500\n",
      "Epoch 125/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5433 - accuracy: 0.7153 - val_loss: 0.5508 - val_accuracy: 0.7500\n",
      "Epoch 126/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5426 - accuracy: 0.7153 - val_loss: 0.5502 - val_accuracy: 0.7500\n",
      "Epoch 127/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5420 - accuracy: 0.7170 - val_loss: 0.5495 - val_accuracy: 0.7500\n",
      "Epoch 128/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5414 - accuracy: 0.7188 - val_loss: 0.5489 - val_accuracy: 0.7500\n",
      "Epoch 129/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.5407 - accuracy: 0.7188 - val_loss: 0.5483 - val_accuracy: 0.7500\n",
      "Epoch 130/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5401 - accuracy: 0.7205 - val_loss: 0.5477 - val_accuracy: 0.7500\n",
      "Epoch 131/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5394 - accuracy: 0.7205 - val_loss: 0.5472 - val_accuracy: 0.7500\n",
      "Epoch 132/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5388 - accuracy: 0.7205 - val_loss: 0.5466 - val_accuracy: 0.7500\n",
      "Epoch 133/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5382 - accuracy: 0.7205 - val_loss: 0.5460 - val_accuracy: 0.7500\n",
      "Epoch 134/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5376 - accuracy: 0.7205 - val_loss: 0.5455 - val_accuracy: 0.7500\n",
      "Epoch 135/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5370 - accuracy: 0.7222 - val_loss: 0.5449 - val_accuracy: 0.7500\n",
      "Epoch 136/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5364 - accuracy: 0.7222 - val_loss: 0.5444 - val_accuracy: 0.7552\n",
      "Epoch 137/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5358 - accuracy: 0.7222 - val_loss: 0.5439 - val_accuracy: 0.7552\n",
      "Epoch 138/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5351 - accuracy: 0.7222 - val_loss: 0.5434 - val_accuracy: 0.7552\n",
      "Epoch 139/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5346 - accuracy: 0.7222 - val_loss: 0.5429 - val_accuracy: 0.7552\n",
      "Epoch 140/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5340 - accuracy: 0.7222 - val_loss: 0.5424 - val_accuracy: 0.7500\n",
      "Epoch 141/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5334 - accuracy: 0.7222 - val_loss: 0.5420 - val_accuracy: 0.7500\n",
      "Epoch 142/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5329 - accuracy: 0.7222 - val_loss: 0.5415 - val_accuracy: 0.7500\n",
      "Epoch 143/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5323 - accuracy: 0.7205 - val_loss: 0.5410 - val_accuracy: 0.7500\n",
      "Epoch 144/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5317 - accuracy: 0.7205 - val_loss: 0.5406 - val_accuracy: 0.7500\n",
      "Epoch 145/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5312 - accuracy: 0.7205 - val_loss: 0.5402 - val_accuracy: 0.7500\n",
      "Epoch 146/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5307 - accuracy: 0.7222 - val_loss: 0.5397 - val_accuracy: 0.7500\n",
      "Epoch 147/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5300 - accuracy: 0.7240 - val_loss: 0.5393 - val_accuracy: 0.7500\n",
      "Epoch 148/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5295 - accuracy: 0.7222 - val_loss: 0.5389 - val_accuracy: 0.7448\n",
      "Epoch 149/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5290 - accuracy: 0.7222 - val_loss: 0.5384 - val_accuracy: 0.7448\n",
      "Epoch 150/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5285 - accuracy: 0.7222 - val_loss: 0.5380 - val_accuracy: 0.7500\n",
      "Epoch 151/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5279 - accuracy: 0.7222 - val_loss: 0.5376 - val_accuracy: 0.7500\n",
      "Epoch 152/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5274 - accuracy: 0.7205 - val_loss: 0.5372 - val_accuracy: 0.7500\n",
      "Epoch 153/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5269 - accuracy: 0.7205 - val_loss: 0.5368 - val_accuracy: 0.7500\n",
      "Epoch 154/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5264 - accuracy: 0.7205 - val_loss: 0.5365 - val_accuracy: 0.7500\n",
      "Epoch 155/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5260 - accuracy: 0.7205 - val_loss: 0.5361 - val_accuracy: 0.7500\n",
      "Epoch 156/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5254 - accuracy: 0.7205 - val_loss: 0.5357 - val_accuracy: 0.7500\n",
      "Epoch 157/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5250 - accuracy: 0.7222 - val_loss: 0.5354 - val_accuracy: 0.7500\n",
      "Epoch 158/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5245 - accuracy: 0.7222 - val_loss: 0.5350 - val_accuracy: 0.7448\n",
      "Epoch 159/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5240 - accuracy: 0.7274 - val_loss: 0.5347 - val_accuracy: 0.7448\n",
      "Epoch 160/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5235 - accuracy: 0.7257 - val_loss: 0.5343 - val_accuracy: 0.7396\n",
      "Epoch 161/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5231 - accuracy: 0.7257 - val_loss: 0.5340 - val_accuracy: 0.7396\n",
      "Epoch 162/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5226 - accuracy: 0.7274 - val_loss: 0.5336 - val_accuracy: 0.7396\n",
      "Epoch 163/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5221 - accuracy: 0.7274 - val_loss: 0.5333 - val_accuracy: 0.7396\n",
      "Epoch 164/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5216 - accuracy: 0.7292 - val_loss: 0.5330 - val_accuracy: 0.7396\n",
      "Epoch 165/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5211 - accuracy: 0.7309 - val_loss: 0.5326 - val_accuracy: 0.7396\n",
      "Epoch 166/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5206 - accuracy: 0.7309 - val_loss: 0.5323 - val_accuracy: 0.7396\n",
      "Epoch 167/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5202 - accuracy: 0.7309 - val_loss: 0.5320 - val_accuracy: 0.7396\n",
      "Epoch 168/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5198 - accuracy: 0.7309 - val_loss: 0.5317 - val_accuracy: 0.7396\n",
      "Epoch 169/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5194 - accuracy: 0.7257 - val_loss: 0.5315 - val_accuracy: 0.7396\n",
      "Epoch 170/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5189 - accuracy: 0.7274 - val_loss: 0.5312 - val_accuracy: 0.7396\n",
      "Epoch 171/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5185 - accuracy: 0.7292 - val_loss: 0.5309 - val_accuracy: 0.7396\n",
      "Epoch 172/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5181 - accuracy: 0.7292 - val_loss: 0.5306 - val_accuracy: 0.7396\n",
      "Epoch 173/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5176 - accuracy: 0.7292 - val_loss: 0.5303 - val_accuracy: 0.7396\n",
      "Epoch 174/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5172 - accuracy: 0.7309 - val_loss: 0.5301 - val_accuracy: 0.7396\n",
      "Epoch 175/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5168 - accuracy: 0.7309 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 176/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5164 - accuracy: 0.7309 - val_loss: 0.5296 - val_accuracy: 0.7448\n",
      "Epoch 177/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5160 - accuracy: 0.7292 - val_loss: 0.5293 - val_accuracy: 0.7396\n",
      "Epoch 178/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5155 - accuracy: 0.7309 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 179/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5152 - accuracy: 0.7309 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 180/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5147 - accuracy: 0.7309 - val_loss: 0.5286 - val_accuracy: 0.7448\n",
      "Epoch 181/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5144 - accuracy: 0.7309 - val_loss: 0.5284 - val_accuracy: 0.7448\n",
      "Epoch 182/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5140 - accuracy: 0.7309 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 183/1500\n",
      "18/18 [==============================] - 0s 777us/step - loss: 0.5136 - accuracy: 0.7326 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 184/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5132 - accuracy: 0.7326 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 185/1500\n",
      "18/18 [==============================] - 0s 835us/step - loss: 0.5128 - accuracy: 0.7344 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 186/1500\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.5124 - accuracy: 0.7344 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 187/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5120 - accuracy: 0.7344 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 188/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5115 - accuracy: 0.7344 - val_loss: 0.5268 - val_accuracy: 0.7448\n",
      "Epoch 189/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5111 - accuracy: 0.7344 - val_loss: 0.5266 - val_accuracy: 0.7396\n",
      "Epoch 190/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5106 - accuracy: 0.7326 - val_loss: 0.5264 - val_accuracy: 0.7396\n",
      "Epoch 191/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5102 - accuracy: 0.7344 - val_loss: 0.5262 - val_accuracy: 0.7396\n",
      "Epoch 192/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5098 - accuracy: 0.7344 - val_loss: 0.5260 - val_accuracy: 0.7396\n",
      "Epoch 193/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5093 - accuracy: 0.7344 - val_loss: 0.5258 - val_accuracy: 0.7396\n",
      "Epoch 194/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5088 - accuracy: 0.7344 - val_loss: 0.5256 - val_accuracy: 0.7396\n",
      "Epoch 195/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5084 - accuracy: 0.7344 - val_loss: 0.5254 - val_accuracy: 0.7396\n",
      "Epoch 196/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5080 - accuracy: 0.7344 - val_loss: 0.5252 - val_accuracy: 0.7396\n",
      "Epoch 197/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5076 - accuracy: 0.7344 - val_loss: 0.5250 - val_accuracy: 0.7396\n",
      "Epoch 198/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5071 - accuracy: 0.7344 - val_loss: 0.5248 - val_accuracy: 0.7396\n",
      "Epoch 199/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5067 - accuracy: 0.7344 - val_loss: 0.5246 - val_accuracy: 0.7396\n",
      "Epoch 200/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5063 - accuracy: 0.7344 - val_loss: 0.5245 - val_accuracy: 0.7396\n",
      "Epoch 201/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5059 - accuracy: 0.7344 - val_loss: 0.5243 - val_accuracy: 0.7396\n",
      "Epoch 202/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5055 - accuracy: 0.7344 - val_loss: 0.5241 - val_accuracy: 0.7396\n",
      "Epoch 203/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5051 - accuracy: 0.7344 - val_loss: 0.5240 - val_accuracy: 0.7396\n",
      "Epoch 204/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.5047 - accuracy: 0.7344 - val_loss: 0.5238 - val_accuracy: 0.7396\n",
      "Epoch 205/1500\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.5043 - accuracy: 0.7361 - val_loss: 0.5237 - val_accuracy: 0.7396\n",
      "Epoch 206/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5038 - accuracy: 0.7396 - val_loss: 0.5235 - val_accuracy: 0.7396\n",
      "Epoch 207/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5034 - accuracy: 0.7413 - val_loss: 0.5233 - val_accuracy: 0.7396\n",
      "Epoch 208/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.5031 - accuracy: 0.7413 - val_loss: 0.5232 - val_accuracy: 0.7396\n",
      "Epoch 209/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5027 - accuracy: 0.7413 - val_loss: 0.5230 - val_accuracy: 0.7396\n",
      "Epoch 210/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5023 - accuracy: 0.7413 - val_loss: 0.5229 - val_accuracy: 0.7396\n",
      "Epoch 211/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.5019 - accuracy: 0.7448 - val_loss: 0.5227 - val_accuracy: 0.7396\n",
      "Epoch 212/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5015 - accuracy: 0.7448 - val_loss: 0.5225 - val_accuracy: 0.7344\n",
      "Epoch 213/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5011 - accuracy: 0.7448 - val_loss: 0.5224 - val_accuracy: 0.7396\n",
      "Epoch 214/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5008 - accuracy: 0.7448 - val_loss: 0.5222 - val_accuracy: 0.7396\n",
      "Epoch 215/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.5004 - accuracy: 0.7448 - val_loss: 0.5221 - val_accuracy: 0.7396\n",
      "Epoch 216/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.5000 - accuracy: 0.7448 - val_loss: 0.5220 - val_accuracy: 0.7396\n",
      "Epoch 217/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4996 - accuracy: 0.7448 - val_loss: 0.5218 - val_accuracy: 0.7396\n",
      "Epoch 218/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4993 - accuracy: 0.7465 - val_loss: 0.5217 - val_accuracy: 0.7396\n",
      "Epoch 219/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4989 - accuracy: 0.7465 - val_loss: 0.5216 - val_accuracy: 0.7396\n",
      "Epoch 220/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4985 - accuracy: 0.7465 - val_loss: 0.5214 - val_accuracy: 0.7396\n",
      "Epoch 221/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4982 - accuracy: 0.7465 - val_loss: 0.5213 - val_accuracy: 0.7396\n",
      "Epoch 222/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4978 - accuracy: 0.7465 - val_loss: 0.5212 - val_accuracy: 0.7396\n",
      "Epoch 223/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4975 - accuracy: 0.7465 - val_loss: 0.5211 - val_accuracy: 0.7396\n",
      "Epoch 224/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4971 - accuracy: 0.7465 - val_loss: 0.5210 - val_accuracy: 0.7396\n",
      "Epoch 225/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4968 - accuracy: 0.7483 - val_loss: 0.5208 - val_accuracy: 0.7396\n",
      "Epoch 226/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4964 - accuracy: 0.7465 - val_loss: 0.5207 - val_accuracy: 0.7396\n",
      "Epoch 227/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4961 - accuracy: 0.7483 - val_loss: 0.5206 - val_accuracy: 0.7396\n",
      "Epoch 228/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4957 - accuracy: 0.7483 - val_loss: 0.5205 - val_accuracy: 0.7396\n",
      "Epoch 229/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4955 - accuracy: 0.7483 - val_loss: 0.5204 - val_accuracy: 0.7396\n",
      "Epoch 230/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4951 - accuracy: 0.7483 - val_loss: 0.5203 - val_accuracy: 0.7396\n",
      "Epoch 231/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4948 - accuracy: 0.7483 - val_loss: 0.5202 - val_accuracy: 0.7396\n",
      "Epoch 232/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4945 - accuracy: 0.7483 - val_loss: 0.5201 - val_accuracy: 0.7396\n",
      "Epoch 233/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4941 - accuracy: 0.7483 - val_loss: 0.5200 - val_accuracy: 0.7396\n",
      "Epoch 234/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4938 - accuracy: 0.7483 - val_loss: 0.5199 - val_accuracy: 0.7396\n",
      "Epoch 235/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4936 - accuracy: 0.7500 - val_loss: 0.5198 - val_accuracy: 0.7396\n",
      "Epoch 236/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4932 - accuracy: 0.7500 - val_loss: 0.5197 - val_accuracy: 0.7396\n",
      "Epoch 237/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4929 - accuracy: 0.7500 - val_loss: 0.5196 - val_accuracy: 0.7396\n",
      "Epoch 238/1500\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4926 - accuracy: 0.7500 - val_loss: 0.5195 - val_accuracy: 0.7344\n",
      "Epoch 239/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4923 - accuracy: 0.7500 - val_loss: 0.5194 - val_accuracy: 0.7344\n",
      "Epoch 240/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4920 - accuracy: 0.7517 - val_loss: 0.5193 - val_accuracy: 0.7344\n",
      "Epoch 241/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4917 - accuracy: 0.7500 - val_loss: 0.5192 - val_accuracy: 0.7344\n",
      "Epoch 242/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4914 - accuracy: 0.7500 - val_loss: 0.5191 - val_accuracy: 0.7344\n",
      "Epoch 243/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4911 - accuracy: 0.7500 - val_loss: 0.5190 - val_accuracy: 0.7344\n",
      "Epoch 244/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4908 - accuracy: 0.7500 - val_loss: 0.5189 - val_accuracy: 0.7344\n",
      "Epoch 245/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4905 - accuracy: 0.7500 - val_loss: 0.5188 - val_accuracy: 0.7344\n",
      "Epoch 246/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4901 - accuracy: 0.7517 - val_loss: 0.5187 - val_accuracy: 0.7344\n",
      "Epoch 247/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4899 - accuracy: 0.7517 - val_loss: 0.5186 - val_accuracy: 0.7344\n",
      "Epoch 248/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4895 - accuracy: 0.7535 - val_loss: 0.5185 - val_accuracy: 0.7344\n",
      "Epoch 249/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4892 - accuracy: 0.7517 - val_loss: 0.5184 - val_accuracy: 0.7396\n",
      "Epoch 250/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4889 - accuracy: 0.7517 - val_loss: 0.5183 - val_accuracy: 0.7396\n",
      "Epoch 251/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4887 - accuracy: 0.7535 - val_loss: 0.5182 - val_accuracy: 0.7396\n",
      "Epoch 252/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4883 - accuracy: 0.7535 - val_loss: 0.5182 - val_accuracy: 0.7396\n",
      "Epoch 253/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4880 - accuracy: 0.7517 - val_loss: 0.5181 - val_accuracy: 0.7396\n",
      "Epoch 254/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4878 - accuracy: 0.7500 - val_loss: 0.5180 - val_accuracy: 0.7396\n",
      "Epoch 255/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4874 - accuracy: 0.7500 - val_loss: 0.5179 - val_accuracy: 0.7396\n",
      "Epoch 256/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4872 - accuracy: 0.7500 - val_loss: 0.5178 - val_accuracy: 0.7396\n",
      "Epoch 257/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4869 - accuracy: 0.7500 - val_loss: 0.5178 - val_accuracy: 0.7396\n",
      "Epoch 258/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4866 - accuracy: 0.7500 - val_loss: 0.5177 - val_accuracy: 0.7396\n",
      "Epoch 259/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4863 - accuracy: 0.7500 - val_loss: 0.5176 - val_accuracy: 0.7396\n",
      "Epoch 260/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4860 - accuracy: 0.7517 - val_loss: 0.5175 - val_accuracy: 0.7396\n",
      "Epoch 261/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4857 - accuracy: 0.7517 - val_loss: 0.5174 - val_accuracy: 0.7396\n",
      "Epoch 262/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4853 - accuracy: 0.7517 - val_loss: 0.5173 - val_accuracy: 0.7396\n",
      "Epoch 263/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4851 - accuracy: 0.7517 - val_loss: 0.5172 - val_accuracy: 0.7396\n",
      "Epoch 264/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4848 - accuracy: 0.7535 - val_loss: 0.5171 - val_accuracy: 0.7396\n",
      "Epoch 265/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4845 - accuracy: 0.7535 - val_loss: 0.5170 - val_accuracy: 0.7396\n",
      "Epoch 266/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4842 - accuracy: 0.7535 - val_loss: 0.5169 - val_accuracy: 0.7396\n",
      "Epoch 267/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4839 - accuracy: 0.7535 - val_loss: 0.5168 - val_accuracy: 0.7344\n",
      "Epoch 268/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4836 - accuracy: 0.7517 - val_loss: 0.5168 - val_accuracy: 0.7344\n",
      "Epoch 269/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4834 - accuracy: 0.7535 - val_loss: 0.5167 - val_accuracy: 0.7344\n",
      "Epoch 270/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4830 - accuracy: 0.7535 - val_loss: 0.5166 - val_accuracy: 0.7344\n",
      "Epoch 271/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4827 - accuracy: 0.7517 - val_loss: 0.5166 - val_accuracy: 0.7344\n",
      "Epoch 272/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4825 - accuracy: 0.7535 - val_loss: 0.5165 - val_accuracy: 0.7344\n",
      "Epoch 273/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4822 - accuracy: 0.7535 - val_loss: 0.5164 - val_accuracy: 0.7344\n",
      "Epoch 274/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4819 - accuracy: 0.7535 - val_loss: 0.5164 - val_accuracy: 0.7344\n",
      "Epoch 275/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4816 - accuracy: 0.7535 - val_loss: 0.5163 - val_accuracy: 0.7344\n",
      "Epoch 276/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4814 - accuracy: 0.7535 - val_loss: 0.5163 - val_accuracy: 0.7292\n",
      "Epoch 277/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4811 - accuracy: 0.7535 - val_loss: 0.5162 - val_accuracy: 0.7292\n",
      "Epoch 278/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4809 - accuracy: 0.7535 - val_loss: 0.5161 - val_accuracy: 0.7292\n",
      "Epoch 279/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4806 - accuracy: 0.7535 - val_loss: 0.5161 - val_accuracy: 0.7292\n",
      "Epoch 280/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4804 - accuracy: 0.7535 - val_loss: 0.5160 - val_accuracy: 0.7292\n",
      "Epoch 281/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4801 - accuracy: 0.7535 - val_loss: 0.5160 - val_accuracy: 0.7292\n",
      "Epoch 282/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4798 - accuracy: 0.7552 - val_loss: 0.5159 - val_accuracy: 0.7292\n",
      "Epoch 283/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4796 - accuracy: 0.7535 - val_loss: 0.5158 - val_accuracy: 0.7292\n",
      "Epoch 284/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4794 - accuracy: 0.7535 - val_loss: 0.5158 - val_accuracy: 0.7292\n",
      "Epoch 285/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4791 - accuracy: 0.7535 - val_loss: 0.5157 - val_accuracy: 0.7292\n",
      "Epoch 286/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4789 - accuracy: 0.7552 - val_loss: 0.5157 - val_accuracy: 0.7292\n",
      "Epoch 287/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4786 - accuracy: 0.7535 - val_loss: 0.5156 - val_accuracy: 0.7292\n",
      "Epoch 288/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4784 - accuracy: 0.7535 - val_loss: 0.5156 - val_accuracy: 0.7292\n",
      "Epoch 289/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4781 - accuracy: 0.7552 - val_loss: 0.5155 - val_accuracy: 0.7292\n",
      "Epoch 290/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4779 - accuracy: 0.7552 - val_loss: 0.5154 - val_accuracy: 0.7292\n",
      "Epoch 291/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4776 - accuracy: 0.7535 - val_loss: 0.5154 - val_accuracy: 0.7292\n",
      "Epoch 292/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4773 - accuracy: 0.7569 - val_loss: 0.5153 - val_accuracy: 0.7292\n",
      "Epoch 293/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4771 - accuracy: 0.7569 - val_loss: 0.5152 - val_accuracy: 0.7292\n",
      "Epoch 294/1500\n",
      "18/18 [==============================] - 0s 781us/step - loss: 0.4769 - accuracy: 0.7569 - val_loss: 0.5152 - val_accuracy: 0.7292\n",
      "Epoch 295/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4767 - accuracy: 0.7569 - val_loss: 0.5151 - val_accuracy: 0.7292\n",
      "Epoch 296/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4764 - accuracy: 0.7569 - val_loss: 0.5150 - val_accuracy: 0.7292\n",
      "Epoch 297/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4761 - accuracy: 0.7587 - val_loss: 0.5150 - val_accuracy: 0.7292\n",
      "Epoch 298/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4759 - accuracy: 0.7587 - val_loss: 0.5149 - val_accuracy: 0.7292\n",
      "Epoch 299/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4756 - accuracy: 0.7587 - val_loss: 0.5148 - val_accuracy: 0.7292\n",
      "Epoch 300/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4754 - accuracy: 0.7587 - val_loss: 0.5148 - val_accuracy: 0.7292\n",
      "Epoch 301/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4752 - accuracy: 0.7587 - val_loss: 0.5147 - val_accuracy: 0.7292\n",
      "Epoch 302/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4749 - accuracy: 0.7587 - val_loss: 0.5147 - val_accuracy: 0.7292\n",
      "Epoch 303/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4746 - accuracy: 0.7604 - val_loss: 0.5146 - val_accuracy: 0.7292\n",
      "Epoch 304/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4744 - accuracy: 0.7604 - val_loss: 0.5145 - val_accuracy: 0.7292\n",
      "Epoch 305/1500\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4741 - accuracy: 0.7604 - val_loss: 0.5145 - val_accuracy: 0.7292\n",
      "Epoch 306/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4739 - accuracy: 0.7604 - val_loss: 0.5144 - val_accuracy: 0.7292\n",
      "Epoch 307/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4736 - accuracy: 0.7604 - val_loss: 0.5144 - val_accuracy: 0.7292\n",
      "Epoch 308/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4735 - accuracy: 0.7604 - val_loss: 0.5143 - val_accuracy: 0.7292\n",
      "Epoch 309/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4732 - accuracy: 0.7604 - val_loss: 0.5143 - val_accuracy: 0.7292\n",
      "Epoch 310/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4730 - accuracy: 0.7604 - val_loss: 0.5142 - val_accuracy: 0.7292\n",
      "Epoch 311/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4728 - accuracy: 0.7604 - val_loss: 0.5142 - val_accuracy: 0.7292\n",
      "Epoch 312/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4725 - accuracy: 0.7604 - val_loss: 0.5141 - val_accuracy: 0.7292\n",
      "Epoch 313/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4723 - accuracy: 0.7604 - val_loss: 0.5141 - val_accuracy: 0.7292\n",
      "Epoch 314/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4722 - accuracy: 0.7604 - val_loss: 0.5140 - val_accuracy: 0.7240\n",
      "Epoch 315/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4719 - accuracy: 0.7622 - val_loss: 0.5139 - val_accuracy: 0.7240\n",
      "Epoch 316/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4717 - accuracy: 0.7622 - val_loss: 0.5139 - val_accuracy: 0.7240\n",
      "Epoch 317/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4715 - accuracy: 0.7622 - val_loss: 0.5138 - val_accuracy: 0.7240\n",
      "Epoch 318/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4712 - accuracy: 0.7639 - val_loss: 0.5138 - val_accuracy: 0.7240\n",
      "Epoch 319/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4710 - accuracy: 0.7639 - val_loss: 0.5137 - val_accuracy: 0.7240\n",
      "Epoch 320/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4708 - accuracy: 0.7639 - val_loss: 0.5136 - val_accuracy: 0.7344\n",
      "Epoch 321/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4706 - accuracy: 0.7587 - val_loss: 0.5136 - val_accuracy: 0.7396\n",
      "Epoch 322/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4704 - accuracy: 0.7587 - val_loss: 0.5135 - val_accuracy: 0.7396\n",
      "Epoch 323/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4701 - accuracy: 0.7569 - val_loss: 0.5135 - val_accuracy: 0.7396\n",
      "Epoch 324/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4699 - accuracy: 0.7569 - val_loss: 0.5134 - val_accuracy: 0.7396\n",
      "Epoch 325/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4697 - accuracy: 0.7569 - val_loss: 0.5133 - val_accuracy: 0.7396\n",
      "Epoch 326/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4695 - accuracy: 0.7569 - val_loss: 0.5133 - val_accuracy: 0.7396\n",
      "Epoch 327/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4693 - accuracy: 0.7569 - val_loss: 0.5132 - val_accuracy: 0.7448\n",
      "Epoch 328/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4691 - accuracy: 0.7569 - val_loss: 0.5132 - val_accuracy: 0.7448\n",
      "Epoch 329/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4689 - accuracy: 0.7569 - val_loss: 0.5131 - val_accuracy: 0.7448\n",
      "Epoch 330/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4687 - accuracy: 0.7569 - val_loss: 0.5131 - val_accuracy: 0.7448\n",
      "Epoch 331/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4685 - accuracy: 0.7569 - val_loss: 0.5130 - val_accuracy: 0.7448\n",
      "Epoch 332/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4684 - accuracy: 0.7569 - val_loss: 0.5130 - val_accuracy: 0.7448\n",
      "Epoch 333/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4682 - accuracy: 0.7569 - val_loss: 0.5129 - val_accuracy: 0.7448\n",
      "Epoch 334/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4680 - accuracy: 0.7569 - val_loss: 0.5129 - val_accuracy: 0.7448\n",
      "Epoch 335/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4678 - accuracy: 0.7569 - val_loss: 0.5129 - val_accuracy: 0.7448\n",
      "Epoch 336/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4676 - accuracy: 0.7569 - val_loss: 0.5128 - val_accuracy: 0.7448\n",
      "Epoch 337/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4675 - accuracy: 0.7569 - val_loss: 0.5128 - val_accuracy: 0.7448\n",
      "Epoch 338/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4673 - accuracy: 0.7569 - val_loss: 0.5128 - val_accuracy: 0.7500\n",
      "Epoch 339/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4671 - accuracy: 0.7552 - val_loss: 0.5127 - val_accuracy: 0.7448\n",
      "Epoch 340/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4669 - accuracy: 0.7552 - val_loss: 0.5127 - val_accuracy: 0.7448\n",
      "Epoch 341/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4668 - accuracy: 0.7569 - val_loss: 0.5126 - val_accuracy: 0.7448\n",
      "Epoch 342/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4666 - accuracy: 0.7552 - val_loss: 0.5126 - val_accuracy: 0.7448\n",
      "Epoch 343/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4664 - accuracy: 0.7569 - val_loss: 0.5126 - val_accuracy: 0.7448\n",
      "Epoch 344/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4663 - accuracy: 0.7569 - val_loss: 0.5125 - val_accuracy: 0.7448\n",
      "Epoch 345/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4661 - accuracy: 0.7552 - val_loss: 0.5125 - val_accuracy: 0.7448\n",
      "Epoch 346/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4660 - accuracy: 0.7569 - val_loss: 0.5125 - val_accuracy: 0.7448\n",
      "Epoch 347/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4657 - accuracy: 0.7569 - val_loss: 0.5124 - val_accuracy: 0.7448\n",
      "Epoch 348/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4655 - accuracy: 0.7552 - val_loss: 0.5124 - val_accuracy: 0.7448\n",
      "Epoch 349/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4654 - accuracy: 0.7569 - val_loss: 0.5123 - val_accuracy: 0.7448\n",
      "Epoch 350/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4652 - accuracy: 0.7552 - val_loss: 0.5123 - val_accuracy: 0.7448\n",
      "Epoch 351/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4651 - accuracy: 0.7569 - val_loss: 0.5123 - val_accuracy: 0.7448\n",
      "Epoch 352/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4648 - accuracy: 0.7552 - val_loss: 0.5123 - val_accuracy: 0.7448\n",
      "Epoch 353/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4646 - accuracy: 0.7552 - val_loss: 0.5122 - val_accuracy: 0.7448\n",
      "Epoch 354/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4644 - accuracy: 0.7569 - val_loss: 0.5122 - val_accuracy: 0.7448\n",
      "Epoch 355/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4643 - accuracy: 0.7552 - val_loss: 0.5122 - val_accuracy: 0.7448\n",
      "Epoch 356/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4641 - accuracy: 0.7552 - val_loss: 0.5121 - val_accuracy: 0.7448\n",
      "Epoch 357/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4640 - accuracy: 0.7569 - val_loss: 0.5121 - val_accuracy: 0.7448\n",
      "Epoch 358/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4638 - accuracy: 0.7569 - val_loss: 0.5121 - val_accuracy: 0.7448\n",
      "Epoch 359/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4636 - accuracy: 0.7569 - val_loss: 0.5120 - val_accuracy: 0.7448\n",
      "Epoch 360/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4634 - accuracy: 0.7587 - val_loss: 0.5120 - val_accuracy: 0.7448\n",
      "Epoch 361/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4633 - accuracy: 0.7569 - val_loss: 0.5120 - val_accuracy: 0.7448\n",
      "Epoch 362/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4631 - accuracy: 0.7587 - val_loss: 0.5120 - val_accuracy: 0.7448\n",
      "Epoch 363/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4629 - accuracy: 0.7587 - val_loss: 0.5120 - val_accuracy: 0.7448\n",
      "Epoch 364/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4628 - accuracy: 0.7587 - val_loss: 0.5119 - val_accuracy: 0.7448\n",
      "Epoch 365/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4626 - accuracy: 0.7587 - val_loss: 0.5119 - val_accuracy: 0.7448\n",
      "Epoch 366/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4625 - accuracy: 0.7622 - val_loss: 0.5119 - val_accuracy: 0.7448\n",
      "Epoch 367/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4623 - accuracy: 0.7604 - val_loss: 0.5119 - val_accuracy: 0.7448\n",
      "Epoch 368/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4622 - accuracy: 0.7622 - val_loss: 0.5119 - val_accuracy: 0.7448\n",
      "Epoch 369/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4620 - accuracy: 0.7622 - val_loss: 0.5119 - val_accuracy: 0.7448\n",
      "Epoch 370/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4618 - accuracy: 0.7622 - val_loss: 0.5118 - val_accuracy: 0.7448\n",
      "Epoch 371/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4617 - accuracy: 0.7622 - val_loss: 0.5118 - val_accuracy: 0.7448\n",
      "Epoch 372/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4615 - accuracy: 0.7622 - val_loss: 0.5118 - val_accuracy: 0.7448\n",
      "Epoch 373/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4614 - accuracy: 0.7639 - val_loss: 0.5118 - val_accuracy: 0.7448\n",
      "Epoch 374/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4613 - accuracy: 0.7656 - val_loss: 0.5118 - val_accuracy: 0.7448\n",
      "Epoch 375/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4611 - accuracy: 0.7639 - val_loss: 0.5117 - val_accuracy: 0.7448\n",
      "Epoch 376/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4609 - accuracy: 0.7656 - val_loss: 0.5117 - val_accuracy: 0.7448\n",
      "Epoch 377/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4608 - accuracy: 0.7656 - val_loss: 0.5117 - val_accuracy: 0.7448\n",
      "Epoch 378/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4606 - accuracy: 0.7674 - val_loss: 0.5117 - val_accuracy: 0.7448\n",
      "Epoch 379/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4605 - accuracy: 0.7674 - val_loss: 0.5117 - val_accuracy: 0.7448\n",
      "Epoch 380/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4603 - accuracy: 0.7691 - val_loss: 0.5116 - val_accuracy: 0.7448\n",
      "Epoch 381/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4602 - accuracy: 0.7708 - val_loss: 0.5116 - val_accuracy: 0.7448\n",
      "Epoch 382/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4601 - accuracy: 0.7691 - val_loss: 0.5116 - val_accuracy: 0.7448\n",
      "Epoch 383/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4599 - accuracy: 0.7708 - val_loss: 0.5116 - val_accuracy: 0.7448\n",
      "Epoch 384/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4597 - accuracy: 0.7708 - val_loss: 0.5116 - val_accuracy: 0.7448\n",
      "Epoch 385/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4596 - accuracy: 0.7708 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 386/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4595 - accuracy: 0.7708 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 387/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4593 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 388/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4592 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 389/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4591 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 390/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4589 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 391/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4587 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 392/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4586 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 393/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4585 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7500\n",
      "Epoch 394/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4583 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7500\n",
      "Epoch 395/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4582 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7500\n",
      "Epoch 396/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4581 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7500\n",
      "Epoch 397/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4579 - accuracy: 0.7743 - val_loss: 0.5115 - val_accuracy: 0.7500\n",
      "Epoch 398/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4578 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 399/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4576 - accuracy: 0.7726 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 400/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4575 - accuracy: 0.7743 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 401/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4574 - accuracy: 0.7743 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 402/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4572 - accuracy: 0.7743 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 403/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4571 - accuracy: 0.7743 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 404/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4569 - accuracy: 0.7743 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 405/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4569 - accuracy: 0.7743 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 406/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4567 - accuracy: 0.7743 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 407/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4565 - accuracy: 0.7726 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 408/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4564 - accuracy: 0.7743 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 409/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4563 - accuracy: 0.7743 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 410/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4561 - accuracy: 0.7743 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 411/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4560 - accuracy: 0.7726 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 412/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4559 - accuracy: 0.7743 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 413/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4558 - accuracy: 0.7726 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 414/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4557 - accuracy: 0.7708 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 415/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4555 - accuracy: 0.7726 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 416/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4554 - accuracy: 0.7743 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 417/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4553 - accuracy: 0.7743 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 418/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4552 - accuracy: 0.7726 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 419/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4551 - accuracy: 0.7743 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 420/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4549 - accuracy: 0.7743 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 421/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4548 - accuracy: 0.7743 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 422/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4547 - accuracy: 0.7743 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 423/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4546 - accuracy: 0.7760 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 424/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4544 - accuracy: 0.7760 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 425/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4543 - accuracy: 0.7760 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 426/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4542 - accuracy: 0.7760 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 427/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4541 - accuracy: 0.7760 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 428/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4539 - accuracy: 0.7760 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 429/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4538 - accuracy: 0.7760 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 430/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4537 - accuracy: 0.7778 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 431/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4536 - accuracy: 0.7760 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 432/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4535 - accuracy: 0.7778 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 433/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4533 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 434/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4533 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 435/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4532 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 436/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4530 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 437/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4530 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 438/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4529 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 439/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4527 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 440/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4526 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 441/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4526 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 442/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4525 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 443/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4524 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 444/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4523 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 445/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4522 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 446/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4521 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 447/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4520 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 448/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4520 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 449/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4519 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 450/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4518 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 451/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4516 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 452/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4515 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 453/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4515 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 454/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4514 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7604\n",
      "Epoch 455/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4513 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 456/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4512 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 457/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4511 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 458/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4510 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 459/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4509 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 460/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4509 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 461/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4508 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 462/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4507 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 463/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4506 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 464/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4505 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 465/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4504 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 466/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4503 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 467/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4502 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 468/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4501 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 469/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4501 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 470/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4500 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 471/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4499 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 472/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4498 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 473/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4497 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 474/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4497 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 475/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4495 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 476/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4495 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 477/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4494 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 478/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4493 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 479/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4492 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 480/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4492 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7604\n",
      "Epoch 481/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4491 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 482/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4489 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 483/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4489 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 484/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4488 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 485/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4487 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 486/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4486 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 487/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4486 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 488/1500\n",
      "18/18 [==============================] - 0s 808us/step - loss: 0.4485 - accuracy: 0.7795 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 489/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4484 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 490/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4483 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 491/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4483 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 492/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4481 - accuracy: 0.7778 - val_loss: 0.5109 - val_accuracy: 0.7552\n",
      "Epoch 493/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4480 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 494/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4480 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 495/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4479 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 496/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4479 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 497/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4478 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 498/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4477 - accuracy: 0.7778 - val_loss: 0.5110 - val_accuracy: 0.7552\n",
      "Epoch 499/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4476 - accuracy: 0.7795 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 500/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4476 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 501/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4475 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 502/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4474 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 503/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4473 - accuracy: 0.7795 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 504/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4473 - accuracy: 0.7778 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 505/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4472 - accuracy: 0.7795 - val_loss: 0.5111 - val_accuracy: 0.7552\n",
      "Epoch 506/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4471 - accuracy: 0.7778 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 507/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4471 - accuracy: 0.7778 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 508/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4470 - accuracy: 0.7795 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 509/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4469 - accuracy: 0.7778 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 510/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4468 - accuracy: 0.7795 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 511/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4468 - accuracy: 0.7812 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 512/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4467 - accuracy: 0.7795 - val_loss: 0.5112 - val_accuracy: 0.7552\n",
      "Epoch 513/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4466 - accuracy: 0.7812 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 514/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4466 - accuracy: 0.7812 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 515/1500\n",
      "18/18 [==============================] - 0s 917us/step - loss: 0.4465 - accuracy: 0.7812 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 516/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4465 - accuracy: 0.7812 - val_loss: 0.5113 - val_accuracy: 0.7552\n",
      "Epoch 517/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4464 - accuracy: 0.7795 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 518/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4463 - accuracy: 0.7812 - val_loss: 0.5114 - val_accuracy: 0.7552\n",
      "Epoch 519/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4462 - accuracy: 0.7795 - val_loss: 0.5114 - val_accuracy: 0.7604\n",
      "Epoch 520/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4462 - accuracy: 0.7812 - val_loss: 0.5114 - val_accuracy: 0.7604\n",
      "Epoch 521/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4462 - accuracy: 0.7795 - val_loss: 0.5114 - val_accuracy: 0.7604\n",
      "Epoch 522/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4461 - accuracy: 0.7812 - val_loss: 0.5114 - val_accuracy: 0.7604\n",
      "Epoch 523/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4460 - accuracy: 0.7760 - val_loss: 0.5114 - val_accuracy: 0.7604\n",
      "Epoch 524/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4459 - accuracy: 0.7778 - val_loss: 0.5114 - val_accuracy: 0.7604\n",
      "Epoch 525/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4458 - accuracy: 0.7760 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 526/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4458 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 527/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4457 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 528/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4457 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 529/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4456 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 530/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4455 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 531/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4455 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 532/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4454 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 533/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4453 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 534/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4453 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 535/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4452 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 536/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4451 - accuracy: 0.7795 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 537/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4450 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 538/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4449 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7604\n",
      "Epoch 539/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4449 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 540/1500\n",
      "18/18 [==============================] - 0s 917us/step - loss: 0.4448 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 541/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4447 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 542/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4447 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 543/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4446 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 544/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4446 - accuracy: 0.7812 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 545/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4445 - accuracy: 0.7812 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 546/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4445 - accuracy: 0.7812 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 547/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4444 - accuracy: 0.7812 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 548/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4444 - accuracy: 0.7812 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 549/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4442 - accuracy: 0.7812 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 550/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4442 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 551/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4441 - accuracy: 0.7812 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 552/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4441 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 553/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4440 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 554/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4440 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 555/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4439 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 556/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4438 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 557/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4438 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 558/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4438 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 559/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4436 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 560/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4437 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 561/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4436 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 562/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4436 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 563/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4435 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 564/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4434 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 565/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4434 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 566/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4433 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 567/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4432 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 568/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4432 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 569/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4432 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 570/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4431 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 571/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4430 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 572/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4430 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 573/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4429 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 574/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4429 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 575/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4428 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 576/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4428 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 577/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4427 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 578/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4427 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 579/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4426 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 580/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4425 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 581/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4425 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 582/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4425 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 583/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4424 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 584/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4424 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 585/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4423 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 586/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4423 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 587/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4422 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 588/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4421 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 589/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4421 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 590/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4421 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 591/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4420 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 592/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4420 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 593/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4419 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 594/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4419 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 595/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4418 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 596/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4418 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 597/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4417 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 598/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4416 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 599/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4416 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7604\n",
      "Epoch 600/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4416 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 601/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4415 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 602/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4414 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 603/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4414 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 604/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4413 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 605/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4413 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 606/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4412 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 607/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4412 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 608/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4411 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 609/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4411 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 610/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4410 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7604\n",
      "Epoch 611/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4410 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 612/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4409 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 613/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4409 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 614/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4408 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 615/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4408 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 616/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4407 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 617/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4407 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 618/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4406 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 619/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4406 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 620/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4405 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 621/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4405 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 622/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4405 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7656\n",
      "Epoch 623/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4404 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7656\n",
      "Epoch 624/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4403 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 625/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4403 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 626/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4403 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 627/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4402 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 628/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4402 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 629/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4401 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 630/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4401 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 631/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4400 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 632/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4400 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 633/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4400 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 634/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4399 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7656\n",
      "Epoch 635/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4398 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7656\n",
      "Epoch 636/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4398 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7656\n",
      "Epoch 637/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4397 - accuracy: 0.7778 - val_loss: 0.5115 - val_accuracy: 0.7656\n",
      "Epoch 638/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4397 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 639/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4397 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 640/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4396 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 641/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4396 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 642/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4396 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 643/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4395 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 644/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4395 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 645/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4394 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 646/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4394 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 647/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4393 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 648/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4393 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 649/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4393 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 650/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4393 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 651/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4392 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 652/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4391 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 653/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4391 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 654/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4390 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 655/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4390 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 656/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4389 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 657/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4389 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 658/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4389 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 659/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4388 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 660/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4388 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 661/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4388 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 662/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4388 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 663/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4386 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 664/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4386 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 665/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4386 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 666/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4385 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 667/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4386 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 668/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4384 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 669/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4384 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 670/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4384 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 671/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4383 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 672/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4383 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 673/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4382 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 674/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4382 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 675/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4382 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 676/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4381 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 677/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4381 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 678/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4381 - accuracy: 0.7778 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 679/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4381 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 680/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4380 - accuracy: 0.7795 - val_loss: 0.5116 - val_accuracy: 0.7656\n",
      "Epoch 681/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4379 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 682/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4379 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 683/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4379 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 684/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4378 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 685/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4378 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 686/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4378 - accuracy: 0.7778 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 687/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4377 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 688/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4377 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 689/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4376 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 690/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4376 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 691/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4375 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 692/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4375 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 693/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4375 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 694/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4374 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 695/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4374 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 696/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4374 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 697/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4373 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 698/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4373 - accuracy: 0.7812 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 699/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4372 - accuracy: 0.7812 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 700/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4372 - accuracy: 0.7812 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 701/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4372 - accuracy: 0.7812 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 702/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4371 - accuracy: 0.7812 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 703/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4370 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 704/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4371 - accuracy: 0.7812 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 705/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4370 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 706/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4370 - accuracy: 0.7812 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 707/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4369 - accuracy: 0.7795 - val_loss: 0.5117 - val_accuracy: 0.7656\n",
      "Epoch 708/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4369 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7656\n",
      "Epoch 709/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4368 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 710/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4368 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 711/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4368 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 712/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4367 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 713/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4367 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 714/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4367 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 715/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4366 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 716/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4366 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 717/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4365 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 718/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4365 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 719/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4364 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 720/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4364 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 721/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4364 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 722/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4363 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 723/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4363 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 724/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4362 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 725/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4362 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 726/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4361 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 727/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4361 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 728/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4360 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 729/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4360 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 730/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4360 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 731/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4359 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 732/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4359 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 733/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4359 - accuracy: 0.7812 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 734/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4358 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 735/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4357 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 736/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4357 - accuracy: 0.7795 - val_loss: 0.5118 - val_accuracy: 0.7760\n",
      "Epoch 737/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4357 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 738/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4356 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 739/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4356 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 740/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4355 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 741/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4355 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 742/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4354 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 743/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4354 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 744/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4354 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 745/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4354 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 746/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4353 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 747/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4353 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 748/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4352 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 749/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4351 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 750/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4351 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 751/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4351 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 752/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4350 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 753/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4350 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 754/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4350 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 755/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4349 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 756/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4349 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 757/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4349 - accuracy: 0.7795 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 758/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4348 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 759/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4348 - accuracy: 0.7812 - val_loss: 0.5119 - val_accuracy: 0.7760\n",
      "Epoch 760/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4348 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7760\n",
      "Epoch 761/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4347 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7760\n",
      "Epoch 762/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4347 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7760\n",
      "Epoch 763/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4346 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7760\n",
      "Epoch 764/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4346 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 765/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4346 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 766/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4346 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 767/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4345 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 768/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4345 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 769/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4344 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 770/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4344 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 771/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4343 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 772/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4343 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 773/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4343 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 774/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4342 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 775/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4342 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 776/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4342 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 777/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4342 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 778/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4341 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 779/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4341 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 780/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4340 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 781/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4340 - accuracy: 0.7795 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 782/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4340 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 783/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4339 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 784/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4339 - accuracy: 0.7812 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 785/1500\n",
      "18/18 [==============================] - 0s 780us/step - loss: 0.4338 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 786/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4339 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 787/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4338 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 788/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4338 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 789/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4337 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 790/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4337 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 791/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4337 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 792/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4336 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 793/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4336 - accuracy: 0.7812 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 794/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4336 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 795/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4335 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 796/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4335 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 797/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4335 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 798/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4334 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 799/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4334 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 800/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4334 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 801/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4333 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 802/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4333 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 803/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4332 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 804/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4332 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 805/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4332 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 806/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4332 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 807/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4331 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 808/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4331 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7760\n",
      "Epoch 809/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4330 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 810/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4330 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 811/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4331 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 812/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4330 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 813/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4329 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 814/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4329 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 815/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4328 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 816/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4329 - accuracy: 0.7812 - val_loss: 0.5123 - val_accuracy: 0.7760\n",
      "Epoch 817/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4328 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 818/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4327 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 819/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4327 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 820/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4327 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 821/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4327 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 822/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4326 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 823/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4326 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 824/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4326 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 825/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4325 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 826/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4325 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 827/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4325 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 828/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4324 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 829/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4324 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 830/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4324 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 831/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4323 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 832/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4324 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 833/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4323 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 834/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4322 - accuracy: 0.7830 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 835/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4322 - accuracy: 0.7812 - val_loss: 0.5124 - val_accuracy: 0.7760\n",
      "Epoch 836/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4321 - accuracy: 0.7812 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 837/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4322 - accuracy: 0.7847 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 838/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4321 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 839/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4321 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 840/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4321 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 841/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4321 - accuracy: 0.7812 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 842/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4320 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 843/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4319 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 844/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4319 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 845/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4319 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 846/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4319 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 847/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4319 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 848/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4318 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 849/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4318 - accuracy: 0.7830 - val_loss: 0.5125 - val_accuracy: 0.7760\n",
      "Epoch 850/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4318 - accuracy: 0.7847 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 851/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4318 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 852/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4317 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 853/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4317 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 854/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4316 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 855/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4316 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 856/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4316 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 857/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4316 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7760\n",
      "Epoch 858/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4316 - accuracy: 0.7830 - val_loss: 0.5126 - val_accuracy: 0.7812\n",
      "Epoch 859/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4315 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 860/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4314 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 861/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4315 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 862/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4314 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 863/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4313 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 864/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4313 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 865/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4313 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 866/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4313 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 867/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4312 - accuracy: 0.7830 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 868/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4312 - accuracy: 0.7830 - val_loss: 0.5128 - val_accuracy: 0.7812\n",
      "Epoch 869/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4312 - accuracy: 0.7830 - val_loss: 0.5128 - val_accuracy: 0.7812\n",
      "Epoch 870/1500\n",
      "18/18 [==============================] - 0s 722us/step - loss: 0.4311 - accuracy: 0.7830 - val_loss: 0.5128 - val_accuracy: 0.7812\n",
      "Epoch 871/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4311 - accuracy: 0.7830 - val_loss: 0.5128 - val_accuracy: 0.7812\n",
      "Epoch 872/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4311 - accuracy: 0.7830 - val_loss: 0.5128 - val_accuracy: 0.7812\n",
      "Epoch 873/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4310 - accuracy: 0.7830 - val_loss: 0.5129 - val_accuracy: 0.7812\n",
      "Epoch 874/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4310 - accuracy: 0.7830 - val_loss: 0.5129 - val_accuracy: 0.7812\n",
      "Epoch 875/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4310 - accuracy: 0.7830 - val_loss: 0.5129 - val_accuracy: 0.7812\n",
      "Epoch 876/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4309 - accuracy: 0.7830 - val_loss: 0.5129 - val_accuracy: 0.7812\n",
      "Epoch 877/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4309 - accuracy: 0.7830 - val_loss: 0.5129 - val_accuracy: 0.7812\n",
      "Epoch 878/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4308 - accuracy: 0.7830 - val_loss: 0.5129 - val_accuracy: 0.7812\n",
      "Epoch 879/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4308 - accuracy: 0.7830 - val_loss: 0.5130 - val_accuracy: 0.7812\n",
      "Epoch 880/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4308 - accuracy: 0.7830 - val_loss: 0.5130 - val_accuracy: 0.7812\n",
      "Epoch 881/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4308 - accuracy: 0.7830 - val_loss: 0.5130 - val_accuracy: 0.7812\n",
      "Epoch 882/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4308 - accuracy: 0.7830 - val_loss: 0.5130 - val_accuracy: 0.7812\n",
      "Epoch 883/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4307 - accuracy: 0.7830 - val_loss: 0.5131 - val_accuracy: 0.7812\n",
      "Epoch 884/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4306 - accuracy: 0.7830 - val_loss: 0.5131 - val_accuracy: 0.7812\n",
      "Epoch 885/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4306 - accuracy: 0.7830 - val_loss: 0.5131 - val_accuracy: 0.7812\n",
      "Epoch 886/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4305 - accuracy: 0.7830 - val_loss: 0.5131 - val_accuracy: 0.7812\n",
      "Epoch 887/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4305 - accuracy: 0.7830 - val_loss: 0.5131 - val_accuracy: 0.7812\n",
      "Epoch 888/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4305 - accuracy: 0.7830 - val_loss: 0.5131 - val_accuracy: 0.7812\n",
      "Epoch 889/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4304 - accuracy: 0.7830 - val_loss: 0.5132 - val_accuracy: 0.7812\n",
      "Epoch 890/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4304 - accuracy: 0.7847 - val_loss: 0.5132 - val_accuracy: 0.7812\n",
      "Epoch 891/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4304 - accuracy: 0.7830 - val_loss: 0.5132 - val_accuracy: 0.7812\n",
      "Epoch 892/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4303 - accuracy: 0.7830 - val_loss: 0.5132 - val_accuracy: 0.7812\n",
      "Epoch 893/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4303 - accuracy: 0.7830 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 894/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4303 - accuracy: 0.7830 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 895/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4303 - accuracy: 0.7847 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 896/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4302 - accuracy: 0.7830 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 897/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4302 - accuracy: 0.7865 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 898/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4301 - accuracy: 0.7865 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 899/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4302 - accuracy: 0.7865 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 900/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4301 - accuracy: 0.7865 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 901/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4301 - accuracy: 0.7847 - val_loss: 0.5134 - val_accuracy: 0.7812\n",
      "Epoch 902/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4300 - accuracy: 0.7847 - val_loss: 0.5134 - val_accuracy: 0.7812\n",
      "Epoch 903/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4300 - accuracy: 0.7847 - val_loss: 0.5134 - val_accuracy: 0.7812\n",
      "Epoch 904/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4300 - accuracy: 0.7847 - val_loss: 0.5134 - val_accuracy: 0.7812\n",
      "Epoch 905/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4299 - accuracy: 0.7847 - val_loss: 0.5134 - val_accuracy: 0.7812\n",
      "Epoch 906/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4299 - accuracy: 0.7847 - val_loss: 0.5135 - val_accuracy: 0.7812\n",
      "Epoch 907/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4298 - accuracy: 0.7847 - val_loss: 0.5135 - val_accuracy: 0.7812\n",
      "Epoch 908/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4298 - accuracy: 0.7847 - val_loss: 0.5135 - val_accuracy: 0.7812\n",
      "Epoch 909/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4298 - accuracy: 0.7847 - val_loss: 0.5135 - val_accuracy: 0.7812\n",
      "Epoch 910/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4297 - accuracy: 0.7847 - val_loss: 0.5136 - val_accuracy: 0.7812\n",
      "Epoch 911/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4297 - accuracy: 0.7847 - val_loss: 0.5136 - val_accuracy: 0.7812\n",
      "Epoch 912/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4297 - accuracy: 0.7847 - val_loss: 0.5136 - val_accuracy: 0.7812\n",
      "Epoch 913/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4296 - accuracy: 0.7847 - val_loss: 0.5136 - val_accuracy: 0.7812\n",
      "Epoch 914/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4296 - accuracy: 0.7847 - val_loss: 0.5136 - val_accuracy: 0.7812\n",
      "Epoch 915/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4297 - accuracy: 0.7830 - val_loss: 0.5137 - val_accuracy: 0.7812\n",
      "Epoch 916/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4296 - accuracy: 0.7847 - val_loss: 0.5137 - val_accuracy: 0.7812\n",
      "Epoch 917/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4295 - accuracy: 0.7865 - val_loss: 0.5137 - val_accuracy: 0.7812\n",
      "Epoch 918/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4295 - accuracy: 0.7865 - val_loss: 0.5137 - val_accuracy: 0.7812\n",
      "Epoch 919/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4295 - accuracy: 0.7865 - val_loss: 0.5138 - val_accuracy: 0.7812\n",
      "Epoch 920/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4294 - accuracy: 0.7865 - val_loss: 0.5138 - val_accuracy: 0.7812\n",
      "Epoch 921/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4294 - accuracy: 0.7847 - val_loss: 0.5138 - val_accuracy: 0.7812\n",
      "Epoch 922/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4293 - accuracy: 0.7865 - val_loss: 0.5138 - val_accuracy: 0.7812\n",
      "Epoch 923/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4293 - accuracy: 0.7847 - val_loss: 0.5138 - val_accuracy: 0.7812\n",
      "Epoch 924/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4293 - accuracy: 0.7847 - val_loss: 0.5139 - val_accuracy: 0.7812\n",
      "Epoch 925/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4293 - accuracy: 0.7847 - val_loss: 0.5139 - val_accuracy: 0.7812\n",
      "Epoch 926/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4292 - accuracy: 0.7865 - val_loss: 0.5139 - val_accuracy: 0.7812\n",
      "Epoch 927/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4291 - accuracy: 0.7865 - val_loss: 0.5139 - val_accuracy: 0.7812\n",
      "Epoch 928/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4292 - accuracy: 0.7865 - val_loss: 0.5140 - val_accuracy: 0.7760\n",
      "Epoch 929/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4291 - accuracy: 0.7865 - val_loss: 0.5140 - val_accuracy: 0.7760\n",
      "Epoch 930/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4291 - accuracy: 0.7865 - val_loss: 0.5139 - val_accuracy: 0.7760\n",
      "Epoch 931/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4290 - accuracy: 0.7882 - val_loss: 0.5140 - val_accuracy: 0.7760\n",
      "Epoch 932/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4291 - accuracy: 0.7882 - val_loss: 0.5140 - val_accuracy: 0.7760\n",
      "Epoch 933/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4290 - accuracy: 0.7865 - val_loss: 0.5140 - val_accuracy: 0.7760\n",
      "Epoch 934/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4289 - accuracy: 0.7882 - val_loss: 0.5141 - val_accuracy: 0.7760\n",
      "Epoch 935/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4289 - accuracy: 0.7865 - val_loss: 0.5141 - val_accuracy: 0.7760\n",
      "Epoch 936/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4289 - accuracy: 0.7882 - val_loss: 0.5141 - val_accuracy: 0.7760\n",
      "Epoch 937/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4288 - accuracy: 0.7882 - val_loss: 0.5141 - val_accuracy: 0.7760\n",
      "Epoch 938/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4288 - accuracy: 0.7882 - val_loss: 0.5141 - val_accuracy: 0.7760\n",
      "Epoch 939/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4288 - accuracy: 0.7882 - val_loss: 0.5142 - val_accuracy: 0.7760\n",
      "Epoch 940/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4288 - accuracy: 0.7882 - val_loss: 0.5142 - val_accuracy: 0.7760\n",
      "Epoch 941/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4287 - accuracy: 0.7882 - val_loss: 0.5142 - val_accuracy: 0.7760\n",
      "Epoch 942/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4287 - accuracy: 0.7882 - val_loss: 0.5142 - val_accuracy: 0.7760\n",
      "Epoch 943/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4287 - accuracy: 0.7865 - val_loss: 0.5142 - val_accuracy: 0.7760\n",
      "Epoch 944/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4286 - accuracy: 0.7865 - val_loss: 0.5142 - val_accuracy: 0.7760\n",
      "Epoch 945/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4286 - accuracy: 0.7882 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 946/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4285 - accuracy: 0.7882 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 947/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4285 - accuracy: 0.7882 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 948/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4285 - accuracy: 0.7882 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 949/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4284 - accuracy: 0.7882 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 950/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4285 - accuracy: 0.7882 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 951/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4284 - accuracy: 0.7882 - val_loss: 0.5144 - val_accuracy: 0.7760\n",
      "Epoch 952/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4284 - accuracy: 0.7882 - val_loss: 0.5144 - val_accuracy: 0.7760\n",
      "Epoch 953/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4283 - accuracy: 0.7882 - val_loss: 0.5144 - val_accuracy: 0.7760\n",
      "Epoch 954/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4283 - accuracy: 0.7882 - val_loss: 0.5144 - val_accuracy: 0.7760\n",
      "Epoch 955/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4283 - accuracy: 0.7882 - val_loss: 0.5144 - val_accuracy: 0.7760\n",
      "Epoch 956/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4282 - accuracy: 0.7882 - val_loss: 0.5145 - val_accuracy: 0.7760\n",
      "Epoch 957/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4282 - accuracy: 0.7865 - val_loss: 0.5145 - val_accuracy: 0.7760\n",
      "Epoch 958/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4281 - accuracy: 0.7865 - val_loss: 0.5145 - val_accuracy: 0.7760\n",
      "Epoch 959/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4281 - accuracy: 0.7882 - val_loss: 0.5145 - val_accuracy: 0.7760\n",
      "Epoch 960/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4282 - accuracy: 0.7865 - val_loss: 0.5145 - val_accuracy: 0.7760\n",
      "Epoch 961/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4281 - accuracy: 0.7882 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 962/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4281 - accuracy: 0.7865 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 963/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4281 - accuracy: 0.7882 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 964/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4280 - accuracy: 0.7882 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 965/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4280 - accuracy: 0.7882 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 966/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4279 - accuracy: 0.7865 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 967/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4279 - accuracy: 0.7865 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 968/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4279 - accuracy: 0.7882 - val_loss: 0.5147 - val_accuracy: 0.7760\n",
      "Epoch 969/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4279 - accuracy: 0.7882 - val_loss: 0.5147 - val_accuracy: 0.7760\n",
      "Epoch 970/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4279 - accuracy: 0.7882 - val_loss: 0.5147 - val_accuracy: 0.7760\n",
      "Epoch 971/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4278 - accuracy: 0.7882 - val_loss: 0.5147 - val_accuracy: 0.7760\n",
      "Epoch 972/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4277 - accuracy: 0.7882 - val_loss: 0.5147 - val_accuracy: 0.7760\n",
      "Epoch 973/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4277 - accuracy: 0.7882 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 974/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4277 - accuracy: 0.7882 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 975/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4276 - accuracy: 0.7882 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 976/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4276 - accuracy: 0.7899 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 977/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4276 - accuracy: 0.7882 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 978/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4276 - accuracy: 0.7899 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 979/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4275 - accuracy: 0.7882 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 980/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4275 - accuracy: 0.7899 - val_loss: 0.5149 - val_accuracy: 0.7760\n",
      "Epoch 981/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4274 - accuracy: 0.7899 - val_loss: 0.5149 - val_accuracy: 0.7760\n",
      "Epoch 982/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4274 - accuracy: 0.7882 - val_loss: 0.5149 - val_accuracy: 0.7760\n",
      "Epoch 983/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4274 - accuracy: 0.7899 - val_loss: 0.5149 - val_accuracy: 0.7760\n",
      "Epoch 984/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4274 - accuracy: 0.7899 - val_loss: 0.5149 - val_accuracy: 0.7760\n",
      "Epoch 985/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4273 - accuracy: 0.7917 - val_loss: 0.5150 - val_accuracy: 0.7760\n",
      "Epoch 986/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4273 - accuracy: 0.7899 - val_loss: 0.5150 - val_accuracy: 0.7760\n",
      "Epoch 987/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4272 - accuracy: 0.7899 - val_loss: 0.5150 - val_accuracy: 0.7760\n",
      "Epoch 988/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4272 - accuracy: 0.7917 - val_loss: 0.5150 - val_accuracy: 0.7760\n",
      "Epoch 989/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4272 - accuracy: 0.7934 - val_loss: 0.5151 - val_accuracy: 0.7760\n",
      "Epoch 990/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4272 - accuracy: 0.7917 - val_loss: 0.5151 - val_accuracy: 0.7760\n",
      "Epoch 991/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4271 - accuracy: 0.7917 - val_loss: 0.5151 - val_accuracy: 0.7760\n",
      "Epoch 992/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4271 - accuracy: 0.7934 - val_loss: 0.5151 - val_accuracy: 0.7760\n",
      "Epoch 993/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4270 - accuracy: 0.7917 - val_loss: 0.5152 - val_accuracy: 0.7760\n",
      "Epoch 994/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4270 - accuracy: 0.7917 - val_loss: 0.5152 - val_accuracy: 0.7760\n",
      "Epoch 995/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4270 - accuracy: 0.7917 - val_loss: 0.5152 - val_accuracy: 0.7760\n",
      "Epoch 996/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4269 - accuracy: 0.7934 - val_loss: 0.5153 - val_accuracy: 0.7760\n",
      "Epoch 997/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4269 - accuracy: 0.7917 - val_loss: 0.5153 - val_accuracy: 0.7760\n",
      "Epoch 998/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4268 - accuracy: 0.7917 - val_loss: 0.5153 - val_accuracy: 0.7760\n",
      "Epoch 999/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4268 - accuracy: 0.7917 - val_loss: 0.5153 - val_accuracy: 0.7760\n",
      "Epoch 1000/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4268 - accuracy: 0.7899 - val_loss: 0.5153 - val_accuracy: 0.7760\n",
      "Epoch 1001/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4267 - accuracy: 0.7934 - val_loss: 0.5153 - val_accuracy: 0.7760\n",
      "Epoch 1002/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4267 - accuracy: 0.7917 - val_loss: 0.5154 - val_accuracy: 0.7760\n",
      "Epoch 1003/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4267 - accuracy: 0.7917 - val_loss: 0.5154 - val_accuracy: 0.7760\n",
      "Epoch 1004/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4267 - accuracy: 0.7951 - val_loss: 0.5154 - val_accuracy: 0.7760\n",
      "Epoch 1005/1500\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.4266 - accuracy: 0.7934 - val_loss: 0.5154 - val_accuracy: 0.7760\n",
      "Epoch 1006/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4265 - accuracy: 0.7934 - val_loss: 0.5154 - val_accuracy: 0.7760\n",
      "Epoch 1007/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4265 - accuracy: 0.7917 - val_loss: 0.5154 - val_accuracy: 0.7760\n",
      "Epoch 1008/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4265 - accuracy: 0.7934 - val_loss: 0.5155 - val_accuracy: 0.7760\n",
      "Epoch 1009/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4264 - accuracy: 0.7934 - val_loss: 0.5155 - val_accuracy: 0.7760\n",
      "Epoch 1010/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4264 - accuracy: 0.7951 - val_loss: 0.5155 - val_accuracy: 0.7760\n",
      "Epoch 1011/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4264 - accuracy: 0.7934 - val_loss: 0.5155 - val_accuracy: 0.7760\n",
      "Epoch 1012/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4264 - accuracy: 0.7934 - val_loss: 0.5155 - val_accuracy: 0.7760\n",
      "Epoch 1013/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4264 - accuracy: 0.7917 - val_loss: 0.5155 - val_accuracy: 0.7760\n",
      "Epoch 1014/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4263 - accuracy: 0.7917 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1015/1500\n",
      "18/18 [==============================] - 0s 780us/step - loss: 0.4262 - accuracy: 0.7951 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1016/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4263 - accuracy: 0.7951 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1017/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4262 - accuracy: 0.7951 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1018/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4262 - accuracy: 0.7951 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1019/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4261 - accuracy: 0.7934 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1020/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4261 - accuracy: 0.7934 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1021/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4261 - accuracy: 0.7951 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1022/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4261 - accuracy: 0.7934 - val_loss: 0.5156 - val_accuracy: 0.7760\n",
      "Epoch 1023/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4260 - accuracy: 0.7934 - val_loss: 0.5157 - val_accuracy: 0.7760\n",
      "Epoch 1024/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4260 - accuracy: 0.7934 - val_loss: 0.5157 - val_accuracy: 0.7760\n",
      "Epoch 1025/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4259 - accuracy: 0.7934 - val_loss: 0.5157 - val_accuracy: 0.7760\n",
      "Epoch 1026/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4260 - accuracy: 0.7951 - val_loss: 0.5157 - val_accuracy: 0.7760\n",
      "Epoch 1027/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4259 - accuracy: 0.7951 - val_loss: 0.5157 - val_accuracy: 0.7760\n",
      "Epoch 1028/1500\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4258 - accuracy: 0.7934 - val_loss: 0.5157 - val_accuracy: 0.7812\n",
      "Epoch 1029/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4258 - accuracy: 0.7934 - val_loss: 0.5157 - val_accuracy: 0.7812\n",
      "Epoch 1030/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4258 - accuracy: 0.7934 - val_loss: 0.5157 - val_accuracy: 0.7812\n",
      "Epoch 1031/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4258 - accuracy: 0.7934 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1032/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4257 - accuracy: 0.7951 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1033/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4257 - accuracy: 0.7951 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1034/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4257 - accuracy: 0.7969 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1035/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4256 - accuracy: 0.7934 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1036/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4255 - accuracy: 0.7951 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1037/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4256 - accuracy: 0.7934 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1038/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4255 - accuracy: 0.7934 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1039/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4255 - accuracy: 0.7934 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1040/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4255 - accuracy: 0.7934 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1041/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4254 - accuracy: 0.7934 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 1042/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4254 - accuracy: 0.7934 - val_loss: 0.5159 - val_accuracy: 0.7812\n",
      "Epoch 1043/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4254 - accuracy: 0.7951 - val_loss: 0.5159 - val_accuracy: 0.7812\n",
      "Epoch 1044/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4253 - accuracy: 0.7934 - val_loss: 0.5159 - val_accuracy: 0.7812\n",
      "Epoch 1045/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4254 - accuracy: 0.7951 - val_loss: 0.5159 - val_accuracy: 0.7812\n",
      "Epoch 1046/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4253 - accuracy: 0.7969 - val_loss: 0.5159 - val_accuracy: 0.7812\n",
      "Epoch 1047/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4253 - accuracy: 0.7951 - val_loss: 0.5159 - val_accuracy: 0.7812\n",
      "Epoch 1048/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4252 - accuracy: 0.7951 - val_loss: 0.5160 - val_accuracy: 0.7812\n",
      "Epoch 1049/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4252 - accuracy: 0.7934 - val_loss: 0.5160 - val_accuracy: 0.7812\n",
      "Epoch 1050/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4252 - accuracy: 0.7934 - val_loss: 0.5160 - val_accuracy: 0.7812\n",
      "Epoch 1051/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4251 - accuracy: 0.7934 - val_loss: 0.5160 - val_accuracy: 0.7812\n",
      "Epoch 1052/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4251 - accuracy: 0.7934 - val_loss: 0.5160 - val_accuracy: 0.7812\n",
      "Epoch 1053/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4251 - accuracy: 0.7951 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 1054/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4250 - accuracy: 0.7934 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 1055/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4250 - accuracy: 0.7934 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 1056/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4250 - accuracy: 0.7934 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 1057/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4249 - accuracy: 0.7969 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 1058/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4249 - accuracy: 0.7934 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 1059/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4248 - accuracy: 0.7934 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 1060/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4248 - accuracy: 0.7951 - val_loss: 0.5162 - val_accuracy: 0.7812\n",
      "Epoch 1061/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4248 - accuracy: 0.7934 - val_loss: 0.5162 - val_accuracy: 0.7812\n",
      "Epoch 1062/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4248 - accuracy: 0.7951 - val_loss: 0.5162 - val_accuracy: 0.7812\n",
      "Epoch 1063/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4247 - accuracy: 0.7934 - val_loss: 0.5162 - val_accuracy: 0.7812\n",
      "Epoch 1064/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4247 - accuracy: 0.7951 - val_loss: 0.5162 - val_accuracy: 0.7812\n",
      "Epoch 1065/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4247 - accuracy: 0.7951 - val_loss: 0.5162 - val_accuracy: 0.7812\n",
      "Epoch 1066/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4247 - accuracy: 0.7934 - val_loss: 0.5163 - val_accuracy: 0.7812\n",
      "Epoch 1067/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4247 - accuracy: 0.7951 - val_loss: 0.5163 - val_accuracy: 0.7812\n",
      "Epoch 1068/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4246 - accuracy: 0.7934 - val_loss: 0.5163 - val_accuracy: 0.7812\n",
      "Epoch 1069/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4246 - accuracy: 0.7969 - val_loss: 0.5163 - val_accuracy: 0.7812\n",
      "Epoch 1070/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4245 - accuracy: 0.7934 - val_loss: 0.5163 - val_accuracy: 0.7812\n",
      "Epoch 1071/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4245 - accuracy: 0.7969 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 1072/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4244 - accuracy: 0.7934 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 1073/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4245 - accuracy: 0.7969 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 1074/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4244 - accuracy: 0.7969 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 1075/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4243 - accuracy: 0.7934 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 1076/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4243 - accuracy: 0.7969 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 1077/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4242 - accuracy: 0.7951 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 1078/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4242 - accuracy: 0.7969 - val_loss: 0.5165 - val_accuracy: 0.7812\n",
      "Epoch 1079/1500\n",
      "18/18 [==============================] - 0s 804us/step - loss: 0.4242 - accuracy: 0.7969 - val_loss: 0.5165 - val_accuracy: 0.7812\n",
      "Epoch 1080/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4242 - accuracy: 0.7951 - val_loss: 0.5165 - val_accuracy: 0.7812\n",
      "Epoch 1081/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4242 - accuracy: 0.7951 - val_loss: 0.5165 - val_accuracy: 0.7812\n",
      "Epoch 1082/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4241 - accuracy: 0.7969 - val_loss: 0.5165 - val_accuracy: 0.7812\n",
      "Epoch 1083/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4241 - accuracy: 0.7934 - val_loss: 0.5165 - val_accuracy: 0.7812\n",
      "Epoch 1084/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4240 - accuracy: 0.7969 - val_loss: 0.5165 - val_accuracy: 0.7812\n",
      "Epoch 1085/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4240 - accuracy: 0.7986 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 1086/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4240 - accuracy: 0.7969 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 1087/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4239 - accuracy: 0.7969 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 1088/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4239 - accuracy: 0.7969 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 1089/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4239 - accuracy: 0.7969 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 1090/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4239 - accuracy: 0.7969 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 1091/1500\n",
      "18/18 [==============================] - 0s 748us/step - loss: 0.4238 - accuracy: 0.7969 - val_loss: 0.5167 - val_accuracy: 0.7812\n",
      "Epoch 1092/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4238 - accuracy: 0.7951 - val_loss: 0.5167 - val_accuracy: 0.7812\n",
      "Epoch 1093/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4238 - accuracy: 0.7951 - val_loss: 0.5167 - val_accuracy: 0.7812\n",
      "Epoch 1094/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4238 - accuracy: 0.7969 - val_loss: 0.5167 - val_accuracy: 0.7812\n",
      "Epoch 1095/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4237 - accuracy: 0.7969 - val_loss: 0.5167 - val_accuracy: 0.7812\n",
      "Epoch 1096/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4237 - accuracy: 0.7969 - val_loss: 0.5167 - val_accuracy: 0.7812\n",
      "Epoch 1097/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4236 - accuracy: 0.7986 - val_loss: 0.5168 - val_accuracy: 0.7812\n",
      "Epoch 1098/1500\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.4236 - accuracy: 0.7986 - val_loss: 0.5168 - val_accuracy: 0.7812\n",
      "Epoch 1099/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4235 - accuracy: 0.7969 - val_loss: 0.5168 - val_accuracy: 0.7812\n",
      "Epoch 1100/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4235 - accuracy: 0.7969 - val_loss: 0.5168 - val_accuracy: 0.7812\n",
      "Epoch 1101/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4235 - accuracy: 0.7951 - val_loss: 0.5168 - val_accuracy: 0.7812\n",
      "Epoch 1102/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4235 - accuracy: 0.7951 - val_loss: 0.5168 - val_accuracy: 0.7812\n",
      "Epoch 1103/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4234 - accuracy: 0.7969 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1104/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4234 - accuracy: 0.7951 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1105/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4234 - accuracy: 0.7986 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1106/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4233 - accuracy: 0.7951 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1107/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4233 - accuracy: 0.7951 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1108/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4233 - accuracy: 0.7986 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1109/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4232 - accuracy: 0.7969 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1110/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4232 - accuracy: 0.7986 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1111/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4231 - accuracy: 0.7951 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1112/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4231 - accuracy: 0.7969 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1113/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4231 - accuracy: 0.7969 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1114/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4231 - accuracy: 0.7969 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1115/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4231 - accuracy: 0.7986 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1116/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4230 - accuracy: 0.7969 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1117/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4230 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1118/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4230 - accuracy: 0.7969 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1119/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4229 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1120/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4229 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1121/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4229 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1122/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4229 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1123/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4228 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1124/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4228 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1125/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4228 - accuracy: 0.7969 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1126/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4228 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1127/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4227 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1128/1500\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4227 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1129/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4227 - accuracy: 0.8003 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1130/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4226 - accuracy: 0.7986 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1131/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4226 - accuracy: 0.8003 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1132/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4227 - accuracy: 0.7986 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 1133/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4226 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1134/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4225 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1135/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4225 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1136/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4225 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1137/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4225 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1138/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4224 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1139/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4224 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1140/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4224 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1141/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4224 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1142/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4223 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1143/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4223 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1144/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4223 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1145/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4223 - accuracy: 0.8003 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1146/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4222 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1147/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4222 - accuracy: 0.7969 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1148/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4222 - accuracy: 0.7986 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 1149/1500\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4222 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1150/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4222 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1151/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4221 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1152/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4221 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1153/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4221 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1154/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4220 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1155/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4220 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1156/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4220 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1157/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4219 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1158/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4220 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1159/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4220 - accuracy: 0.8003 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1160/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4219 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1161/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4219 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1162/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4218 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1163/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4218 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1164/1500\n",
      "18/18 [==============================] - 0s 780us/step - loss: 0.4218 - accuracy: 0.7986 - val_loss: 0.5171 - val_accuracy: 0.7812\n",
      "Epoch 1165/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4218 - accuracy: 0.8003 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1166/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4218 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1167/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4217 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1168/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4217 - accuracy: 0.7969 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1169/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4217 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1170/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4217 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1171/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4216 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1172/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4217 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1173/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4216 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1174/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4216 - accuracy: 0.8003 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1175/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4215 - accuracy: 0.7986 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1176/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4215 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1177/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4215 - accuracy: 0.7986 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1178/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4215 - accuracy: 0.8003 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1179/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4215 - accuracy: 0.8003 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1180/1500\n",
      "18/18 [==============================] - 0s 889us/step - loss: 0.4215 - accuracy: 0.8003 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1181/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4214 - accuracy: 0.8003 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 1182/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4214 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1183/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4213 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1184/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4213 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1185/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4213 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1186/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4213 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1187/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4213 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1188/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4212 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1189/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4212 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1190/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4212 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1191/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4211 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1192/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4211 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1193/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4211 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1194/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4210 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1195/1500\n",
      "18/18 [==============================] - 0s 780us/step - loss: 0.4210 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1196/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4210 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1197/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4210 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1198/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4210 - accuracy: 0.8003 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 1199/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4210 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1200/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4209 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1201/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4209 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1202/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4209 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1203/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4209 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1204/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4209 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1205/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4208 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1206/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4208 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7812\n",
      "Epoch 1207/1500\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4208 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1208/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4208 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1209/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4207 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1210/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4207 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1211/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4207 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1212/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4207 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1213/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4207 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1214/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4206 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1215/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4206 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1216/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4205 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1217/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4205 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1218/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4205 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1219/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4204 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1220/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4204 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1221/1500\n",
      "18/18 [==============================] - 0s 780us/step - loss: 0.4204 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1222/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4204 - accuracy: 0.8003 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1223/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4203 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1224/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4203 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1225/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4202 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1226/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4202 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1227/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4202 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1228/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4202 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1229/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4201 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1230/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4201 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1231/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4200 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1232/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4201 - accuracy: 0.7969 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1233/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4200 - accuracy: 0.7969 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1234/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4200 - accuracy: 0.7986 - val_loss: 0.5174 - val_accuracy: 0.7760\n",
      "Epoch 1235/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4200 - accuracy: 0.7986 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1236/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4200 - accuracy: 0.7986 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1237/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4199 - accuracy: 0.7986 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1238/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4199 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1239/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4199 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1240/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4198 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1241/1500\n",
      "18/18 [==============================] - 0s 861us/step - loss: 0.4198 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1242/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4197 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1243/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4197 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1244/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4197 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1245/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4196 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1246/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4196 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1247/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4196 - accuracy: 0.7951 - val_loss: 0.5175 - val_accuracy: 0.7760\n",
      "Epoch 1248/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4196 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7708\n",
      "Epoch 1249/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4195 - accuracy: 0.7951 - val_loss: 0.5175 - val_accuracy: 0.7708\n",
      "Epoch 1250/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4195 - accuracy: 0.7969 - val_loss: 0.5175 - val_accuracy: 0.7708\n",
      "Epoch 1251/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4195 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1252/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4195 - accuracy: 0.7951 - val_loss: 0.5175 - val_accuracy: 0.7708\n",
      "Epoch 1253/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4195 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1254/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4194 - accuracy: 0.7951 - val_loss: 0.5175 - val_accuracy: 0.7708\n",
      "Epoch 1255/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4194 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1256/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4194 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1257/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4194 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1258/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4193 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1259/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4193 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1260/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4193 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1261/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4193 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1262/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4192 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1263/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4192 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1264/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4192 - accuracy: 0.7986 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1265/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4192 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1266/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4192 - accuracy: 0.7951 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1267/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4192 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1268/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4191 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1269/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4190 - accuracy: 0.7986 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 1270/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4191 - accuracy: 0.7969 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 1271/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4190 - accuracy: 0.7969 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 1272/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4190 - accuracy: 0.7986 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 1273/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4190 - accuracy: 0.7969 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 1274/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4190 - accuracy: 0.7969 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 1275/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4189 - accuracy: 0.7969 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 1276/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4189 - accuracy: 0.7986 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 1277/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4189 - accuracy: 0.7969 - val_loss: 0.5178 - val_accuracy: 0.7708\n",
      "Epoch 1278/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4188 - accuracy: 0.7969 - val_loss: 0.5178 - val_accuracy: 0.7708\n",
      "Epoch 1279/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4188 - accuracy: 0.7969 - val_loss: 0.5178 - val_accuracy: 0.7708\n",
      "Epoch 1280/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4188 - accuracy: 0.7969 - val_loss: 0.5178 - val_accuracy: 0.7708\n",
      "Epoch 1281/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4188 - accuracy: 0.7969 - val_loss: 0.5178 - val_accuracy: 0.7708\n",
      "Epoch 1282/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4188 - accuracy: 0.7969 - val_loss: 0.5178 - val_accuracy: 0.7708\n",
      "Epoch 1283/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4187 - accuracy: 0.7969 - val_loss: 0.5179 - val_accuracy: 0.7708\n",
      "Epoch 1284/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4187 - accuracy: 0.7969 - val_loss: 0.5179 - val_accuracy: 0.7708\n",
      "Epoch 1285/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4187 - accuracy: 0.7969 - val_loss: 0.5179 - val_accuracy: 0.7708\n",
      "Epoch 1286/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4187 - accuracy: 0.7969 - val_loss: 0.5180 - val_accuracy: 0.7708\n",
      "Epoch 1287/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4187 - accuracy: 0.7969 - val_loss: 0.5180 - val_accuracy: 0.7708\n",
      "Epoch 1288/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4187 - accuracy: 0.7969 - val_loss: 0.5180 - val_accuracy: 0.7708\n",
      "Epoch 1289/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4186 - accuracy: 0.7986 - val_loss: 0.5180 - val_accuracy: 0.7708\n",
      "Epoch 1290/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4186 - accuracy: 0.7969 - val_loss: 0.5181 - val_accuracy: 0.7708\n",
      "Epoch 1291/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4186 - accuracy: 0.7969 - val_loss: 0.5180 - val_accuracy: 0.7708\n",
      "Epoch 1292/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4186 - accuracy: 0.7969 - val_loss: 0.5181 - val_accuracy: 0.7708\n",
      "Epoch 1293/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4185 - accuracy: 0.7969 - val_loss: 0.5181 - val_accuracy: 0.7708\n",
      "Epoch 1294/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4185 - accuracy: 0.7986 - val_loss: 0.5181 - val_accuracy: 0.7708\n",
      "Epoch 1295/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4185 - accuracy: 0.7969 - val_loss: 0.5182 - val_accuracy: 0.7708\n",
      "Epoch 1296/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4185 - accuracy: 0.7986 - val_loss: 0.5182 - val_accuracy: 0.7708\n",
      "Epoch 1297/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4184 - accuracy: 0.7986 - val_loss: 0.5182 - val_accuracy: 0.7708\n",
      "Epoch 1298/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4184 - accuracy: 0.7986 - val_loss: 0.5182 - val_accuracy: 0.7708\n",
      "Epoch 1299/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4184 - accuracy: 0.7986 - val_loss: 0.5182 - val_accuracy: 0.7708\n",
      "Epoch 1300/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4184 - accuracy: 0.7986 - val_loss: 0.5182 - val_accuracy: 0.7708\n",
      "Epoch 1301/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4183 - accuracy: 0.7986 - val_loss: 0.5183 - val_accuracy: 0.7708\n",
      "Epoch 1302/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4183 - accuracy: 0.7969 - val_loss: 0.5182 - val_accuracy: 0.7708\n",
      "Epoch 1303/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4184 - accuracy: 0.7986 - val_loss: 0.5183 - val_accuracy: 0.7708\n",
      "Epoch 1304/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4183 - accuracy: 0.7986 - val_loss: 0.5183 - val_accuracy: 0.7708\n",
      "Epoch 1305/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4182 - accuracy: 0.7969 - val_loss: 0.5183 - val_accuracy: 0.7708\n",
      "Epoch 1306/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4183 - accuracy: 0.7986 - val_loss: 0.5183 - val_accuracy: 0.7708\n",
      "Epoch 1307/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4182 - accuracy: 0.7986 - val_loss: 0.5183 - val_accuracy: 0.7708\n",
      "Epoch 1308/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4182 - accuracy: 0.7986 - val_loss: 0.5184 - val_accuracy: 0.7708\n",
      "Epoch 1309/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4182 - accuracy: 0.7986 - val_loss: 0.5184 - val_accuracy: 0.7708\n",
      "Epoch 1310/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4181 - accuracy: 0.7986 - val_loss: 0.5184 - val_accuracy: 0.7708\n",
      "Epoch 1311/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4181 - accuracy: 0.7986 - val_loss: 0.5184 - val_accuracy: 0.7708\n",
      "Epoch 1312/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4181 - accuracy: 0.7986 - val_loss: 0.5184 - val_accuracy: 0.7708\n",
      "Epoch 1313/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4181 - accuracy: 0.7986 - val_loss: 0.5185 - val_accuracy: 0.7708\n",
      "Epoch 1314/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4180 - accuracy: 0.7986 - val_loss: 0.5185 - val_accuracy: 0.7708\n",
      "Epoch 1315/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4180 - accuracy: 0.7986 - val_loss: 0.5185 - val_accuracy: 0.7708\n",
      "Epoch 1316/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4180 - accuracy: 0.7986 - val_loss: 0.5185 - val_accuracy: 0.7708\n",
      "Epoch 1317/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4180 - accuracy: 0.7986 - val_loss: 0.5185 - val_accuracy: 0.7708\n",
      "Epoch 1318/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4180 - accuracy: 0.7986 - val_loss: 0.5185 - val_accuracy: 0.7708\n",
      "Epoch 1319/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4180 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1320/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4179 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1321/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4179 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1322/1500\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4178 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1323/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4178 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1324/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4178 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1325/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4178 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1326/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4178 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1327/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4178 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1328/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4177 - accuracy: 0.7986 - val_loss: 0.5186 - val_accuracy: 0.7708\n",
      "Epoch 1329/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4177 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1330/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4178 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1331/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4177 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1332/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4177 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1333/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4176 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1334/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4176 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1335/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4176 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1336/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4176 - accuracy: 0.7986 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 1337/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4176 - accuracy: 0.7986 - val_loss: 0.5188 - val_accuracy: 0.7708\n",
      "Epoch 1338/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4175 - accuracy: 0.7986 - val_loss: 0.5188 - val_accuracy: 0.7708\n",
      "Epoch 1339/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4175 - accuracy: 0.7986 - val_loss: 0.5188 - val_accuracy: 0.7708\n",
      "Epoch 1340/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4175 - accuracy: 0.7986 - val_loss: 0.5188 - val_accuracy: 0.7708\n",
      "Epoch 1341/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4175 - accuracy: 0.7986 - val_loss: 0.5188 - val_accuracy: 0.7708\n",
      "Epoch 1342/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4175 - accuracy: 0.7986 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 1343/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4174 - accuracy: 0.7986 - val_loss: 0.5188 - val_accuracy: 0.7708\n",
      "Epoch 1344/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4174 - accuracy: 0.7986 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 1345/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4174 - accuracy: 0.7986 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 1346/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4173 - accuracy: 0.7986 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 1347/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4173 - accuracy: 0.7986 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 1348/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4173 - accuracy: 0.7986 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 1349/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4173 - accuracy: 0.7986 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 1350/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4172 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1351/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4172 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1352/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4173 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1353/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4172 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1354/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4172 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1355/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4172 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1356/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4171 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1357/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4171 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1358/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4170 - accuracy: 0.7986 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 1359/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4171 - accuracy: 0.7986 - val_loss: 0.5191 - val_accuracy: 0.7708\n",
      "Epoch 1360/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4170 - accuracy: 0.7986 - val_loss: 0.5191 - val_accuracy: 0.7708\n",
      "Epoch 1361/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4170 - accuracy: 0.7969 - val_loss: 0.5191 - val_accuracy: 0.7708\n",
      "Epoch 1362/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4170 - accuracy: 0.7986 - val_loss: 0.5191 - val_accuracy: 0.7708\n",
      "Epoch 1363/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4170 - accuracy: 0.7969 - val_loss: 0.5191 - val_accuracy: 0.7708\n",
      "Epoch 1364/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4170 - accuracy: 0.7986 - val_loss: 0.5191 - val_accuracy: 0.7708\n",
      "Epoch 1365/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4170 - accuracy: 0.7969 - val_loss: 0.5191 - val_accuracy: 0.7708\n",
      "Epoch 1366/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4169 - accuracy: 0.7986 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 1367/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4169 - accuracy: 0.7969 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 1368/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4169 - accuracy: 0.7969 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 1369/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4168 - accuracy: 0.7986 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 1370/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4168 - accuracy: 0.7969 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 1371/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4168 - accuracy: 0.7986 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 1372/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4168 - accuracy: 0.7969 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 1373/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4168 - accuracy: 0.7969 - val_loss: 0.5193 - val_accuracy: 0.7708\n",
      "Epoch 1374/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4167 - accuracy: 0.7969 - val_loss: 0.5193 - val_accuracy: 0.7708\n",
      "Epoch 1375/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4167 - accuracy: 0.7969 - val_loss: 0.5193 - val_accuracy: 0.7708\n",
      "Epoch 1376/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4167 - accuracy: 0.7969 - val_loss: 0.5193 - val_accuracy: 0.7708\n",
      "Epoch 1377/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4167 - accuracy: 0.7969 - val_loss: 0.5193 - val_accuracy: 0.7708\n",
      "Epoch 1378/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4167 - accuracy: 0.7969 - val_loss: 0.5193 - val_accuracy: 0.7708\n",
      "Epoch 1379/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4166 - accuracy: 0.7969 - val_loss: 0.5193 - val_accuracy: 0.7708\n",
      "Epoch 1380/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4166 - accuracy: 0.7969 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1381/1500\n",
      "18/18 [==============================] - 0s 807us/step - loss: 0.4166 - accuracy: 0.7969 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1382/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4166 - accuracy: 0.7969 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1383/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4166 - accuracy: 0.7969 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1384/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4165 - accuracy: 0.7969 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1385/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4165 - accuracy: 0.7969 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1386/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4165 - accuracy: 0.7969 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1387/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4165 - accuracy: 0.7986 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1388/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4165 - accuracy: 0.7986 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 1389/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4164 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1390/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4164 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1391/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4164 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1392/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4164 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1393/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4164 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1394/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4163 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1395/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4163 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1396/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4163 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1397/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4163 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1398/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4162 - accuracy: 0.7986 - val_loss: 0.5195 - val_accuracy: 0.7708\n",
      "Epoch 1399/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4163 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1400/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4162 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1401/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4162 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1402/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1403/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1404/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1405/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1406/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 1407/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1408/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1409/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4161 - accuracy: 0.8003 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1410/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1411/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4160 - accuracy: 0.7986 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1412/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4160 - accuracy: 0.7986 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1413/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4160 - accuracy: 0.8003 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1414/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4159 - accuracy: 0.7986 - val_loss: 0.5197 - val_accuracy: 0.7708\n",
      "Epoch 1415/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4159 - accuracy: 0.8003 - val_loss: 0.5198 - val_accuracy: 0.7708\n",
      "Epoch 1416/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4159 - accuracy: 0.7986 - val_loss: 0.5198 - val_accuracy: 0.7708\n",
      "Epoch 1417/1500\n",
      "18/18 [==============================] - 0s 805us/step - loss: 0.4159 - accuracy: 0.7986 - val_loss: 0.5198 - val_accuracy: 0.7708\n",
      "Epoch 1418/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4158 - accuracy: 0.7986 - val_loss: 0.5198 - val_accuracy: 0.7708\n",
      "Epoch 1419/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4159 - accuracy: 0.7986 - val_loss: 0.5198 - val_accuracy: 0.7760\n",
      "Epoch 1420/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4159 - accuracy: 0.8038 - val_loss: 0.5198 - val_accuracy: 0.7760\n",
      "Epoch 1421/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4158 - accuracy: 0.8021 - val_loss: 0.5199 - val_accuracy: 0.7760\n",
      "Epoch 1422/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4158 - accuracy: 0.8038 - val_loss: 0.5199 - val_accuracy: 0.7760\n",
      "Epoch 1423/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4158 - accuracy: 0.8003 - val_loss: 0.5199 - val_accuracy: 0.7760\n",
      "Epoch 1424/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4158 - accuracy: 0.8021 - val_loss: 0.5199 - val_accuracy: 0.7760\n",
      "Epoch 1425/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4158 - accuracy: 0.8003 - val_loss: 0.5200 - val_accuracy: 0.7760\n",
      "Epoch 1426/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4157 - accuracy: 0.8003 - val_loss: 0.5200 - val_accuracy: 0.7760\n",
      "Epoch 1427/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4157 - accuracy: 0.8038 - val_loss: 0.5200 - val_accuracy: 0.7760\n",
      "Epoch 1428/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4157 - accuracy: 0.8021 - val_loss: 0.5200 - val_accuracy: 0.7760\n",
      "Epoch 1429/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4157 - accuracy: 0.8038 - val_loss: 0.5200 - val_accuracy: 0.7760\n",
      "Epoch 1430/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4157 - accuracy: 0.8038 - val_loss: 0.5200 - val_accuracy: 0.7760\n",
      "Epoch 1431/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.5201 - val_accuracy: 0.7760\n",
      "Epoch 1432/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.5201 - val_accuracy: 0.7760\n",
      "Epoch 1433/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.5201 - val_accuracy: 0.7760\n",
      "Epoch 1434/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.5201 - val_accuracy: 0.7760\n",
      "Epoch 1435/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.5201 - val_accuracy: 0.7760\n",
      "Epoch 1436/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.5202 - val_accuracy: 0.7760\n",
      "Epoch 1437/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.5202 - val_accuracy: 0.7760\n",
      "Epoch 1438/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4155 - accuracy: 0.8038 - val_loss: 0.5202 - val_accuracy: 0.7760\n",
      "Epoch 1439/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4155 - accuracy: 0.8038 - val_loss: 0.5202 - val_accuracy: 0.7760\n",
      "Epoch 1440/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4155 - accuracy: 0.8038 - val_loss: 0.5202 - val_accuracy: 0.7760\n",
      "Epoch 1441/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4155 - accuracy: 0.8038 - val_loss: 0.5203 - val_accuracy: 0.7760\n",
      "Epoch 1442/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4154 - accuracy: 0.8038 - val_loss: 0.5203 - val_accuracy: 0.7760\n",
      "Epoch 1443/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4154 - accuracy: 0.8038 - val_loss: 0.5203 - val_accuracy: 0.7760\n",
      "Epoch 1444/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4155 - accuracy: 0.8038 - val_loss: 0.5203 - val_accuracy: 0.7760\n",
      "Epoch 1445/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4154 - accuracy: 0.8038 - val_loss: 0.5203 - val_accuracy: 0.7760\n",
      "Epoch 1446/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4154 - accuracy: 0.8038 - val_loss: 0.5204 - val_accuracy: 0.7760\n",
      "Epoch 1447/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4153 - accuracy: 0.8038 - val_loss: 0.5204 - val_accuracy: 0.7760\n",
      "Epoch 1448/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4153 - accuracy: 0.8056 - val_loss: 0.5204 - val_accuracy: 0.7760\n",
      "Epoch 1449/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4153 - accuracy: 0.8038 - val_loss: 0.5205 - val_accuracy: 0.7760\n",
      "Epoch 1450/1500\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4154 - accuracy: 0.8038 - val_loss: 0.5205 - val_accuracy: 0.7760\n",
      "Epoch 1451/1500\n",
      "18/18 [==============================] - 0s 777us/step - loss: 0.4153 - accuracy: 0.8056 - val_loss: 0.5205 - val_accuracy: 0.7760\n",
      "Epoch 1452/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4153 - accuracy: 0.8038 - val_loss: 0.5205 - val_accuracy: 0.7760\n",
      "Epoch 1453/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4153 - accuracy: 0.8038 - val_loss: 0.5205 - val_accuracy: 0.7760\n",
      "Epoch 1454/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4153 - accuracy: 0.8038 - val_loss: 0.5206 - val_accuracy: 0.7760\n",
      "Epoch 1455/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4152 - accuracy: 0.8038 - val_loss: 0.5206 - val_accuracy: 0.7760\n",
      "Epoch 1456/1500\n",
      "18/18 [==============================] - 0s 779us/step - loss: 0.4152 - accuracy: 0.8056 - val_loss: 0.5206 - val_accuracy: 0.7760\n",
      "Epoch 1457/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4152 - accuracy: 0.8021 - val_loss: 0.5206 - val_accuracy: 0.7760\n",
      "Epoch 1458/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4152 - accuracy: 0.8038 - val_loss: 0.5206 - val_accuracy: 0.7760\n",
      "Epoch 1459/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4152 - accuracy: 0.8038 - val_loss: 0.5206 - val_accuracy: 0.7760\n",
      "Epoch 1460/1500\n",
      "18/18 [==============================] - 0s 833us/step - loss: 0.4151 - accuracy: 0.8021 - val_loss: 0.5207 - val_accuracy: 0.7760\n",
      "Epoch 1461/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4151 - accuracy: 0.8056 - val_loss: 0.5207 - val_accuracy: 0.7760\n",
      "Epoch 1462/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4151 - accuracy: 0.8038 - val_loss: 0.5207 - val_accuracy: 0.7760\n",
      "Epoch 1463/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4151 - accuracy: 0.8003 - val_loss: 0.5207 - val_accuracy: 0.7760\n",
      "Epoch 1464/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4151 - accuracy: 0.8038 - val_loss: 0.5207 - val_accuracy: 0.7760\n",
      "Epoch 1465/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4150 - accuracy: 0.8021 - val_loss: 0.5207 - val_accuracy: 0.7760\n",
      "Epoch 1466/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4150 - accuracy: 0.8038 - val_loss: 0.5207 - val_accuracy: 0.7760\n",
      "Epoch 1467/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4150 - accuracy: 0.8021 - val_loss: 0.5208 - val_accuracy: 0.7760\n",
      "Epoch 1468/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4150 - accuracy: 0.8021 - val_loss: 0.5208 - val_accuracy: 0.7760\n",
      "Epoch 1469/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4150 - accuracy: 0.8021 - val_loss: 0.5208 - val_accuracy: 0.7760\n",
      "Epoch 1470/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4150 - accuracy: 0.8038 - val_loss: 0.5208 - val_accuracy: 0.7760\n",
      "Epoch 1471/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4149 - accuracy: 0.8021 - val_loss: 0.5209 - val_accuracy: 0.7760\n",
      "Epoch 1472/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4150 - accuracy: 0.8021 - val_loss: 0.5209 - val_accuracy: 0.7760\n",
      "Epoch 1473/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4150 - accuracy: 0.8021 - val_loss: 0.5209 - val_accuracy: 0.7760\n",
      "Epoch 1474/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4149 - accuracy: 0.8021 - val_loss: 0.5209 - val_accuracy: 0.7760\n",
      "Epoch 1475/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4149 - accuracy: 0.8038 - val_loss: 0.5209 - val_accuracy: 0.7760\n",
      "Epoch 1476/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4149 - accuracy: 0.8021 - val_loss: 0.5210 - val_accuracy: 0.7760\n",
      "Epoch 1477/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4149 - accuracy: 0.8021 - val_loss: 0.5210 - val_accuracy: 0.7760\n",
      "Epoch 1478/1500\n",
      "18/18 [==============================] - 0s 752us/step - loss: 0.4148 - accuracy: 0.8021 - val_loss: 0.5210 - val_accuracy: 0.7760\n",
      "Epoch 1479/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4148 - accuracy: 0.8021 - val_loss: 0.5210 - val_accuracy: 0.7760\n",
      "Epoch 1480/1500\n",
      "18/18 [==============================] - 0s 776us/step - loss: 0.4148 - accuracy: 0.8021 - val_loss: 0.5211 - val_accuracy: 0.7760\n",
      "Epoch 1481/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4148 - accuracy: 0.8021 - val_loss: 0.5211 - val_accuracy: 0.7760\n",
      "Epoch 1482/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4147 - accuracy: 0.8021 - val_loss: 0.5211 - val_accuracy: 0.7760\n",
      "Epoch 1483/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4148 - accuracy: 0.8021 - val_loss: 0.5211 - val_accuracy: 0.7760\n",
      "Epoch 1484/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4148 - accuracy: 0.8021 - val_loss: 0.5212 - val_accuracy: 0.7760\n",
      "Epoch 1485/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4147 - accuracy: 0.8021 - val_loss: 0.5212 - val_accuracy: 0.7760\n",
      "Epoch 1486/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4147 - accuracy: 0.8021 - val_loss: 0.5212 - val_accuracy: 0.7760\n",
      "Epoch 1487/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4147 - accuracy: 0.8021 - val_loss: 0.5213 - val_accuracy: 0.7760\n",
      "Epoch 1488/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4147 - accuracy: 0.8021 - val_loss: 0.5213 - val_accuracy: 0.7760\n",
      "Epoch 1489/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4147 - accuracy: 0.8021 - val_loss: 0.5213 - val_accuracy: 0.7760\n",
      "Epoch 1490/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5213 - val_accuracy: 0.7760\n",
      "Epoch 1491/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5213 - val_accuracy: 0.7760\n",
      "Epoch 1492/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5214 - val_accuracy: 0.7760\n",
      "Epoch 1493/1500\n",
      "18/18 [==============================] - 0s 806us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5214 - val_accuracy: 0.7760\n",
      "Epoch 1494/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5214 - val_accuracy: 0.7760\n",
      "Epoch 1495/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5214 - val_accuracy: 0.7760\n",
      "Epoch 1496/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4145 - accuracy: 0.8021 - val_loss: 0.5214 - val_accuracy: 0.7760\n",
      "Epoch 1497/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4145 - accuracy: 0.8021 - val_loss: 0.5215 - val_accuracy: 0.7760\n",
      "Epoch 1498/1500\n",
      "18/18 [==============================] - 0s 750us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5215 - val_accuracy: 0.7760\n",
      "Epoch 1499/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4146 - accuracy: 0.8021 - val_loss: 0.5215 - val_accuracy: 0.7760\n",
      "Epoch 1500/1500\n",
      "18/18 [==============================] - 0s 778us/step - loss: 0.4145 - accuracy: 0.8021 - val_loss: 0.5215 - val_accuracy: 0.7760\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(6, input_shape=(8,), activation=\"relu\"))\n",
    "model_2.add(Dense(6,  activation=\"relu\"))\n",
    "model_2.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_2.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy over iterations')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_2.history[\"loss\"])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(range(n), (run_hist_2.history[\"loss\"]),'r.', label=\"Train Loss\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_loss\"]),'b.', label=\"Validation Loss\")\n",
    "ax.legend()\n",
    "ax.set_title('Loss over iterations')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(range(n), (run_hist_2.history[\"accuracy\"]),'r.', label=\"Train Acc\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_accuracy\"]),'b.', label=\"Validation Acc\")\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title('Accuracy over iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.776\n",
      "roc-auc is 0.814\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_class_nn_2 = model_2.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_2 = model_2.predict(X_test_norm)\n",
    "print('')\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_2, 'NN-2')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
